[["index.html", "Computational Probability and Statistics Preface 0.1 Who is this book for? 0.2 Book Structure and How to Use It 0.3 Prerequisites 0.4 Packages 0.5 Acknowledgements", " Computational Probability and Statistics Ken Horton Kris Pruitt Bradley Warner 2021-02-08 Preface This book is based on the notes we created for our students as part of a one semester course on probability and statistics. We developed these notes from three primary resources. The most important is the Openintro Introductory Statistics with Randomization and Simulation (Diez, Barr, and Çetinkaya-Rundel 2014) book. In parts we have used their notes and homework problems. However, in most cases we have altered their work to fit our needs. The second most important book for our work is Introductory to Probability and Statistics with R (Kerns 2010). Finally, we have used some examples, code, and ideas from the first addition of Priums book Foundations and Applications of Statistics: An Introduction Using R (Pruim 2011). 0.1 Who is this book for? We designed this book for study of statistics that maximizes computational ideas while minimizing algebraic symbol manipulation. Although we do discuss traditional small sample normal based inference and some of the classical probability distributions, we rely heavily on ideas such as simulation, permutations, and bootstrap. This means that students with a background in differential and integral calculus will be successful with this book. The book makes extensive using of the R programming language. In particular we focus both on the tidyverse and mosaic packages. We include a significant amount of code in our notes and frequently demonstrate multiple ways of completing a task. We have used this book for juniors and sophomores. 0.2 Book Structure and How to Use It The book is divided into 4 parts. Each part starts with a case study that introduces many of the main ideas of each part. Each chapter is designed to be a standalone 50 minute lesson. Within each lesson, we give exercises that can be worked in class and we have learning objectives. This assumes students have access to R. Finally, we keep the number of homework problems to a reasonable level and assign all problems. The four parts are: Descriptive Statistical Modeling: This part introduces the student to data collection methods, summary statistics, visual summaries, and exploratory data analysis. Probability: We discuss the foundation ideas of probability, counting methods, and common distributions. We use both calculus and simulation to find moments and probabilities. We introduce basic ideas of multivariate probability. We include method of moments and maximum likelihood estimators. Statistical Inference: We discuss many of the basic inference ideas found in a traditional introductory statistics class but we add ideas of bootstrap and permutation methods. Statistical Prediction: The final part introduces prediction methods mainly in the form of linear regression. This part does also include inference for regression. The learning outcomes for this course are to use computational and mathematical statistical/probabilistic concepts for: Developing probabilistic models Developing statistical models for inference and description Advancing practical and theoretical analytic experience and skills 0.3 Prerequisites To take this course, students are expected to have completed calculus up through and including integral calculus. We do have multivariate ideas in the course but they are easily taught and dont require calculus III. We dont assume the students have any programming experience and thus we include a great deal of code. We have supplemented the course with Data Camp courses. We have also used Rstudio Cloud to help students get started without the burden of loading and maintaining software. 0.4 Packages These notes make use of the following packages in R knitr (Xie 2020b), rmarkdown (Allaire et al. 2020), mosaic (Pruim, Kaplan, and Horton 2020), mosaicCalc (Kaplan, Pruim, and Horton 2020), tidyverse (Wickham 2019), ISLR (James et al. 2017), vcd (Meyer, Zeileis, and Hornik 2020), ggplot2 (Wickham et al. 2020), MASS (Ripley 2019), openintro (Çetinkaya-Rundel et al. 2020), broom (Robinson, Hayes, and Couch 2020), kableExtra (Zhu 2020), DT (Xie, Cheng, and Tan 2020). 0.5 Acknowledgements We have been lucky to have numerous open sources to help facilitate this work. This book was written using the bookdown package (Xie 2020a). This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. References "],["CS1.html", "Chapter 1 Case Study 1.1 Objectives 1.2 Introduction to Descriptive Statistical Modeling 1.3 The data analytic process 1.4 Case study 1.5 Homework Problems", " Chapter 1 Case Study 1.1 Objectives Use R for basic analysis and visualization. Compile a report using knitr. 1.2 Introduction to Descriptive Statistical Modeling In this first block of material we will focus on data types, collection methods, summaries, and visualizations. We also intend to introduce computing via the R package. Programming in R requires some focus early in the course and we will supplement with some online courses. There is relatively little mathematics in this first block. 1.3 The data analytic process Scientists seek to answer questions using rigorous methods and careful observations. These observations  collected from the likes of field notes, surveys, and experiments  form the backbone of a statistical investigation and are called data. Statistics is the study of how best to collect, analyze, and draw conclusions from data. It is helpful to put statistics in the context of a general process of investigation: Identify a question or problem. Collect relevant data on the topic. Explore and understand the data. Analyze the data. Form a conclusion. Make decisions based on the conclusion. This is typical of an explanatory process because it starts with a research question and proceeds. However, sometimes an analysis is exploratory. There is data but not necessarily a research question. The purpose of the analysis is to find interesting features in the data and sometimes generate hypotheses. In this course we focus on the explanatory aspects of analysis but we have examples of exploratory. Statistics as a subject focuses on making stages 2-5 objective, rigorous, and efficient. That is, statistics has three primary components: How best can we collect data? How should it be analyzed? And what can we infer from the analysis? The topics scientists investigate are as diverse as the questions they ask. However, many of these investigations can be addressed with a small number of data collection techniques, analytic tools, and fundamental concepts in statistical inference. This lesson provides a glimpse into these and other themes we will encounter throughout the rest of the course. 1.4 Case study In this lesson we will consider an experiment that studies effectiveness of stents in treating patients at risk of stroke.12 Stents are small mesh tubes that are placed inside narrow or weak arteries to assist in patient recovery after cardiac events and reduce the risk of an additional heart attack or death. Many doctors have hoped that there would be similar benefits for patients at risk of stroke. We start by writing the principal question the researchers hope to answer: 1.4.1 Research question Does the use of stents reduce the risk of stroke? 1.4.2 Collect the relevant data The researchers who asked this question collected data on 451 at-risk patients. Each volunteer patient was randomly assigned to one of two groups: Treatment group. Patients in the treatment group received a stent and medical management. The medical management included medications, management of risk factors, and help in lifestyle modification. Control group. Patients in the control group received the same medical management as the treatment group but did not receive stents. Researchers randomly assigned 224 patients to the treatment group and 227 to the control group. In this study, the control group provides a reference point against which we can measure the medical impact of stents in the treatment group. This is an experiment and not an observational study. We will learn more about these ideas in this block. Researchers studied the effect of stents at two time points: 30 days after enrollment and 365 days after enrollment. 1.4.3 Import data We begin our first use of R If you need to install a package, most likely it will be on CRAN, the Comprehensive R Archive Network. Before a package can be used, it must be installed on the computer (once per computer or account) and loaded into a session (once per R session). When you exit R, the package stays installed on the computer but will not be reloaded when R is started again. In summary, R has packages that can be downloaded and installed from online repositories such as CRAN. When you install a package, which only needs to be done once per computer or account, in R all it is doing is placing the source code in a library folder designated during the installation of R. Packages are typically collections of functions and variables that are specific to a certain task or subject matter. For example, to install the mosaic package, enter: install.packages(&quot;mosaic&quot;) # fetch package from CRAN In RStudio there is a Packages tab that makes it easy to add and maintain packages. To use a package in a session, we must load it which makes it available to the current session only. When you start R again, you will have to load packages again. The command library() with the package name supplied as the argument is all that is needed. For this session, we will load tidyverse and mosaic. Note: the box below is executing the R commands, this is known as reproducible research since you can see the code and then you can run or modify as you need. library(tidyverse) library(mosaic) Next read in the data into the working environment. stent_study &lt;- read_csv(&quot;data/stent_study.csv&quot;) Lets break this code down. We are reading from a .csv file and assigning the results into an object called stent_study. The assignment arrow &lt;- means we assign what is on the right to what is on the left. The R function we use in this case is read_csv(); when using R functions, you should ask yourself: What do I want R to do? What information must I provide for R to do this? We want R to read in a .csv file. We can get help on this function by typing ?read_csv at the prompt. The only required input to read_csv() is the file location. We have our data stored in a folder called data under the working directory. We can determine the working directory by typing getwd() at the prompt. getwd() Similarly, if we wish to change the working directory, we can do so by using the setwd() function: setwd(&#39;C:/Users/Brad.Warner/Documents/Classes/Math 377/Another Folder&#39;) In R if you use the view(), you will see the data in what looks like a standard spreadsheet. view(stent_study) 1.4.4 Explore data Before we attempt to answer the research question, lets look at the data. We want R to print out the first 10 rows of the data. The appropriate function is head() and it needs the data object. By default, R will output the first 6 rows. By using the n= argument, we can specify how many rows we want to view. head(stent_study,n=10) ## # A tibble: 10 x 3 ## group outcome30 outcome365 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 control no_event no_event ## 2 trmt no_event no_event ## 3 control no_event no_event ## 4 trmt no_event no_event ## 5 trmt no_event no_event ## 6 control no_event no_event ## 7 trmt no_event no_event ## 8 control no_event no_event ## 9 control no_event no_event ## 10 control no_event no_event We also want to inspect the data. The function is inspect() and R needs the data object stent_study. inspect(stent_study) ## ## categorical variables: ## name class levels n missing ## 1 group character 2 451 0 ## 2 outcome30 character 2 451 0 ## 3 outcome365 character 2 451 0 ## distribution ## 1 control (50.3%), trmt (49.7%) ## 2 no_event (89.8%), stroke (10.2%) ## 3 no_event (83.8%), stroke (16.2%) To keep things simple we will only look at the outcome30 variable in this case study. We will summarize the data in a table. Later in the course, we will learn to do this using the tidy package; for now we use the mosaic package. This package makes use of the modeling formula that you will use extensively later in this course and in Math 378. We want to summarize the data by making a table. In mosaic this is the tally() function. Before using this function, we have to understand the basic formula notation that mosaic uses. The basic format is: goal( y ~ x, data = MyData, ... ) # pseudo-code for the formula template We read y ~ x as y tilde x and interpret it in the equivalent forms: y broken down by x; y modeled by x; y explained by x; y depends on x; or y accounted for by x. For graphics, its reasonable to read the formula as y vs. x, which is exactly the convention used for coordinate axes. For this exercise, we want to apply tally() to the variables group and outcome30. In this case it does not matter which we call y and x; however, it is more natural to think of outcome30 as a dependent variable. tally(outcome30~group,data=stent_study,margins = TRUE) ## group ## outcome30 control trmt ## no_event 214 191 ## stroke 13 33 ## Total 227 224 The margins option totals the columns. Of the 224 patients in the treatment group, 33 had a stroke by the end of the first month. Using these two numbers, we can use R to compute the proportion of patients in the treatment group who had a stroke by the end of their first month. 33/(33+191) ## [1] 0.1473214 Exercise: What proportion of the control group had a stroke? And why is this answer different from what inspect() reports? Lets have R calculate proportions for us. Use ? to look at the help menu for tally(). Note that one of the option arguments of the tally() function is format=. Setting this equal to proportion will output the proportions instead of the counts. tally(outcome30~group,data=stent_study,format=&#39;proportion&#39;,margins = TRUE) ## group ## outcome30 control trmt ## no_event 0.94273128 0.85267857 ## stroke 0.05726872 0.14732143 ## Total 1.00000000 1.00000000 We can compute summary statistics from the table. A summary statistic is a single number summarizing a large amount of data.3 For instance, the primary results of the study after 1 month could be described by two summary statistics: the proportion of people who had a stroke in the treatment and control groups. Proportion who had a stroke in the treatment (stent) group: \\(33/224 = 0.15 = 15\\%\\). Proportion who had a stroke in the control group: \\(13/227 = 0.06 = 6\\%\\). 1.4.5 Visualize the data It is often important to visualize the data. The table is a type of visualization but in this section we will introduce a graphical method called bar charts. We will use the ggformula package to visualize. It is a wrapper to the ggplot2 package which is becoming the industry standard for generating professional graphics. However, its interface is difficult to learn and we will ease into by using ggformula which makes use of the formula notation introduced above. The ggformula package was loaded when we loaded mosaic.4 To generate a basic graphic, we need to ask ourselves what information we are trying to see, what particular type of graph is best, what corresponding R function to use, and what information that R function needs in order to build a plot. For categorical data we want a bar chart and the R function gf_bar() needs the data object and the variable(s) of interest. Here is our first attempt. In Figure 1.1, we leave the y portion of our formula blank. Doing this implies that we simply want to view the number/count of outcome30 by type. We will see the two levels of outcome30 on the x-axis and counts on the y-axis. gf_bar(~outcome30,data=stent_study) Figure 1.1: Using ggformula to create a bar chart. Exercise: Explain Figure 1.1.. This plot graphically shows us the total number of stroke and the total number of no_event. However, this is not what we want. We want to compare the 30-day outcomes for both treatment groups. So we need to break the data into different groups based on treatment type. In the formula language we now update it to the form: goal( y ~ x|z, data = MyData, ... ) # pseudo-code for the formula template We read y ~ x|z as y tilde x by z and interpret in the equivalent forms: y modeled by x for each z; y explained by x within each z; or y accounted for by x within z. For graphics, its reasonable to read the formula as y vs. x for each z. Figure 1.2 shows the results. gf_bar(~outcome30|group,data = stent_study) Figure 1.2: Bar charts conditioned on the group variable. 1.4.5.1 More advanced graphics As a prelude for things to come, the above graphic needs work. The labels dont help; there is no title; we could add color; does it make more sense to use proportions? Here is the code and results for a better graph, see Figure 1.3.. Dont worry if this seems a bit advanced, but feel free to examine each new component of this code. stent_study %&gt;% gf_props(~group,fill=~outcome30,position=&#39;fill&#39;) %&gt;% gf_labs(title=&quot;Impact of Stents of Stroke&quot;, subtitle=&#39;Experiment with 451 Patients&#39;, x=&quot;Experimental Group&quot;, y=&quot;Number of Events&quot;) %&gt;% gf_theme(theme_bw()) Figure 1.3: Better graph. Notice that we used the pipe operator, %&gt;%. This operator allows us to string functions together in a manner that makes it easier to read the code. In the above code we are sending the data object stent_study into the function gf_props() to use as data so we dont need the data = argument. In math, this is a composition of functions. Instead of f(g(x)) we could use a pipe f(g(x)) = g(x) %&gt;% f(). 1.4.6 Conclusion These two summary statistics are useful in looking for differences in the groups, and we are in for a surprise: an additional 9% of patients in the treatment group had a stroke! This is important for two reasons. First, it is contrary to what doctors expected, which was that stents would reduce the rate of strokes. Second, it leads to a statistical question: do the data show a real difference due to the treatment? This second question is subtle. Suppose you flip a coin 100 times. While the chance a coin lands heads in any given coin flip is 50%, we probably wont observe exactly 50 heads. This type of fluctuation is part of almost any type of data generating process. It is possible that the 9% difference in the stent study is due to this natural variation. However, the larger the difference we observe (for a particular sample size), the less believable it is that the difference is due to chance. So what we are really asking is the following: is the difference so large that we should reject the notion that it was due to chance? This is a preview of step 4, analyze the data, and step 5, form a conclusion, of the analysis cycle. While we havent yet covered statistical tools to fully address these steps, we can comprehend the conclusions of the published analysis: there was compelling evidence of harm by stents in this study of stroke patients. Be careful: do not generalize the results of this study to all patients and all stents. This study looked at patients with very specific characteristics who volunteered to be a part of this study and who may not be representative of all stroke patients. In addition, there are many types of stents and this study only considered the self-expanding Wingspan stent (Boston Scientific). However, this study does leave us with an important lesson: we should keep our eyes open for surprises. 1.5 Homework Problems Create an Rmd file 01 Data Case Study Application.Rmd in R, it may be provided, and start by inserting your name in the header. Code blocks below can be inserted and then you can complete the code and answer the questions. When you are done, knit it into a pdf file. To create an R code chunk, use CTRL-ALT-I or on the insert tab of the window, use the drop down to select R. Anything between the dashes is interpreted as R code. For more on RMarkdown see the video, https://www.youtube.com/watch?v=DNS7i2m4sB0 This video assumes you are using R on your computer but we are using RStudio Cloud. Thus we can knit to a pdf since it is setup for us. You can also take the first chapter of the Data Camp course Reporting with R Markdown to learn more. Stent study continued. Complete a similar analysis for the stent data but this time for the one year data. In particular Read the data into your working directory. stent_study &lt;-read_csv(___) Complete similar steps as in the class notes. i. Use inspect on the data. ii. Create a table of outcome365 and group. Comment on the results. iii. Create a barchart of the data. This is a summary of the data. inspect(___) tally(outcome365~___,data=stent_study,format=___,margins = TRUE) stent_study %&gt;% gf_props(~___,fill=~___,position=&#39;fill&#39;) %&gt;% gf_labs(title=___ subtitle=___, x=___, y=___) Migraine and acupuncture. A migraine is a particularly painful type of headache, which patients sometimes wish to treat with acupuncture. To determine whether acupuncture relieves migraine pain, researchers conducted a randomized controlled study where 89 females diagnosed with migraine headaches were randomly assigned to one of two groups: treatment or control. 43 patients in the treatment group received acupuncture that is specifically designed to treat migraines. 46 patients in the control group received placebo acupuncture (needle insertion at nonacupoint locations). 24 hours after patients received acupuncture, they were asked if they were pain free.5 The data is in the file migraine_study.csv in the folder data. Complete the following work: Read the data an object called migraine_study. migraine_study &lt;- read_csv(&quot;data/___&quot;) head(migraine_study) Create a table of the data. tally(___) Report the percent of patients in the treatment group who were pain free 24 hours after receiving acupuncture. Repeat for the control group. At first glance, does acupuncture appear to be an effective treatment for migraines? Explain your reasoning. Do the data provide convincing evidence that there is a real pain reduction for those patients in the treatment group? Or do you think that the observed difference might just be due to chance? Compile, knit, this report into a pdf. Chimowitz MI, Lynn MJ, Derdeyn CP, et al. 2011. Stenting versus Aggressive Medical Therapy for Intracranial Arterial Stenosis. New England Journal of Medicine 365:993-1003. NY Times article reporting on the study Formally, a summary statistic is a value computed from the data. Some summary statistics are more useful than others. https://cran.r-project.org/web/packages/ggformula/vignettes/ggformula-blog.html G. Allais et al. Ear acupuncture in the treatment of migraine attacks: a randomized trial on the efficacy of appropriate versus inappropriate acupoints. In: Neurological Sci. 32.1 (2011), pp. 173175. "],["DB.html", "Chapter 2 Data Basics 2.1 Objectives 2.2 Data basics 2.3 Homework Problems", " Chapter 2 Data Basics 2.1 Objectives Define and use properly in context all new terminology to include but not limited to case, observational unit, variables, data frame, associated variables, independent, and discrete and continuous variables. Identify and define the different types of variables. From reading a study, explain the research question. Create a scatterplot in R and determine the association of two numerical variables from the plot. 2.2 Data basics Effective presentation and description of data is a first step in most analyses. This lesson introduces one structure for organizing data as well as some terminology that will be used throughout this course. 2.2.1 Observations, variables, and data matrices For reference we will be using a data set concerning 50 emails received in 2012. These observations will be referred to as the email50 data set, and they are a random sample from a larger data set. This data is in the openintro package so lets load our packages. library(usdata) Table 2.1 shows 5 rows of the email50 data set concerning 50 emails from 2012. The data object email50 is a subset of email and we have selected to only list 5 rows and 5 variables for ease of observation. Each row in the table represents a single email or case.6 The columns represent characteristics, called variables, for each of the emails. For example, the first row represents email 1, which is not spam, contains 21,705 characters, 551 line breaks, is written in HTML format, and contains only small numbers. Table 2.1: First 5 rows of email data frame spam num_char line_breaks format number 0 21.705 551 1 small 0 7.011 183 1 big 1 0.631 28 0 none 0 15.829 242 1 small Lets look at the first 10 rows of data from email50 using R. Remember to ask the two questions: What do we want R to do? and What must we give R for it to do this? We want the first 10 rows so we use head and R needs the data object and the number of rows. The data object is called email50 and is accessible once the openintro package is loaded. head(email50,n=10) ## # A tibble: 10 x 21 ## spam to_multiple from cc sent_email time image attach ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 1 0 1 2012-01-04 06:19:16 0 0 ## 2 0 0 1 0 0 2012-02-16 13:10:06 0 0 ## 3 1 0 1 4 0 2012-01-04 08:36:23 0 2 ## 4 0 0 1 0 0 2012-01-04 10:49:52 0 0 ## 5 0 0 1 0 0 2012-01-27 02:34:45 0 0 ## 6 0 0 1 0 0 2012-01-17 10:31:57 0 0 ## 7 0 0 1 0 0 2012-03-17 22:18:55 0 0 ## 8 0 0 1 0 1 2012-03-31 07:58:56 0 0 ## 9 0 0 1 1 1 2012-01-10 18:57:54 0 0 ## 10 0 0 1 0 0 2012-01-07 12:29:16 0 0 ## # ... with 13 more variables: dollar &lt;dbl&gt;, winner &lt;fct&gt;, inherit &lt;dbl&gt;, ## # viagra &lt;dbl&gt;, password &lt;dbl&gt;, num_char &lt;dbl&gt;, line_breaks &lt;int&gt;, ## # format &lt;dbl&gt;, re_subj &lt;dbl&gt;, exclaim_subj &lt;dbl&gt;, urgent_subj &lt;dbl&gt;, ## # exclaim_mess &lt;dbl&gt;, number &lt;fct&gt; In practice, it is especially important to ask clarifying questions to ensure important aspects of the data are understood. For instance, it is always important to be sure we know what each variable means and the units of measurement. Descriptions of all variables in the email50 data set are given in its documentation which can be accessed in R by using the ? command: ?email50 (Note that not all data sets will have associated documentation; the authors of openintro package included this documentation with the email50 data set contained in the package.) The data in email50 represent a data matrix or in R terminology data frame or tibble,7 which is a common way to organize data. Each row of a data matrix corresponds to a unique case, and each column corresponds to a variable. This is called tidy data.8 The data frame for the stroke study introduced in the previous lesson had patients as the cases and there were three variables recorded for each patient. If we are thinking of patients as the unit of observation, then this data is tidy. ## # A tibble: 10 x 3 ## group outcome30 outcome365 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 control no_event no_event ## 2 trmt no_event no_event ## 3 control no_event no_event ## 4 trmt no_event no_event ## 5 trmt no_event no_event ## 6 control no_event no_event ## 7 trmt no_event no_event ## 8 control no_event no_event ## 9 control no_event no_event ## 10 control no_event no_event If we think of an outcome as a unit of observation then it is not tidy since the two outcome columns are variable values (month or year). The tidy data for this case would be: ## # A tibble: 10 x 4 ## patient_id group time result ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 control month no_event ## 2 1 control year no_event ## 3 2 trmt month no_event ## 4 2 trmt year no_event ## 5 3 control month no_event ## 6 3 control year no_event ## 7 4 trmt month no_event ## 8 4 trmt year no_event ## 9 5 trmt month no_event ## 10 5 trmt year no_event There are three interrelated rules which make a data set tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Why ensure that your data is tidy? There are two main advantages: Theres a general advantage to picking one consistent way of storing data. If you have a consistent data structure, its easier to learn the tools that work with it because they have an underlying uniformity. Theres a specific advantage to placing variables in columns because it allows Rs vectorised nature to shine. This will be more clear as the semester progresses. Since most built-in R functions work with vectors of values, it makes transforming tidy data feel particularly natural. Data frames are a convenient way to record and store data. If another individual or case is added to the data set, an additional row can be easily added. Similarly, another column can be added for a new variable. Exercise: We consider a publicly available data set that summarizes information about the 3,142 counties in the United States, and we create a data set called county_M377 data set. This data set will include information about each county: its name, the state where it resides, its population in 2000 and 2010, per capita federal spending, poverty rate, and four additional characteristics, we create this data object next. The parent data set is part of the usdata library and is called county_complete. The variables are summarized in help menu built into the usdata package9. How might these data be organized in a data matrix?10 Using R we will create our data object. library(usdata) We only want a subset of the columns and we will use the select verb in dplyr to select and rename columns. We also create a new variable which is federal spending per capita. county_M377 &lt;- county_complete %&gt;% select(name, state, pop2000, pop2010, fed_spend=fed_spending_2009, poverty=poverty_2010, homeownership = homeownership_2010, multi_unit = housing_multi_unit_2010, income = per_capita_income_2010, med_income = median_household_income_2010) %&gt;% mutate(fed_spend=fed_spend/pop2010) Using R, we will display seven rows of the county data frame. head(county_M377,n=7) ## # A tibble: 7 x 10 ## name state pop2000 pop2010 fed_spend poverty homeownership multi_unit income ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Auta~ Alab~ 43671 54571 6.07 10.6 77.5 7.2 24568 ## 2 Bald~ Alab~ 140415 182265 6.14 12.2 76.7 22.6 26469 ## 3 Barb~ Alab~ 29038 27457 8.75 25 68 11.1 15875 ## 4 Bibb~ Alab~ 20826 22915 7.12 12.6 82.9 6.6 19918 ## 5 Blou~ Alab~ 51024 57322 5.13 13.4 82 3.7 21070 ## 6 Bull~ Alab~ 11714 10914 9.97 25.3 76.9 9.9 20289 ## 7 Butl~ Alab~ 21399 20947 9.31 25 69 13.7 16916 ## # ... with 1 more variable: med_income &lt;dbl&gt; 2.2.2 Types of variables Examine the fed_spend, pop2010, and state variables in the county data set. Each of these variables is inherently different from the other two yet many of them share certain characteristics. First consider fed_spend, which is said to be a numerical variable since it can take a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. On the other hand, we would not classify a variable reporting telephone area codes as numerical; even though area codes are made up of numerical digits, their average, sum, and difference have no clear meaning. The pop2010 variable is also numerical; it is sensible to add, subtract, or take averages with those values, although it seems to be a little different than fed_spend. This variable of the population count can only be a whole non-negative number (\\(0\\), \\(1\\), \\(2\\), \\(...\\)). For this reason, the population variable is said to be discrete since it can only take specific numerical values. On the other hand, the federal spending variable is said to be continuous. Now technically, there are no truly continuous numerical variables since all measurements are finite up to some level of accuracy or measurement precision. However, in this course we will treat both variables types of numerical variables the same, that is as continuous variables. The only place this will be different is in probability models which we see in the next block. The variable state can take up to 51 values after accounting for Washington, DC: AL, , and WY. Because the responses themselves are categories, state is called a categorical variable,11 and the possible values are called the variables levels. Figure 2.1: Taxonomy of Variables. Finally, consider a hypothetical variable on education, which describes the highest level of education completed and takes on one of the values noHS, HS, College or Graduate_school. This variable seems to be a hybrid: it is a categorical variable but the levels have a natural ordering. A variable with these properties is called an ordinal variable. To simplify analyses, any ordinal variables in this course will be treated as categorical variables. In R categorical variables can be treated in different ways; one of the key differences is that we can leave them as character values or as factors. When R handles factors, it is only concerned about the levels of values of the factors. We will learn more about this as the semester progresses. Figure 2.1 captures this classification of variables. Exercise: Data were collected about students in a statistics course. Three variables were recorded for each student: number of siblings, student height, and whether the student had previously taken a statistics course. Classify each of the variables as continuous numerical, discrete numerical, or categorical. The number of siblings and student height represent numerical variables. Because the number of siblings is a count, it is discrete. Height varies continuously, so it is a continuous numerical variable. The last variable classifies students into two categories  those who have and those who have not taken a statistics course  which makes this variable categorical. Exercise: Consider the variables group and outcome30 from the stent study in the case study lesson. Are these numerical or categorical variables?12 2.2.3 Relationships between variables Many analyses are motivated by a researcher looking for a relationship between two or more variables, this is the heart of statistical modeling. A social scientist may like to answer some of the following questions: Is federal spending, on average, higher or lower in counties with high rates of poverty? If homeownership is lower than the national average in one county, will the percent of multi-unit structures in that county likely be above or below the national average? To answer these questions, data must be collected, such as the county_complete data set. Examining summary statistics could provide insights for each of the two questions about counties. Additionally, graphs can be used to visually summarize data and are useful for answering such questions as well. Scatterplots are one type of graph used to study the relationship between two numerical variables. Figure 2.2 compares the variables fed_spend and poverty. Each point on the plot represents a single county. For instance, the highlighted dot corresponds to County 1088 in the county_M377 data set: Owsley County, Kentucky, which had a poverty rate of 41.5% and federal spending of $21.50 per capita. The dense cloud in the scatterplot suggests a relationship between the two variables: counties with a high poverty rate also tend to have slightly more federal spending. We might brainstorm as to why this relationship exists and investigate each idea to determine which is the most reasonable explanation. Figure 2.2: A scatterplot showing fed_spend against poverty. Owsley County of Kentucky, with a poverty rate of 41.5% and federal spending of $21.50 per capita, is highlighted. Exercise: Examine the variables in the email50 data set. Create two questions about the relationships between these variables that are of interest to you.13 The fed_spend and poverty variables are said to be associated because the plot shows a discernible pattern. When two variables show some connection with one another, they are called associated variables. Associated variables can also be called dependent variables and vice-versa. Example: The relationship between the homeownership rate and the percent of units in multi-unit structures (e.g. apartments, condos) is visualized using a scatterplot in Figure 2.3. Are these variables associated? It appears that the larger the fraction of units in multi-unit structures, the lower the homeownership rate. Since there is some relationship between the variables, they are associated. Figure 2.3: A scatterplot of the homeownership rate versus the percent of units that are in multi-unit structures for all 3,143 counties. Because there is a downward trend in Figure 2.3  counties with more units in multi-unit structures are associated with lower homeownership  these variables are said to be negatively associated. A positive association is shown in the relationship between the poverty and fed_spend variables represented in Figure 2.2, where counties with higher poverty rates tend to receive more federal spending per capita. If two variables are not associated, then they are said to be independent. That is, two variables are independent if there is no evident relationship between the two. A pair of variables are either related in some way (associated) or not (independent). No pair of variables is both associated and independent. 2.2.4 Creating a scatterplot In this section we will create a simple scatterplot and then ask you to create one on your own. First we will recreate the scatterplot seen in Figure 2.2. This figure uses the county_M377 data set. Here are two questions: What do we want R to do? and What must we give R for it to do this? We want R to create a scatterplot and to do this it needs, at a minimum, the data object, what we want on the \\(x\\)-axis, and what we want on the \\(y\\)-axis. More information on ggformula can be found by clicking on the link.14 county_M377 %&gt;% gf_point(fed_spend~poverty) Figure 2.4: Scatterplot with ggformula. Figure 2.4 is bad, there are poor axis labels, no title, dense clustering of points, the \\(y\\)-axis is being driven by a couple of extreme points. We will need to clear this up. Again, try to read the code and use help() or ? to determine the purpose of each command in Figure 2.5. county_M377 %&gt;% filter(fed_spend&lt;32) %&gt;% gf_point(fed_spend~poverty, xlab=&quot;Poverty Rate (Percent)&quot;, ylab=&quot;Federal Spending Per Capita&quot;, title=&quot;A scatterplot showing fed_spend against poverty&quot;, subtitle = &quot;Owsley County of Kentucky&quot;, cex=1,alpha=0.2) %&gt;% gf_theme(theme_classic()) Figure 2.5: Better example of a scatterplot. Exercise: Create the scatterplot in Figure 2.3. 2.3 Homework Problems Identify study components Identify (i) the cases, (ii) the variables and their types, and (iii) the main research question in the studies described below. Researchers collected data to examine the relationship between pollutants and preterm births in Southern California. During the study air pollution levels were measured by air quality monitoring stations. Specifically, levels of carbon monoxide were recorded in parts per million, nitrogen dioxide and ozone in parts per hundred million, and coarse particulate matter (PM\\(_{10}\\)) in \\(\\mu g/m^3\\). Length of gestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure during gestation was calculated for each birth. The analysis suggested that increased ambient PM\\(_{10}\\) and, to a lesser degree, CO concentrations may be associated with the occurrence of preterm births.15 The Buteyko method is a shallow breathing technique developed by Konstantin Buteyko, a Russian doctor, in 1952. Anecdotal evidence suggests that the Buteyko method can reduce asthma symptoms and improve quality of life. In a scientific study to determine the effectiveness of this method, researchers recruited 600 asthma patients aged 18-69 who relied on medication for asthma treatment. These patients were split into two research groups: one practiced the Buteyko method and the other did not. Patients were scored on quality of life, activity, asthma symptoms, and medication reduction on a scale from 0 to 10. On average, the participants in the Buteyko group experienced a significant reduction in asthma symptoms and an improvement in quality of life.16 A case is also sometimes called a unit of observation or an observational unit A tibble is a data frame with attributes for such things as better display and printing For more information on tidy data see the blog and the book. These data were collected from the US Census website. Each county may be viewed as a case, and there are ten pieces of information recorded for each case. A table with 3,142 rows and 10 columns could hold these data, where each row represents a county and each column represents a particular piece of information. Sometimes also called a nominal variable. There are only two possible values for each variable, and in both cases they describe categories. Thus, each is a categorical variable. Two sample questions: (1) Intuition suggests that if there are many line breaks in an email then there would also tend to be many characters: does this hold true? (2) Is there a connection between whether an email format is plain text (versus HTML) and whether it is a spam message? https://cran.r-project.org/web/packages/ggformula/vignettes/ggformula-blog.html B. Ritz et al. Effect of air pollution on preterm birth among children born in Southern California between 1989 and 1993. In: Epidemiology 11.5 (2000), pp. 502511. J. McGowan. Health Education: Does the Buteyko Institute Method make a difference? In: Thorax 58 (2003). "],["ODCP.html", "Chapter 3 Overview of Data Collection Principles 3.1 Objectives 3.2 Overview of data collection principles 3.3 Homework Problems", " Chapter 3 Overview of Data Collection Principles 3.1 Objectives Define and use properly in context all new terminology. From a description of a research project, at a minimum be able to describe the population of interest, the generalizability of the study, the response and predictor variables, differentiate whether it is observational or experimental, and determine the type of sample. 3.2 Overview of data collection principles The first step in conducting research is to identify topics or questions that are to be investigated. A clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. It is also important to consider how data are collected so that they are reliable and help achieve the research goals. 3.2.1 Populations and samples Consider the following three research questions: What is the average mercury content in swordfish in the Atlantic Ocean? Over the last 5 years, what is the average time to complete a degree for Duke undergraduate students? Does a new drug reduce the number of deaths in patients with severe heart disease? Each research question refers to a target population. In the first question, the target population is all swordfish in the Atlantic Ocean, and each fish represents a case. It is usually too expensive to collect data for every case in a population. Instead, a sample is taken. A sample represents a subset of the cases and is often a small fraction of the population. For instance, 60 swordfish (or some other number) in the population might be selected, and this sample data may be used to provide an estimate of the population average and answer the research question. Exercise: For the second and third questions above, identify the target population and what represents an individual case.17 3.2.2 Anecdotal evidence Consider the following possible responses to the three research questions: A man on the news got mercury poisoning from eating swordfish, so the average mercury concentration in swordfish must be dangerously high. I met two students who took more than 7 years to graduate from Duke, so it must take longer to graduate at Duke than at many other colleges. My friends dad had a heart attack and died after they gave him a new heart disease drug, so the drug must not work. Each conclusion is based on data. However, there are two problems. First, the data only represent one or two cases. Second, and more importantly, it is unclear whether these cases are actually representative of the population. Data collected in this haphazard fashion are called anecdotal evidence. Figure 3.1: In February 2010, some media pundits cited one large snow storm as evidence against global warming. As comedian Jon Stewart pointed out, Its one storm, in one region, of one country. Anecdotal evidence: Be careful of data collected haphazardly. Such evidence may be true and verifiable, but it may only represent extraordinary cases. Anecdotal evidence typically is composed of unusual cases that we recall based on their striking characteristics. For instance, we are more likely to remember the two people we met who took 7 years to graduate than the six others who graduated in four years. Instead of looking at the most unusual cases, we should examine a sample of many cases that represent the population. 3.2.3 Sampling from a population We might try to estimate the time to graduation for Duke undergraduates in the last 5 years by collecting a sample of students. All graduates in the last 5 years represent the population, and graduates who are selected for review are collectively called the sample. In general, we always seek to randomly select a sample from a population. The most basic type of random selection is equivalent to how raffles are conducted. For example, in selecting graduates, we could write each graduates name on a raffle ticket and draw 100 tickets. The selected names would represent a random sample of 100 graduates. This is illustrated in Figure 3.2. Figure 3.2: In this graphic, five graduates are randomly selected from the population to be included in the sample. Why pick a sample randomly? Why not just pick a sample by hand? Consider the following scenario. Example: Suppose we ask a student who happens to be majoring in nutrition to select several graduates for the study. What kind of students do you think she might collect? Do you think her sample would be representative of all graduates?18 Figure 3.3: Instead of sampling from all graduates equally, a nutrition major might inadvertently pick graduates with health-related majors disproportionately often. If someone was permitted to pick and choose exactly which graduates were included in the sample, it is entirely possible that the sample could be skewed to that persons interests, which may be entirely unintentional. This introduces bias into a sample, see Figure 3.3. Sampling randomly helps resolve this problem. The most basic random sample is called a simple random sample, which is equivalent to using a raffle to select cases. This means that each case in the population has an equal chance of being included and there is no implied connection between the cases in the sample. Sometimes a simple random sample is difficult to implement and an alternative method is helpful. One such substitute is a systematic sample, where one case is sampled after letting a fixed number of others, say 10 other cases, pass by. Since this approach uses a mechanism that is not easily subject to personal biases, it often yields a reasonably representative sample. This course will focus on simple random samples since the use of systematic samples is uncommon and requires additional considerations of the context. The act of taking a simple random sample helps minimize bias. However, bias can crop up in other ways. Even when people are picked at random, e.g. for surveys, caution must be exercised if the non-response is high. For instance, if only 30% of the people randomly sampled for a survey actually respond, and it is unclear whether the respondents are representative of the entire population, the survey might suffer from non-response bias. Figure 3.4: Due to the possibility of non-response, surveys studies may only reach a certain group within the population. It is difficult, and often impossible, to completely fix this problem Another common pitfall is a convenience sample, where individuals who are easily accessible are more likely to be included in the sample, see Figure 3.4 . For instance, if a political survey is done by stopping people walking in the Bronx, it will not represent all of New York City. It is often difficult to discern what sub-population a convenience sample represents. Exercise: We can easily access ratings for products, sellers, and companies through websites. These ratings are based only on those people who go out of their way to provide a rating. If 50% of online reviews for a product are negative, do you think this means that 50% of buyers are dissatisfied with the product?19 3.2.4 Explanatory and response variables Consider the following question for the county data set: Is federal spending, on average, higher or lower in counties with high rates of poverty? If we suspect poverty might affect spending in a county, then poverty is the explanatory variable and federal spending is the response variable in the relationship.20 If there are many variables, it may be possible to consider a number of them as explanatory variables. Explanatory and response variables To identify the explanatory variable in a pair of variables, identify which of the two is suspected of affecting the other. Caution: Association does not imply causation. Labeling variables as explanatory and response does not guarantee the relationship between the two is actually causal, even if there is an association identified between the two variables. We use these labels only to keep track of which variable we suspect affects the other. We also use this language to help in our use of R and the formula notation. In some cases, there is no explanatory or response variable. Consider the following question: If homeownership in a particular county is lower than the national average, will the percent of multi-unit structures in that county likely be above or below the national average? It is difficult to decide which of these variables should be considered the explanatory and response variable; i.e. the direction is ambiguous, so no explanatory or response labels are suggested here. 3.2.5 Introducing observational studies and experiments There are two primary types of data collection: observational studies and experiments. Researchers perform an observational study when they collect data in a way that does not directly interfere with how the data arise. For instance, researchers may collect information via surveys, review medical or company records, or follow a cohort of many similar individuals to study why certain diseases might develop. In each of these situations, researchers merely observe what happens. In general, observational studies can provide evidence of a naturally occurring association between variables, but by themselves, they cannot show a causal connection. When researchers want to investigate the possibility of a causal connection, they conduct an experiment. Usually there will be both an explanatory and a response variable. For instance, we may suspect administering a drug will reduce mortality in heart attack patients over the following year. To check if there really is a causal connection between the explanatory variable and the response, researchers will collect a sample of individuals and split them into groups. The individuals in each group are assigned a treatment. When individuals are randomly assigned to a treatment group, the experiment is called a randomized experiment. For example, each heart attack patient in the drug trial could be randomly assigned, perhaps by flipping a coin, into one of two groups: the first group receives a placebo (fake treatment) and the second group receives the drug. The case study at the beginning of the semester is another example of an experiment, though that study did not employ a placebo. Math 359 is a course on the design and analysis of experimental data, DOE. In the Air Force these types of experiments are an important part of test and evaluation. Many Air Force analysts are expert practitioners of DOE. In this course we will minimize our discussion of DOE. Association \\(\\neq\\) Causation Again, association does not imply causation. In a data analysis, association does not imply causation, and causation can only be inferred from a randomized experiment. Although, a hot field is the analysis of causal relationships in observational data. This is important because consider cigarette smoking, how do we know it causes lung cancer? We only have observational data and clearly cannot do an experiment. We think analysts will be charged in the near future with using causal reasoning on observational data. 3.3 Homework Problems Generalizability and causality. Identify the population of interest and the sample in the studies described below. These are the same studies from the previous lesson. Also comment on whether or not the results of the study can be generalized to the population and if the findings of the study can be used to establish causal relationships. Researchers collected data to examine the relationship between pollutants and preterm births in Southern California. During the study air pollution levels were measured by air quality monitoring stations. Specifically, levels of carbon monoxide were recorded in parts per million, nitrogen dioxide and ozone in parts per hundred million, and coarse particulate matter (PM\\(_{10}\\)) in \\(\\mu g/m^3\\). Length of gestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure during gestation was calculated for each birth. The analysis suggested that increased ambient PM\\(_{10}\\) and, to a lesser degree, CO concentrations may be associated with the occurrence of preterm births.21 The Buteyko method is a shallow breathing technique developed by Konstantin Buteyko, a Russian doctor, in 1952. Anecdotal evidence suggests that the Buteyko method can reduce asthma symptoms and improve quality of life. In a scientific study to determine the effectiveness of this method, researchers recruited 600 asthma patients aged 18-69 who relied on medication for asthma treatment. These patients were split into two research groups: one practiced the Buteyko method and the other did not. Patients were scored on quality of life, activity, asthma symptoms, and medication reduction on a scale from 0 to 10. On average, the participants in the Buteyko group experienced a significant reduction in asthma symptoms and an improvement in quality of life.22 GPA and study time. A survey was conducted on 55 undergraduates from Duke University who took an introductory statistics course in Spring 2012. Among many other questions, this survey asked them about their GPA and the number of hours they spent studying per week. The scatterplot below displays the relationship between these two variables. What is the explanatory variable and what is the response variable? Describe the relationship between the two variables. Make sure to discuss unusual observations, if any. Is this an experiment or an observational study? Can we conclude that studying longer hours leads to higher GPAs? Income and education The scatterplot below shows the relationship between per capita income (in thousands of dollars) and percent of population with a bachelors degree in 3,143 counties in the US in 2010. What are the explanatory and response variables? Describe the relationship between the two variables. Make sure to discuss unusual observations, if any. Can we conclude that having a bachelors degree increases ones income? 2) Notice that the first question is only relevant to students who complete their degree; the average cannot be computed using a student who never finished her degree. Thus, only Duke undergraduate students who have graduated in the last five years represent cases in the population under consideration. Each such student would represent an individual case. 3) A person with severe heart disease represents a case. The population includes all people with severe heart disease. Perhaps she would pick a disproportionate number of graduates from health-related fields. Or perhaps her selection would be well-representative of the population. When selecting samples by hand, we run the risk of picking a biased sample, even if that bias is unintentional or difficult to discern. Answers will vary. From our own anecdotal experiences, we believe people tend to rant more about products that fell below expectations than rave about those that perform as expected. For this reason, we suspect there is a negative bias in product ratings on sites like Amazon. However, since our experiences may not be representative, we also keep an open mind. Sometimes the explanatory variable is called the independent variable and the response variable is called the dependent variable. However, this becomes confusing since a pair of variables might be independent or dependent, so be careful and consider the context when using or reading these words. B. Ritz et al. Effect of air pollution on preterm birth among children born in Southern California between 1989 and 1993. In: Epidemiology 11.5 (2000), pp. 502511. J. McGowan. Health Education: Does the Buteyko Institute Method make a difference? In: Thorax 58 (2003). "],["STUDY.html", "Chapter 4 Studies 4.1 Objectives 4.2 Observation studies, sampling strategies, and experiments 4.3 Homework Problems", " Chapter 4 Studies 4.1 Objectives Define and use properly in context all new terminology. Given a study description, be able to identify and explain the study using correct terms. Given a scenario, describe flaws in reasoning and propose study and sampling designs. 4.2 Observation studies, sampling strategies, and experiments 4.2.1 Observational studies Generally, data in observational studies are collected only by monitoring what occurs, while experiments require the primary explanatory variable in a study be assigned for each subject by the researchers. Making causal conclusions based on experiments is often reasonable. However, making the same causal conclusions based on observational data can be treacherous and is not recommended. Thus, observational studies are generally only sufficient to show associations. Exercise: Suppose an observational study tracked sunscreen use and skin cancer, and it was found that the more sunscreen someone used, the more likely the person was to have skin cancer. Does this mean sunscreen causes skin cancer?23 Some previous research24 tells us that using sunscreen actually reduces skin cancer risk, so maybe there is another variable that can explain this hypothetical association between sunscreen usage and skin cancer. One important piece of information that is absent is sun exposure. If someone is out in the sun all day, she is more likely to use sunscreen and more likely to get skin cancer. Exposure to the sun is unaccounted for in the simple investigation. Figure 4.1: Sun exposure is a confounding variable because it is related to both response and explanatory variables. Sun exposure is what is called a confounding variable,25 which is a variable that is correlated with both the explanatory and response variables, see Figure 4.1 . While one method to justify making causal conclusions from observational studies is to exhaust the search for confounding variables, there is no guarantee that all confounding variables can be examined or measured. Lets look at an example of confounding visually. Using the SAT data from the mosaic package lets look at expenditure per pupil versus SAT scores. Figure 4.2 is a plot of the data. Exercise: What conclusion to you reach from the plot in Figure 4.2?26 Figure 4.2: Average SAT score versus expenditure per pupil; reminder: each observation represents an individual state. The implication that spending less might give better results is not justified. Expenditures are confounded with the proportion of students who take the exam, and scores are higher in states where fewer students take the exam. It is interesting to look at the original plot if we place the states into two groups depending on whether more or fewer than 40% of students take the SAT. Figure 4.3 is a plot of the data broken down into the 2 groups. Figure 4.3: Average SAT score versus expenditure per pupil; broken down by level of participation. Once we account for the fraction of students taking the SAT, the relationship between expenditures and SAT scores changes. In the same way, the county data set is an observational study with confounding variables, and its data cannot easily be used to make causal conclusions. Exercise: Figure 4.4 shows a negative association between the homeownership rate and the percentage of multi-unit structures in a county. However, it is unreasonable to conclude that there is a causal relationship between the two variables. Suggest one or more other variables that might explain the relationship in the Figure 4.4.27 Figure 4.4: A scatterplot of the homeownership rate versus the percent of units that are in multi-unit structures for all 3,143 counties. Observational studies come in two forms: prospective and retrospective studies. A prospective study identifies individuals and collects information as events unfold. For instance, medical researchers may identify and follow a group of similar individuals over many years to assess the possible influences of behavior on cancer risk. One example of such a study is The Nurses Health Study, started in 1976 and expanded in 1989.28 This prospective study recruits registered nurses and then collects data from them using questionnaires. Retrospective studies collect data after events have taken place; e.g. researchers may review past events in medical records. Some data sets, such as county, may contain both prospectively- and retrospectively-collected variables. Local governments prospectively collect some variables as events unfolded (e.g. retail sales) while the federal government retrospectively collected others during the 2010 census (e.g. county population). 4.2.2 Three sampling methods Almost all statistical methods are based on the notion of implied randomness. If observational data are not collected in a random framework from a population, results from these statistical methods are not reliable. Here we consider three random sampling techniques: simple, stratified, and cluster sampling. Figures 4.5 , 4.6 , and 4.7 provides a graphical representation of these techniques. Figure 4.5: Examples of simple random sampling. In this figure, simple random sampling was used to randomly select the 18 cases. Figure 4.6: In this figure, stratified sampling was used: cases were grouped into strata, and then simple random sampling was employed within each stratum. Figure 4.7: In this figure, cluster sampling was used, where data were binned into nine clusters, and three of the clusters were randomly selected. Simple random sampling is probably the most intuitive form of random sampling. Consider the salaries of Major League Baseball (MLB) players, where each player is a member of one of the leagues 30 teams. To take a simple random sample of 120 baseball players and their salaries from the 2010 season, we could write the names of that seasons 828 players onto slips of paper, drop the slips into a bucket, shake the bucket around until we are sure the names are all mixed up, then draw out slips until we have the sample of 120 players. In general, a sample is referred to as ``simple random if each case in the population has an equal chance of being included in the final sample and knowing that a case is included in a sample does not provide useful information about which other cases are included. Stratified sampling is a divide-and-conquer sampling strategy. The population is divided into groups called strata. The strata are chosen so that similar cases are grouped together, then a second sampling method, usually simple random sampling, is employed within each stratum. In the baseball salary example, the teams could represent the strata; some teams have a lot more money (were looking at you, Yankees). Then we might randomly sample 4 players from each team for a total of 120 players. Stratified sampling is especially useful when the cases in each stratum are very similar with respect to the outcome of interest. The downside is that analyzing data from a stratified sample is a more complex task than analyzing data from a simple random sample. The analysis methods introduced in this course would need to be extended to analyze data collected using stratified sampling. Example: Why would it be good for cases within each stratum to be very similar?29 In cluster sampling, we group observations into clusters, then randomly sample some of the clusters. Sometimes cluster sampling can be a more economical technique than the alternatives. Also, unlike stratified sampling, cluster sampling is most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves dont look very different from one another. For example, if neighborhoods represented clusters, then this sampling method works best when the neighborhoods are very diverse. A downside of cluster sampling is that more advanced analysis techniques are typically required, though the methods in this course can be extended to handle such data. Example: Suppose we are interested in estimating the malaria rate in a densely tropical portion of rural Indonesia. We learn that there are 30 villages in that part of the Indonesian jungle, each more or less similar to the next. What sampling method should be employed?30 Another technique called multistage sampling is similar to cluster sampling, except that we take a simple random sample within each selected cluster. For instance, if we sampled neighborhoods using cluster sampling, we would next sample a subset of homes within each selected neighborhood if we were using multistage sampling. 4.2.3 Experiments Studies where the researchers assign treatments to cases are called experiments. When this assignment includes randomization, e.g. using a coin flip to decide which treatment a patient receives, it is called a randomized experiment. Randomized experiments are fundamentally important when trying to show a causal connection between two variables. 4.2.3.1 Principles of experimental design Randomized experiments are generally built on four principles. Controlling. Researchers assign treatments to cases, and they do their best to control any other differences in the groups. For example, when patients take a drug in pill form, some patients take the pill with only a sip of water while others may have it with an entire glass of water. To control for the effect of water consumption, a doctor may ask all patients to drink a 12 ounce glass of water with the pill. Randomization. Researchers randomize patients into treatment groups to account for variables that cannot be controlled. For example, some patients may be more susceptible to a disease than others due to their dietary habits. Randomizing patients into the treatment or control group helps even out such differences, and it also prevents accidental bias from entering the study. Replication. The more cases researchers observe, the more accurately they can estimate the effect of the explanatory variable on the response. In a single study, we replicate by collecting a sufficiently large sample. Additionally, a group of scientists may replicate an entire study to verify an earlier finding. You replicate to the level of variability you want to estimate. For example, in flight test, we can run the same flight conditions again to get a replicate; however, if the same plane and pilot are being used, the replicate is not getting the pilot-to-pilot or the plane-to-plane variability. Blocking. Researchers sometimes know or suspect that variables, other than the treatment, influence the response. Under these circumstances, they may first group individuals based on this variable and then randomize cases within each block to the treatment groups. This strategy is often referred to as blocking. For instance, if we are looking at the effect of a drug on heart attacks, we might first split patients into low-risk and high-risk blocks, then randomly assign half the patients from each block to the control group and the other half to the treatment group, as shown in Figure 4.8. This strategy ensures each treatment group has an equal number of low-risk and high-risk patients. Figure 4.8: Blocking using a variable depicting patient risk. Patients are first divided into low-risk and high-risk blocks, then each block is evenly divided into the treatment groups using randomization. This strategy ensures an equal representation of patients in each treatment group from both the low-risk and high-risk categories. It is important to incorporate the first three experimental design principles into any study, and this course describes methods for analyzing data from such experiments. Blocking is a slightly more advanced technique, and statistical methods in this course may be extended to analyze data collected using blocking. Math 359 is an entire course devoted to the design and analysis of experiments. 4.2.3.2 Reducing bias in human experiments Randomized experiments are the gold standard for data collection, but they do not ensure an unbiased perspective into the cause and effect relationships in all cases. Human studies are perfect examples where bias can unintentionally arise. Here we reconsider a study where a new drug was used to treat heart attack patients.31 In particular, researchers wanted to know if the drug reduced deaths in patients. These researchers designed a randomized experiment because they wanted to draw causal conclusions about the drugs effect. Study volunteers32 were randomly placed into two study groups. One group, the treatment group, received the drug. The other group, called the control group, did not receive any drug treatment. Put yourself in the place of a person in the study. If you are in the treatment group, you are given a fancy new drug that you anticipate will help you. On the other hand, a person in the other group doesnt receive the drug and sits idly, hoping her participation doesnt increase her risk of death. These perspectives suggest there are actually two effects: the one of interest is the effectiveness of the drug, and the second is an emotional effect that is difficult to quantify. Researchers arent usually interested in the emotional effect, which might bias the study. To circumvent this problem, researchers do not want patients to know which group they are in. When researchers keep the patients uninformed about their treatment, the study is said to be blind. But there is one problem: if a patient doesnt receive a treatment, she will know she is in the control group. The solution to this problem is to give fake treatments to patients in the control group. A fake treatment is called a placebo, and an effective placebo is the key to making a study truly blind. A classic example of a placebo is a sugar pill that is made to look like the actual treatment pill. Often times, a placebo results in a slight but real improvement in patients. This effect has been dubbed the placebo effect. The patients are not the only ones who should be blinded: doctors and researchers can accidentally bias a study. When a doctor knows a patient has been given the real treatment, she might inadvertently give that patient more attention or care than a patient that she knows is on the placebo. To guard against this bias, which again has been found to have a measurable effect in some instances, most modern studies employ a double-blind setup where doctors or researchers who interact with patients are, just like the patients, unaware of who is or is not receiving the treatment.33 Exercise: Look back to the stent study in the first lesson where researchers were testing whether stents were effective at reducing strokes in at-risk patients. Is this an experiment? Was the study blinded? Was it double-blinded?34 4.3 Homework Problems Propose a sampling strategy. A large college class has 160 students. All 160 students attend the lectures together, but the students are divided into 4 groups, each of 40 students, for lab sections administered by different teaching assistants. The professor wants to conduct a survey about how satisfied the students are with the course, and he believes that the lab section a student is in might affect the students overall satisfaction with the course. What type of study is this? Suggest a sampling strategy for carrying out this study. Flawed reasoning. Identify the flaw in reasoning in the following scenarios. Explain what the individuals in the study should have done differently if they wanted to make such strong conclusions. Students at an elementary school are given a questionnaire that they are required to return after their parents have completed it. One of the questions asked is, Do you find that your work schedule makes it difficult for you to spend time with your kids after school? Of the parents who replied, 85% said no. Based on these results, the school officials conclude that a great majority of the parents have no difficulty spending time with their kids after school. A survey is conducted on a simple random sample of 1,000 women who recently gave birth, asking them about whether or not they smoked during pregnancy. A follow-up survey asking if the children have respiratory problems is conducted 3 years later, however, only 567 of these women are reached at the same address. The researcher reports that these 567 women are representative of all mothers. Sampling strategies. A Math 377 student who is curious about the relationship between the amount of time students spend on social networking sites and their performance at school decides to conduct a survey. Four research strategies for collecting data are described below. In each, name the sampling method proposed and any bias you might expect. He randomly samples 40 students from the studys population, gives them the survey, asks them to fill it out and bring it back the next day. He gives out the survey only to his friends, and makes sure each one of them fills out the survey. He posts a link to an online survey on his Facebook wall and asks his friends to fill out the survey. He stands outside the QRC and asks every third person that walks out the door to fill out the survey. Vitamin supplements. In order to assess the effectiveness of taking large doses of vitamin C in reducing the duration of the common cold, researchers recruited 400 healthy volunteers from staff and students at a university. A quarter of the patients were assigned a placebo, and the rest were evenly divided between 1g Vitamin C, 3g Vitamin C, or 3g Vitamin C plus additives to be taken at onset of a cold for the following two days. All tablets had identical appearance and packaging. The nurses who handed the prescribed pills to the patients knew which patient received which treatment, but the researchers assessing the patients when they were sick did not. No significant differences were observed in any measure of cold duration or severity between the four medication groups, and the placebo group had the shortest duration of symptoms. Was this an experiment or an observational study? Why? What are the explanatory and response variables in this study? Were the patients blinded to their treatment? Was this study double-blind? Participants are ultimately able to choose whether or not to use the pills prescribed to them. We might expect that not all of them will adhere and take their pills. Does this introduce a confounding variable to the study? Explain your reasoning. Exercise and mental health. A researcher is interested in the effects of exercise on mental health and she proposes the following study: Use stratified random sampling to ensure representative proportions of 18-30, 31-40 and 41-55 year olds from the population. Next, randomly assign half the subjects from each age group to exercise twice a week, and instruct the rest not to exercise. Conduct a mental health exam at the beginning and at the end of the study, and compare the results. What type of study is this? What are the treatment and control groups in this study? Does this study make use of blocking? If so, what is the blocking variable? Does this study make use of blinding? Comment on whether or not the results of the study can be used to establish a causal relationship between exercise and mental health, and indicate whether or not the conclusions can be generalized to the population at large. Suppose you are given the task of determining if this proposed study should get funding. Would you have any reservations about the study proposal? No. See the paragraph following the exercise for an explanation. http://www.sciencedirect.com/science/article/pii/S0140673698121682 http://archderm.ama-assn.org/cgi/content/abstract/122/5/537 Study with a similar scenario to that described here: http://onlinelibrary.wiley.com/doi/10.1002/ijc.22745/full Also called a lurking variable, confounding factor, or a confounder. It appears that average SAT score declines as expenditures per student increases. Answers will vary. Population density may be important. If a county is very dense, then a larger fraction of residents may live in multi-unit structures. Additionally, the high density may contribute to increases in property value, making homeownership infeasible for many residents. http://www.channing.harvard.edu/nhs/ We might get a more stable estimate for the subpopulation in a stratum if the cases are very similar. These improved estimates for each subpopulation will help us build a reliable estimate for the full population. A simple random sample would likely draw individuals from all 30 villages, which could make data collection extremely expensive. Stratified sampling would be a challenge since it is unclear how we would build strata of similar individuals. However, cluster sampling seems like a very good idea. We might randomly select a small number of villages. This would probably reduce our data collection costs substantially in comparison to a simple random sample and would still give us helpful information. Anturane Reinfarction Trial Research Group. 1980. Sulfinpyrazone in the prevention of sudden death after myocardial infarction. New England Journal of Medicine 302(5):250-256. Human subjects are often called patients, volunteers, or study participants. There are always some researchers in the study who do know which patients are receiving which treatment. However, they do not interact with the studys patients and do not tell the blinded health care professionals who is receiving which treatment. The researchers assigned the patients into their treatment groups, so this study was an experiment. However, the patients could distinguish what treatment they received, so this study was not blind. The study could not be double-blind since it was not blind. "],["NUMDATA.html", "Chapter 5 Numerical Data 5.1 Objectives 5.2 Numerical Data 5.3 Homework Problems", " Chapter 5 Numerical Data 5.1 Objectives Define and use properly in context all new terminology. Generate in R summary statistics for a numeric variable including breaking down by cases. Generate in R appropriate graphical summaries of numerical variables. Be able to interpret and explain output both graphically and numerically. 5.2 Numerical Data This lesson introduces techniques for exploring and summarizing numerical variables, and the email50 and mlb data sets from the openintro package and a subset of county_complete from usdata provide rich opportunities for examples. Recall that outcomes of numerical variables are numbers on which it is reasonable to perform basic arithmetic operations. For example, the pop2010 variable, which represents the populations of counties in 2010, is numerical since we can sensibly discuss the difference or ratio of the populations in two counties. On the other hand, area codes and zip codes are not numerical. 5.2.1 Scatterplots for paired data A scatterplot provides a case-by-case view of data for two numerical variables. In Figure 5.1, we again present a scatterplot used to examine how federal spending and poverty were related in the county data set. Figure 5.1: A scatterplot showing fed_spend against poverty. Owsley County of Kentucky, with a poverty rate of 41.5% and federal spending of $21.50 per capita, is highlighted. Another scatterplot is shown in Figure 5.2, comparing the number of line breaks line_breaks and number of characters num_char in emails for the email50 data set. In any scatterplot, each point represents a single case. Since there are 50 cases in email50, there are 50 points in Figure 5.2. Figure 5.2: A scatterplot of line_breaks versus num_char for the email50 data. To put the number of characters in perspective, this paragraph has 357 characters. Looking at Figure 5.2, it seems that some emails are incredibly long! Upon further investigation, we would actually find that most of the long emails use the HTML format, which means most of the characters in those emails are used to format the email rather than provide text. Exercise: What do scatterplots reveal about the data, and how might they be useful?35 Example: Consider a new data set of 54 cars with two variables: vehicle price and weight.36 A scatterplot of vehicle price versus weight is shown in Figure 5.3. What can be said about the relationship between these variables? Figure 5.3: A scatterplot of price versus weight for 54 cars. The relationship is evidently nonlinear, as highlighted by the dashed line. This is different from previous scatterplots weve seen which show relationships that are very linear. Exercise: Describe two variables that would have a horseshoe shaped association in a scatterplot.37 5.2.2 Dot plots and the mean Sometimes two variables are one too many: only one variable may be of interest. In these cases, a dot plot provides the most basic of displays. A dot plot is a one-variable scatterplot; an example using the number of characters from 50 emails is shown in Figure 5.4. Figure 5.4: A dot plot of num_char for the email50 data set. The mean, sometimes called the average, is a common way to measure the center of a distribution of data. To find the mean number of characters in the 50 emails, we add up all the character counts and divide by the number of emails. For computational convenience, the number of characters is listed in the thousands and rounded to the first decimal. \\[\\bar{x} = \\frac{21.7 + 7.0 + \\cdots + 15.8}{50} = 11.6\\] The sample mean is often labeled \\(\\bar{x}\\), and the letter \\(x\\) is being used as a generic placeholder for the variable of interest, num_char. Mean The sample mean of a numerical variable is the sum of all of the observations divided by the number of observations, Equation (5.1). \\[\\begin{equation} \\bar{x} = \\frac{x_1+x_2+\\cdots+x_n}{n} \\tag{5.1} \\end{equation}\\] where \\(x_1, x_2, \\dots, x_n\\) represent the \\(n\\) observed values. Exercise: Examine the two equations above. What does \\(x_1\\) correspond to? And \\(x_2\\)? Can you infer a general meaning to what \\(x_i\\) might represent?38 Exercise: What was \\(n\\) in this sample of emails?39 The email50 data set is a sample from a larger population of emails that were received in January and March. We could compute a mean for this population in the same way as the sample mean. However, there is a difference in notation: the population mean has a special label: \\(\\mu\\). The symbol \\(\\mu\\) is the Greek letter mu and represents the average of all observations in the population. Sometimes a subscript, such as \\(_x\\), is used to represent which variable the population mean refers to, e.g. \\(\\mu_x\\). Example: The average number of characters across all emails can be estimated using the sample data. Based on the sample of 50 emails, what would be a reasonable estimate of \\(\\mu_x\\), the mean number of characters in all emails in the email data set? (Recall that email50 is a sample from email.) The sample mean, 11.6, may provide a reasonable estimate of \\(\\mu_x\\). While this number will not be perfect, it provides a point estimate of the population mean. Later in the semester, we will develop tools to characterize the accuracy of point estimates, and we will find that point estimates based on larger samples tend to be more accurate than those based on smaller samples. Example: We might like to compute the average income per person in the US. To do so, we might first think to take the mean of the per capita incomes from the 3,143 counties in the county data set. What would be a better approach? The county data set is special in that each county actually represents many individual people. If we were to simply average across the income variable, we would be treating counties with 5,000 and 5,000,000 residents equally in the calculations. Instead, we should compute the total income for each county, add up all the counties totals, and then divide by the number of people in all the counties. If we completed these steps with the county data, we would find that the per capita income for the US is $27,348.43. Had we computed the simple mean of per capita income across counties, the result would have been just $22,504.70! This previous example used what is called a weighted mean, which will be a key topic in the probability section. As a look ahead, the probability mass function gives the population proportions of each value and thus to find the population mean \\(\\mu\\), we will use a weighted mean. 5.2.3 Histograms and shape Dot plots show the exact value of each observation. This is useful for small data sets, but they can become hard to read with larger samples. Rather than showing the value of each observation, think of the value as belonging to a bin. For example, in the email50 data set, we create a table of counts for the number of cases with character counts between 0 and 5,000, then the number of cases between 5,000 and 10,000, and so on. Observations that fall on the boundary of a bin (e.g. 5,000) are allocated to the lower bin. This tabulation is shown below. ## ## (0,5] (5,10] (10,15] (15,20] (20,25] (25,30] (30,35] (35,40] (40,45] (45,50] ## 19 12 6 2 3 5 0 0 2 0 ## (50,55] (55,60] (60,65] ## 0 0 1 These binned counts are plotted as bars in Figure 5.5 into what is called a histogram. Figure 5.5: A histogram of num_char. This distribution is very strongly skewed to the right. Histograms provide a view of the data density. Higher bars represent where the data are relatively more dense. For instance, there are many more emails between 0 and 10,000 characters than emails between 10,000 and 20,000 characters in the data set. The bars make it easy to see how the density of the data changes relative to the number of characters. Histograms are especially convenient for describing the shape of the data distribution. Figure 5.5 shows that most emails have a relatively small number of characters, while fewer emails have a very large number of characters. When data trail off to the right in this way and have a longer right tail, the shape is said to be right skewed.40 Data sets with the reverse characteristic  a long, thin tail to the left  are said to be left skewed. We also say that such a distribution has a long left tail. Data sets that show roughly equal trailing off in both directions are called symmetric. Long tails to identify skew When data trail off in one direction, the distribution has a long tail. If a distribution has a long left tail, it is left skewed. If a distribution has a long right tail, it is right skewed. Exercise: Take a look at the dot plot above, Figure 5.4. Can you see the skew in the data? Is it easier to see the skew in this histogram or the dot plots?41 Exercise: Besides the mean, what can you see in the dot plot that you cannot see in the histogram?42 5.2.3.1 Making our own histogram Lets take some time to make a simple histogram. We will use the ggformula package which is a wrapper for the ggplot package. Here are two questions: What do we want R to do? and What must we give R for it to do this? We want R to make a histogram. In ggformula the plots have the form gf_XXXX so we will use the gf_histogram. To find options and more information type: ?gf_histogram To start we just have to give the formulas and data to R. gf_histogram(~num_char,data=email50) Exercise: Look at the help menu for gf_histogram and change the x-axis label, change the bin width to 5, and have the left bin start at 0. Here is the code for the exercise email50 %&gt;% gf_histogram(~num_char,binwidth = 5,boundary=0, xlab=&quot;The Number of Characters (in thousands)&quot;) %&gt;% gf_theme(theme_classic()) In addition to looking at whether a distribution is skewed or symmetric, histograms can be used to identify modes. A mode is represented by a prominent peak in the distribution.43 There is only one prominent peak in the histogram of num_char. Figure 5.6 show histograms that have one, two, or three prominent peaks. Such distributions are called unimodal, bimodal, and multimodal, respectively. Any distribution with more than 2 prominent peaks is called multimodal. Notice that there was one prominent peak in the unimodal distribution with a second less prominent peak that was not counted since it only differs from its neighboring bins by a few observations. Figure 5.6: Histograms that demonstrate unimodal, bimodal, and multimodal data. Exercise: Height measurements of young students and adult teachers at a K-3 elementary school were taken. How many modes would you anticipate in this height data set?44 Looking for modes Looking for modes isnt about finding a clear and correct answer about the number of modes in a distribution, which is why prominent is not rigorously defined in these notes. The important part of this examination is to better understand your data and how it might be structured. 5.2.4 Variance and standard deviation The mean is used to describe the center of a data set, but the variability in the data is also important. Here, we introduce two measures of variability: the variance and the standard deviation. Both of these are very useful in data analysis, even though the formulas are a bit tedious to calculate by hand. The standard deviation is the easier of the two to conceptually understand, and it roughly describes how far away the typical observation is from the mean. Equation (5.2) is the equation for sample variance. We will demonstrate it with data so that the notation is easier to understand. \\[\\begin{equation} s^2 = \\sum_{i=1}^{n}\\frac{(x_i-\\bar{x})^2}{n-1}=\\frac{(x_1-\\bar{x})^2 + (x_2-\\bar{x})^2 + (x_3-\\bar{x})^2 + \\cdots + (x_n-\\bar{x})^2}{n-1} \\tag{5.2} \\end{equation}\\] where \\(x_1, x_2, \\dots, x_n\\) represent the \\(n\\) observed values. We call the distance of an observation from its mean its deviation. Below are the deviations for the \\(1^{st}\\), \\(2^{nd}\\), \\(3^{rd}\\), and \\(50^{th}\\) observations in the num_char variable. For computational convenience, the number of characters is listed in the thousands and rounded to the first decimal. \\[ \\begin{aligned} x_1^{}-\\bar{x} &amp;= 21.7 - 11.6 = 10.1 \\hspace{5mm}\\text{ } \\\\ x_2^{}-\\bar{x} &amp;= 7.0 - 11.6 = -4.6 \\\\ x_3^{}-\\bar{x} &amp;= 0.6 - 11.6 = -11.0 \\\\ &amp;\\ \\vdots \\\\ x_{50}^{}-\\bar{x} &amp;= 15.8 - 11.6 = 4.2 \\end{aligned} \\] If we square these deviations and then take an average, the result is equal to the sample variance, denoted by \\(s_{}^2\\): \\[ \\begin{aligned} s_{}^2 &amp;= \\frac{10.1_{}^2 + (-4.6)_{}^2 + (-11.0)_{}^2 + \\cdots + 4.2_{}^2}{50-1} \\\\ &amp;= \\frac{102.01 + 21.16 + 121.00 + \\cdots + 17.64}{49} \\\\ &amp;= 172.44 \\end{aligned} \\] We divide by \\(n-1\\), rather than dividing by \\(n\\), when computing the variance; you need not worry about this mathematical nuance yet. Notice that squaring the deviations does two things. First, it makes large values much larger, seen by comparing \\(10.1^2\\), \\((-4.6)^2\\), \\((-11.0)^2\\), and \\(4.2^2\\). Second, it gets rid of any negative signs. The sample standard deviation \\(s\\) is the square root of the variance: \\[s=\\sqrt{172.44} = 13.13\\] The sample standard deviation of the number of characters in an email is 13.13 thousand. A subscript of \\(_x\\) may be added to the variance and standard deviation, i.e. \\(s_x^2\\) and \\(s_x^{}\\), as a reminder that these are the variance and standard deviation of the observations represented by \\(x_1^{}\\), \\(x_2^{}\\), , \\(x_n^{}\\). The \\(_{x}\\) subscript is usually omitted when it is clear which data the variance or standard deviation is referencing. Variance and standard deviation The variance is roughly the average squared distance from the mean. The standard deviation is the square root of the variance and describes how close the data are to the mean. Formulas and methods used to compute the variance and standard deviation for a population are similar to those used for a sample.45 However, like the mean, the population values have special symbols: \\(\\sigma_{}^2\\) for the variance and \\(\\sigma\\) for the standard deviation. The symbol \\(\\sigma\\) is the Greek letter sigma. Tip: standard deviation describes variability Focus on the conceptual meaning of the standard deviation as a descriptor of variability rather than the formulas. Usually 70% of the data will be within one standard deviation of the mean and about 95% will be within two standard deviations. However, as we have seen, these percentages are not strict rules. Figure 5.7: The first of three very different population distributions with the same mean, 0, and standard deviation, 1. Figure 5.8: The second plot with mean 0 and standard deviation 1. Figure 5.9: The final plot with mean 0 and standard deviation 1. Exercise: Earlier the concept of shape of a distribution was introduced. A good description of the shape of a distribution should include modality and whether the distribution is symmetric or skewed to one side. Using the three figures, Figures 5.7, 5.8, and 5.9 as an example, explain why such a description is important.46 Example: Describe the distribution of the num_char variable using the histogram in Figure 5.5. The description should incorporate the center, variability, and shape of the distribution, and it should also be placed in context: the number of characters in emails. Also note any especially unusual cases.47 In practice, the variance and standard deviation are sometimes used as a means to an end, where the end is being able to accurately estimate the uncertainty associated with a sample statistic. For example, later in the course we will use the variance and standard deviation to assess how close the sample mean is to the population mean. 5.2.5 Box plots, quartiles, and the median A box plot summarizes a data set using five statistics while also plotting unusual observations. Figure 5.10 provides a vertical dot plot alongside a box plot of the num_char variable from the email50 data set. Figure 5.10: A vertical dot plot next to a labeled box plot for the number of characters in 50 emails. The median (6,890), splits the data into the bottom 50% and the top 50%, marked in the dot plot by horizontal dashes and open circles, respectively. The first step in building a box plot is drawing a dark line denoting the median, which splits the data in half. Figure 5.10 shows 50% of the data falling below the median (red dashes) and the other 50% falling above the median (blue open circles). There are 50 character counts in the data set (an even number) so the data are perfectly split into two groups of 25. We take the median in this case to be the average of the two observations closest to the \\(50^{th}\\) percentile: \\((\\text{6,768} + \\text{7,012}) / 2 = \\text{6,890}\\). When there are an odd number of observations, there will be exactly one observation that splits the data into two halves, and in this case that observation is the median (no average needed). Median: the number in the middle If the data are ordered from smallest to largest, the median is the observation right in the middle. If there are an even number of observations, there will be two values in the middle, and the median is taken as their average. The second step in building a box plot is drawing a rectangle to represent the middle 50% of the data. The total length of the box, shown vertically in Figure 5.10, is called the interquartile range (IQR, for short). It, like the standard deviation, is a measure of variability in data. The more variable the data, the larger the standard deviation and IQR. The two boundaries of the box are called the first quartile (the \\(25^{th}\\) percentile, i.e. 25% of the data fall below this value) and the third quartile (the \\(75^{th}\\) percentile), and these are often labeled \\(Q_1\\) and \\(Q_3\\), respectively. Interquartile range (IQR) The IQR is the length of the box in a box plot. It is computed as \\[ IQR = Q_3 - Q_1 \\] where \\(Q_1\\) and \\(Q_3\\) are the \\(25^{th}\\) and \\(75^{th}\\) percentiles. Exercise: What percent of the data fall between \\(Q_1\\) and the median? What percent is between the median and \\(Q_3\\)?48 Extending out from the box, the whiskers attempt to capture the data outside of the box, however, their reach is never allowed to be more than \\(1.5\\times IQR\\).49 They capture everything within this reach. In Figure 5.10, the upper whisker does not extend to the last three points, which are beyond \\(Q_3 + 1.5\\times IQR\\), and so it extends only to the last point below this limit. The lower whisker stops at the lowest value, 33, since there is no additional data to reach; the lower whiskers limit is not shown in the figure because the plot does not extend down to \\(Q_1 - 1.5\\times IQR\\). In a sense, the box is like the body of the box plot and the whiskers are like its arms trying to reach the rest of the data. Any observation that lies beyond the whiskers is labeled with a dot. The purpose of labeling these points  instead of just extending the whiskers to the minimum and maximum observed values  is to help identify any observations that appear to be unusually distant from the rest of the data. Unusually distant observations are called outliers. In this case, it would be reasonable to classify the emails with character counts of 41,623, 42,793, and 64,401 as outliers since they are numerically distant from most of the data. Outliers are extreme An outlier is an observation that is extreme relative to the rest of the data. Why it is important to look for outliers Examination of data for possible outliers serves many useful purposes, including 1. Identifying strong skew in the distribution. 2. Identifying data collection or entry errors. For instance, we re-examined the email purported to have 64,401 characters to ensure this value was accurate. 3. Providing insight into interesting properties of the data. Exercise: The observation with value 64,401, an outlier, was found to be an accurate observation. What would such an observation suggest about the nature of character counts in emails?50 Exercise: Using Figure 5.10, estimate the following values for num_char in the email50 data set: (a) \\(Q_1\\), (b) \\(Q_3\\), and (c) IQR.51 Of course R can calculate these summary statistics for us. First we will do these calculations individually and then in one function call. Remember to ask what you want R to do and what it needs. mean(~num_char,data=email50) ## [1] 11.59822 sd(~num_char,data=email50) ## [1] 13.12526 quantile(~num_char,data=email50) ## 0% 25% 50% 75% 100% ## 0.05700 2.53550 6.88950 15.41075 64.40100 iqr(~num_char,data=email50) ## [1] 12.87525 favstats(~num_char,data=email50) ## min Q1 median Q3 max mean sd n missing ## 0.057 2.5355 6.8895 15.41075 64.401 11.59822 13.12526 50 0 5.2.6 Robust statistics How are the sample statistics of the num_char data set affected by the observation with value 64,401? What would have happened if this email wasnt observed? What would happen to these summary statistics if the observation at 64,401 had been even larger, say 150,000? These scenarios are plotted alongside the original data in Figure 5.11, and sample statistics are computed in R. Figure 5.11: Box plots of the original character count data and two modified data sets. ## group min Q1 median Q3 max mean sd n missing ## 1 Dropped 0.057 2.4540 6.7680 14.15600 42.793 10.52061 10.79768 49 0 ## 2 Increased 0.057 2.5355 6.8895 15.41075 150.000 13.31020 22.43436 50 0 ## 3 Original 0.057 2.5355 6.8895 15.41075 64.401 11.59822 13.12526 50 0 The code used to generate this table is p1 &lt;- email50$num_char p2 &lt;- p1[-which.max(p1)] p3 &lt;- p1 p3[which.max(p1)] &lt;- 150 robust &lt;- data.frame(value= c(p1,p2,p3),group=c(rep(&quot;Original&quot;,50),rep(&quot;Dropped&quot;,49),rep(&quot;Increased&quot;,50))) favstats(value~group,data=robust) Notice by using the formula notation, we were able to calculate the summary statistics for each group. Exercise: (a) Which is more affected by extreme observations, the mean or median? The data summary may be helpful. (b) Is the standard deviation or IQR more affected by extreme observations?52 The median and IQR are called robust estimates because extreme observations have little effect on their values. The mean and standard deviation are much more affected by changes in extreme observations. Example: The median and IQR do not change much under the three scenarios above. Why might this be the case?53 Exercise: The distribution of vehicle prices tends to be right skewed, with a few luxury and sports cars lingering out into the right tail. If you were searching for a new car and cared about price, should you be more interested in the mean or median price of vehicles sold, assuming you are in the market for a regular car?54 5.2.7 Transforming data When data are very strongly skewed, we sometimes transform them so they are easier to model. Consider the histogram of salaries for Major League Baseball players salaries from 2010, which is shown in Figure 5.12. Figure 5.12: Histogram of MLB player salaries for 2010, in millions of dollars. Example: The histogram of MLB player salaries is useful in that we can see the data are extremely skewed and centered (as gauged by the median) at about $1 million. What isnt useful about this plot?55 There are some standard transformations that are often applied when much of the data cluster near zero (relative to the larger values in the data set) and all observations are positive. A transformation is a rescaling of the data using a function. For instance, a plot of the natural logarithm56 of player salaries results in a new histogram in Figure 5.13. Transformed data are sometimes easier to work with when applying statistical models because the transformed data are much less skewed and outliers are usually less extreme. Figure 5.13: Histogram of the log-transformed MLB player salaries for 2010. Transformations can also be applied to one or both variables in a scatterplot. A scatterplot of the line_breaks and num_char variables is shown in Figure 5.2 above. We can see a positive association between the variables and that many observations are clustered near zero. Later in this course, we might want to use a straight line to model the data. However, well find that the data in their current state cannot be modeled very well. Figure 5.14 shows a scatterplot where both the line_breaks and num_char variables have been transformed using a log (base \\(e\\)) transformation. While there is a positive association in each plot, the transformed data show a steadier trend, which is easier to model than the untransformed data. Figure 5.14: A scatterplot of line_breaks versus num_char for the email50 data but where each variable has been log-transformed. Transformations other than the logarithm can be useful, too. For instance, the square root (\\(\\sqrt{\\text{original observation}}\\)) and inverse (\\(\\frac{1}{\\text{original observation}}\\)) are used by statisticians. Common goals in transforming data are to see the data structure differently, reduce skew, assist in modeling, or straighten a nonlinear relationship in a scatterplot. 5.3 Homework Problems Create an Rmd file for the work including headers, file creation data, and explanation of your work. Make sure your plots have a title and the axes are labeled. We are asking you to do more in this application to get ready for your Oral Board. Mammals exploratory Data were collected on 39 species of mammals distributed over 13 orders. The data is in the openintro package as mammals Using help, report the units for the variable brain_Wt. Using inspect how many variables are numeric? What type of variable is danger? Create a histogram of total_sleep and describe the distribution. Create a boxplot of life_span and describe the distribution. Report the mean and median life span of a mammal. Calculate the summary statistics for life_span broken down by danger. What is the standard deviation of life span in danger outcome 5? Mammals life spans Continue using the mammals data set. Create side-by-side boxplots for life_span broken down by exposure. Note: you will have to change exposure to a factor(). Report on any findings. What happened to the median and third quartile in exposure group 4? Create faceted histograms. What are the shortcomings of this plot? Create a new variable exposed that is a factor with level Low if exposure is 1 or 2 and High otherwise. Repeat part c with the new variable. Explain what you see in the plot. Mammals life spans continued Create a scatterplot of life span versus length of gestation. What type of an association is apparent between life span and length of gestation? What type of an association would you expect to see if the axes of the plot were reversed, i.e. if we plotted length of gestation versus life span? Create the new scatterplot suggested in c. Are life span and length of gestation independent? Explain your reasoning. Answers may vary. Scatterplots are helpful in quickly spotting associations between variables, whether those associations represent simple or more complex relationships. Subset of data from http://www.amstat.org/publications/jse/v1n1/datasets.lock.html Consider the case where your vertical axis represents something ``good and your horizontal axis represents something that is only good in moderation. Health and water consumption fit this description since water becomes toxic when consumed in excessive quantities. \\(x_1\\) corresponds to the number of characters in the first email in the sample (21.7, in thousands), \\(x_2\\) to the number of characters in the second email (7.0, in thousands), and \\(x_i\\) corresponds to the number of characters in the \\(i^{th}\\) email in the data set. The sample size was \\(n=50\\). Other ways to describe data that are skewed to the right: skewed to the right, skewed to the high end, or skewed to the positive end. The skew is visible in all both plots, though the dot plot is the least useful. Character counts for individual emails. Another definition of mode, which is not typically used in statistics, is the value with the most occurrences. It is common to have no observations with the same value in a data set, which makes this other definition useless for many real data sets. There might be two height groups visible in the data set: one of the students and one of the adults. That is, the data are probably bimodal. But it could be multimodal because within each group we may be able to see a difference in males and females. The only difference is that the population variance has a division by \\(n\\) instead of \\(n-1\\). Starting with Figure 5.7, the three figures show three distributions that look quite different, but all have the same mean, variance, and standard deviation. Using modality, we can distinguish between the first plot (bimodal) and the last two (unimodal). Using skewness, we can distinguish between the last plot (right skewed) and the first two. While a picture, like a histogram, tells a more complete story, we can use modality and shape (symmetry/skew) to characterize basic information about a distribution. The distribution of email character counts is unimodal and very strongly skewed to the high end. Many of the counts fall near the mean at 11,600, and most fall within one standard deviation (13,130) of the mean. There is one exceptionally long email with about 65,000 characters. Since \\(Q_1\\) and \\(Q_3\\) capture the middle 50% of the data and the median splits the data in the middle, 25% of the data fall between \\(Q_1\\) and the median, and another 25% falls between the median and \\(Q_3\\). While the choice of exactly 1.5 is arbitrary, it is the most commonly used value for box plots. That occasionally there may be very long emails. These visual estimates will vary a little from one person to the next: \\(Q_1\\) ~ 3,000, \\(Q_3\\) ~ 15,000, IQR=\\(Q_3 - Q_1\\) ~ 12,000. (The true values: \\(Q_1=\\) 2,536, \\(Q_3=\\) 15,411, IQR = 12,875.) (a) Mean is affected more. (b) Standard deviation is affected more. The median and IQR are only sensitive to numbers near \\(Q_1\\), the median, and \\(Q_3\\). Since values in these regions are relatively stable  there arent large jumps between observations  the median and IQR estimates are also quite stable. Buyers of a regular car should be concerned about the median price. High-end car sales can drastically inflate the mean price while the median will be more robust to the influence of those sales. Most of the data are collected into one bin in the histogram and the data are so strongly skewed that many details in the data are obscured. Statisticians often write the natural logarithm as \\(\\log\\). You might be more familiar with it being written as \\(\\ln\\). "],["CATDATA.html", "Chapter 6 Categorical Data 6.1 Objectives 6.2 Categorical data 6.3 Homework Problems", " Chapter 6 Categorical Data 6.1 Objectives Define and use properly in context all new terminology. Generate in R tables for categorical variable(s). Generate in R appropriate graphical summaries of categorical and numerical variables. Be able to interpret and explain output both graphically and numerically. 6.2 Categorical data Like numerical data, categorical data can also be organized and analyzed. This section introduces tables and other basic tools for categorical data. Remember at the beginning of this block of material, our case study had categorical data so we have seen some of the ideas in this lesson. The email50 data set represents a sample from a larger email data set called email. This larger data set contains information on 3,921 emails. In this section we will use the email data set to examine whether the presence of numbers, small or large, in an email provides any useful value in classifying email as spam or not spam. 6.2.1 Contingency tables and bar plots In the email data set we have two variables: spam and number that we want to summarize. Lets use inspect() to get information and insight about the two variables. We can also type ?email to learn more about the data. First load the openintro library. library(openintro) email %&gt;% select(spam,number) %&gt;% inspect() ## ## categorical variables: ## name class levels n missing ## 1 number factor 3 3921 0 ## distribution ## 1 small (72.1%), none (14%) ... ## ## quantitative variables: ## name class min Q1 median Q3 max mean sd n missing ## ...1 spam numeric 0 0 0 0 1 0.09359857 0.2913066 3921 0 Notice the use of the pipe operator and how it adds to the ease of reading the code. The select() function allows us to narrow the variables down to the two of interest. Then inspect() gives us information about those variables. We read from top line; we start with the data set email, input it into select() and select variables from it, and then use inspect() to summarize the variables. As is indicated number is a categorical variable that describes whether an email contains no numbers, only small numbers (values under 1 million), or at least one big number (a value of 1 million or more). The variable spam is a numeric variable where 1 indicates the email is spam. To treat it as categorical we will want to change it to a factor but first we will build a table that summarizes data for the two variables, see Table 6.1. This table is called a contingency table. Each value in the table represents the number of times a particular combination of variable outcomes occurred. We will show you the code to generate the contingency table. Table 6.1: A contingency table for the email data. Spam Number none small big Total 0 400 2659 495 3554 1 149 168 50 367 Total 549 2827 545 3921 tally(~spam+number,data=email,margins = TRUE) ## number ## spam none small big Total ## 0 400 2659 495 3554 ## 1 149 168 50 367 ## Total 549 2827 545 3921 The value 149 corresponds to the number of emails in the data set that are spam and had no number listed in the email. Row and column totals are also included. The row totals provide the total counts across each row (e.g. \\(149 + 168 + 50 = 367\\)), and column totals are total counts down each column. The row and column totals are known as marginal counts and the values in the table, such as 149, as joint counts. Lets turn spam into a factor and update the email data object. We will use mutate() to do this. email &lt;- email %&gt;% mutate(spam = factor(email$spam,levels=c(1,0),labels=c(&quot;spam&quot;,&quot;not spam&quot;))) Now checking the data again. email %&gt;% select(spam,number) %&gt;% inspect() ## ## categorical variables: ## name class levels n missing ## 1 spam factor 2 3921 0 ## 2 number factor 3 3921 0 ## distribution ## 1 not spam (90.6%), spam (9.4%) ## 2 small (72.1%), none (14%) ... Lets generate the table again. tally(~spam+number,data=email,margins = TRUE) ## number ## spam none small big Total ## spam 149 168 50 367 ## not spam 400 2659 495 3554 ## Total 549 2827 545 3921 A table for a single variable is called a frequency table. The table below is a frequency table for the number variable. tally(~number,data=email) ## number ## none small big ## 549 2827 545 If we replaced the counts with percentages or proportions, the table would be called a relative frequency table. tally(~number,data=email,format=&#39;proportion&#39;) ## number ## none small big ## 0.1400153 0.7209895 0.1389952 round(tally(~number,data=email,format=&#39;percent&#39;),2) ## number ## none small big ## 14.0 72.1 13.9 A bar plot is a common way to display a single categorical variable. Figure 6.1 shows a bar plot for the number variable. email %&gt;% gf_bar(~number) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;Size of Number&quot;,y=&quot;Count&quot;) Figure 6.1: Bar chart of the number variable. Next the counts are converted into proportions (e.g. \\(549/3921=0.140\\) for none) in Figure 6.2. email %&gt;% gf_props(~number) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;Size of Number&quot;,y=&quot;Proportion&quot;) Figure 6.2: (ref:quote62 Again, lets clean up the plot into a style that we could use in a report. email %&gt;% gf_props(~number,title=&quot;The proportions of emails with a number in it&quot;, subtitle=&quot;From 2012&quot;,xlab=&quot;Type of number in the email&quot;, ylab=&quot;Proportion of emails&quot;) %&gt;% gf_theme(theme_bw()) 6.2.2 Column proportions The table below shows the column proportions. The column proportions are computed as the counts divided by their column totals. The value 149 at the intersection of spam and none is replaced by \\(149/549=0.271\\), i.e. 149 divided by its row total, 549. So what does 0.271 represent? It corresponds to the proportion of emails in the sample with no numbers that are spam. We are conditioning, restricting, on emails with no number. This rate of spam is much higher than emails with only small numbers (5.9%) or big numbers (9.2%). Because these spam rates vary between the three levels of number (none, small, big), this provides evidence that the spam and number variables are associated. tally(spam~number,data=email,margins = TRUE,format=&#39;proportion&#39;) ## number ## spam none small big ## spam 0.27140255 0.05942695 0.09174312 ## not spam 0.72859745 0.94057305 0.90825688 ## Total 1.00000000 1.00000000 1.00000000 The tally() function will always condition on the variable on the right hand side of the tilde, ~, when calculating proportions and thus only generate column proportions. The more general table() function of R will allow either column or row proportions. Exercise: Create a table of column proportions where the variable spam is the column variable. tally(number~spam,data=email,margins = TRUE,format=&#39;proportion&#39;) ## spam ## number spam not spam ## none 0.4059946 0.1125492 ## small 0.4577657 0.7481711 ## big 0.1362398 0.1392797 ## Total 1.0000000 1.0000000 Exercise: In the table you just created, what does 0.748 represent?57 Example: Data scientists use statistics to filter spam from incoming email messages. By noting specific characteristics of an email, a data scientist may be able to classify some emails as spam or not spam with high accuracy. One of those characteristics is whether the email contains no numbers, small numbers, or big numbers. Another characteristic is whether or not an email has any HTML content. A contingency table for the spam and format variables is needed. 1 Make format into a categorical factor variable.The levels should be text and HTML.58 2 Create a contingency table from the email data set with format in the columns and spam in the rows. In deciding which variable to use as a column, the data scientist would be interested in how the proportion of spam changes within each email format. This corresponds to column proportions based on format: the proportion of spam in plain text emails and the proportion of spam in HTML emails. email &lt;- email %&gt;% mutate(format = factor(email$format,levels=c(1,0),labels=c(&quot;HTML&quot;,&quot;text&quot;))) tally(spam~format,data=email,margins = TRUE,format=&quot;proportion&quot;) ## format ## spam HTML text ## spam 0.05796038 0.17489540 ## not spam 0.94203962 0.82510460 ## Total 1.00000000 1.00000000 In generating the column proportions, we can see that a higher fraction of plain text emails are spam (\\(209/1195 = 17.5\\%\\)) than compared to HTML emails (\\(158/2726 = 5.8\\%\\)). This information on its own is insufficient to classify an email as spam or not spam, as over 80% of plain text emails are not spam. Yet, when we carefully combine this information with many other characteristics, such as number and other variables, we stand a reasonable chance of being able to classify some email as spam or not spam. In constructing a table, we need to think about which variable we want in the column and which in the row. The formula format in some way makes us think about the response and predictor variables. However in some cases, it is not clear which variable should be in the column and row and the analyst must decide the point to be made with the table. Before settling on one form for a table, it is important to consider the audience and the message they are to receive from the table. Exercise: Create two tables with number and spam where each are in the column, so two table where you change which variable is in the column. Which would be more useful to someone hoping to identify spam emails using the number variable?59 tally(spam~number,email,format=&#39;proportion&#39;,margin=TRUE) ## number ## spam none small big ## spam 0.27140255 0.05942695 0.09174312 ## not spam 0.72859745 0.94057305 0.90825688 ## Total 1.00000000 1.00000000 1.00000000 tally(number~spam,email,format=&#39;proportion&#39;,margin=TRUE) ## spam ## number spam not spam ## none 0.4059946 0.1125492 ## small 0.4577657 0.7481711 ## big 0.1362398 0.1392797 ## Total 1.0000000 1.0000000 6.2.3 Segmented bar and mosaic plots Contingency tables using column proportions are especially useful for examining how two categorical variables are related. Segmented bar and mosaic plots provide a way to visualize the information in these tables. A segmented bar plot is a graphical display of contingency table information. For example, a segmented bar plot representing the table with number in the column is shown in Figure 6.3, where we have first created a bar plot using the number variable and then separated each group by the levels of spam. email %&gt;% gf_bar(~number,fill=~spam) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;Size of Number&quot;,y=&quot;Count&quot;) Figure 6.3: Segmented bar plot for numbers found in emails, where the counts have been further broken down by spam. The column proportions of the table have been translated into a standardized segmented bar plot in Figure 6.4, which is a helpful visualization of the fraction of spam emails in each level of number. email %&gt;% gf_props(~number,fill=~spam,position=&#39;fill&#39;) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;Size of Number&quot;,y=&quot;Proportion&quot;) Figure 6.4: Standardized version of Figure 6.3. Example: Examine both of the segmented bar plots. Which is more useful? Figure 6.3 contains more information, but Figure 6.4 presents the information more clearly. This second plot makes it clear that emails with no number have a relatively high rate of spam email  about 27%! On the other hand, less than 10% of email with small or big numbers are spam. Since the proportion of spam changes across the groups in Figure 6.4, we can conclude the variables are dependent, which is something we were also able to discern using table proportions. Because both the none and big groups have relatively few observations compared to the small group, the association is more difficult to see in Figure 6.3. In some other cases, a segmented bar plot that is not standardized will be more useful in communicating important information. Before settling on a particular segmented bar plot, create standardized and non-standardized forms and decide which is more effective at communicating features of the data. A mosaic plot is a graphical display of contingency table information that is similar to a bar plot for one variable or a segmented bar plot when using two variables. It seems strange, but mosaic plots are not part of the mosaic package. We must load another set of packages called vcd and vcdExtra. Mosaic plot displays help to visualize the pattern of associations among variables in two-way and larger tables. Mosaic plots are controversial since they rely on the perception of area. Human vision is not good at distinguishing areas. We will introduce mosaic plots because it is another way to visualize contingency tables. Figure 6.5 shows a mosaic plot for the number variable. Each row represents a level of number, and the row heights correspond to the proportion of emails of each number type. For instance, there are fewer emails with no numbers than emails with only small numbers, so the none outcome row is shorter in height. In general, mosaic plots use box areas to represent the number of observations. Since there is only one variable, the widths are all constant. Thus area is simply related to row height making this visual easy to read. library(vcd) mosaic(~number,data=email) Figure 6.5: Mosaic plot where emails are grouped by the number variable. This one-variable mosaic plot can be further divided into pieces as in Figure 6.6 using the spam variable. The first variable in the formula is used to determine row height. That is, each row is split proportionally according to the fraction of emails in each number category, these heights are similar to Figure 6.5. Next each row is split horizontally according to the proportion of emails that were spam in that number group. For example, the second row, representing emails with only small numbers, was divided into emails that were spam (left) and not spam (right). The area of the rectangles is proportional to the proportions in the table where each cell count is divided by the total count. First we will generate the table and then represent it as a mosaic plot. tally(~number+spam,data=email,format=&#39;proportion&#39;) ## spam ## number spam not spam ## none 0.03800051 0.10201479 ## small 0.04284621 0.67814333 ## big 0.01275185 0.12624331 mosaic(~number+spam,data=email) Figure 6.6: Mosaic plot with number as the first variable. These plots are hard to use in a visual comparison of area. For example, is the area for small number spam emails different from none number spam emails? The rectangles have different shapes but from the table we can tell the areas are close. An important use of the mosaic plot is to determine if an association between variables may be present. The bottom of the first column represents spam emails that had big numbers, and the bottom row of the second column represents regular emails that had big numbers. We can again use this plot to see that the spam and number variables are associated since some rows are divided in different vertical locations than others, which was the same technique used for checking an association in the standardized version of the segmented bar plot. In a similar way, a mosaic plot representing column proportions where spam is in the column could be constructed. To completely understand the mosaic plot as shown in Figure 6.7 lets first find the proportions of spam. tally(~spam,data=email,format=&quot;proportion&quot;) ## spam ## spam not spam ## 0.09359857 0.90640143 So the row heights will be split 90-10. Next lets find the proportions of number within each value of spam. In the spam row, none will be 41%, small will be 46%, and big will be 13%. tally(number~spam,data=email,margins = TRUE,format=&quot;proportion&quot;) ## spam ## number spam not spam ## none 0.4059946 0.1125492 ## small 0.4577657 0.7481711 ## big 0.1362398 0.1392797 ## Total 1.0000000 1.0000000 However, because it is more insightful for this application to consider the fraction of spam in each category of the number variable, we prefer Figure 6.6. mosaic(~spam+number,data=email) Figure 6.7: Mosaic plot with spam as the first variable 6.2.4 The only pie chart you will see in this course, hopefully While pie charts are well known, they are not typically as useful as other charts in a data analysis. A pie chart is shown in Figure 6.8. It is generally more difficult to compare group sizes in a pie chart than in a bar plot, especially when categories have nearly identical counts or proportions. In the case of the none and big categories, the difference is so slight you may be unable to distinguish any difference in group sizes. pie(table(email$number), col=COL[c(3,1,2)], radius=0.75) Figure 6.8: A pie chart number for the email data set. Pie charts are popular in the Air Force due to the ease of generating them in Excel and PowerPoint. However, the values for each slice are often printed on top of the chart making the chart irrelevant. We recommend a minimum use of pie charts in your work. 6.2.5 Comparing numerical data across groups Some of the more interesting investigations can be considered by examining numerical data across groups. This is the case where one variable is categorical and the other is numerical. The methods required here arent really new. All that is required is to make a numerical plot for each group. Here two convenient methods are introduced: side-by-side box plots and density plots. We will take a look again at the subset of county_complete data set and compare the median household income for counties that gained population from 2000 to 2010 versus counties that had no gain. While we might like to make a causal connection here, remember that these are observational data and so such an interpretation would be unjustified. This section will give us a chance to perform some data wrangling. We will be using the tidyverse verbs in the process. Data wrangling is an important part of analysis work and typically takes a significant portion of the analysis work. Here is the code to generate the data we need. library(usdata) county_M377 &lt;- county_complete %&gt;% select(name, state, pop2000, pop2010, fed_spend=fed_spending_2009, poverty=poverty_2010, homeownership = homeownership_2010, multi_unit = housing_multi_unit_2010, income = per_capita_income_2010, med_income = median_household_income_2010) %&gt;% mutate(fed_spend=fed_spend/pop2010) First, as a reminder, lets look at the data. What do we want R to do? We want to select the variables pop2000, pop2010, and med_income. What does R need? It needs the data object, and variable names. We will use the select() and inspect() functions. county_M377 %&gt;% select(pop2000,pop2010,med_income) %&gt;% inspect() ## ## quantitative variables: ## name class min Q1 median Q3 max mean sd ## ...1 pop2000 numeric 67 11223.50 24621 61775 9519338 89649.99 292547.67 ## ...2 pop2010 numeric 82 11114.50 25872 66780 9818605 98262.04 312946.70 ## ...3 med_income numeric 19351 36956.25 42450 49144 115574 44274.12 11547.49 ## n missing ## ...1 3139 3 ## ...2 3142 0 ## ...3 3142 0 Notice that three counties are missing population values, reported as NA. Lets remove them and find which counties increased population by creating a new variable. cc_reduced &lt;- county_M377 %&gt;% drop_na(pop2000) %&gt;% select(pop2000,pop2010,med_income) %&gt;% mutate(pop_gain = sign(pop2010-pop2000)) tally(~pop_gain,data=cc_reduced) ## pop_gain ## -1 0 1 ## 1097 1 2041 There were 2,041 counties where the population increased from 2000 to 2010, and there were 1,098 counties with no gain, only 1 county had a net of zero, or a loss. Lets just look at the counties with a gain or loss in side-by-side boxplot. Again, we will use filter() to select the two groups and then make the variable pop_gain into a categorical variable, more data wrangling. cc_reduced &lt;- cc_reduced %&gt;% filter(pop_gain != 0) %&gt;% mutate(pop_gain = factor(pop_gain,levels=c(-1,1),labels=c(&quot;Loss&quot;,&quot;Gain&quot;))) inspect(cc_reduced) ## ## categorical variables: ## name class levels n missing ## 1 pop_gain factor 2 3138 0 ## distribution ## 1 Gain (65%), Loss (35%) ## ## quantitative variables: ## name class min Q1 median Q3 max mean ## ...1 pop2000 numeric 67 11217.25 24608.0 61783.5 9519338 89669.37 ## ...2 pop2010 numeric 82 11127.00 25872.0 66972.0 9818605 98359.23 ## ...3 med_income numeric 19351 36950.00 42443.5 49120.0 115574 44253.24 ## sd n missing ## ...1 292592.28 3138 0 ## ...2 313133.28 3138 0 ## ...3 11528.95 3138 0 The side-by-side box plot is a traditional tool for comparing across groups. An example is shown in Figure 6.9 where there are two box plots, one for each group and drawn on the same scale. cc_reduced %&gt;% gf_boxplot(med_income~pop_gain, subtitle=&quot;The income data were collected between 2006 and 2010.&quot;, xlab=&quot;Population change from 2000 to 2010&quot;, ylab=&quot;Median Household Income&quot;) %&gt;% gf_theme(theme_bw()) Figure 6.9: Side-by-side box plot for median household income, where the counties are split by whether there was a population gain or loss from 2000 to 2010. Another useful plotting method uses density plots to compare numerical data across groups. A histogram bins data but is highly dependent on the number and boundary of the bins. A density plot also estimates the distribution of a numerical variable but does this by estimating the density of data points in a small window around each data point. The overall curve is the sum of this small density estimate. A density plot can be thought of as a smooth version of the histogram. Several options go into a density estimate such as the width of the window and type of smoothing function. These ideas are beyond the point here and we will just use the default options. Figure 6.10 is a plot of the two density curves. cc_reduced %&gt;% gf_dens(~med_income,color=~pop_gain,lwd=1) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;Median household income&quot;,y=&quot;&quot;,col=&quot;Population \\nChange&quot;) Figure 6.10: Density plots of median household income for counties with population gain versus population loss Exercise: Use the box plots and density plots to compare the incomes for counties across the two groups. What do you notice about the approximate center of each group? What do you notice about the variability between groups? Is the shape relatively consistent between groups? How many prominent modes are there for each group?60 Exercise: What components of each plot in Figures 8 and 9 do you find most useful?61 6.3 Homework Problems Create an Rmd file for the work including headers, file creation data, and explanation of your work. Make sure your plots have a title and the axes are labeled. Views on immigration 910 randomly sampled registered voters from Tampa, FL were asked if they thought workers who have illegally entered the US should be (i) allowed to keep their jobs and apply for US citizenship, (ii) allowed to keep their jobs as temporary guest workers but not allowed to apply for US citizenship, or (iii) lose their jobs and have to leave the country. The data is in the openintro package in the immigration data object. How many levels of political are there? Create a table using tally. What percent of these Tampa, FL voters identify themselves as conservatives? What percent of these Tampa, FL voters are in favor of the citizenship option? What percent of these Tampa, FL voters identify themselves as conservatives and are in favor of the citizenship option? What percent of these Tampa, FL voters who identify themselves as conservatives are also in favor of the citizenship option? What percent of moderates and liberal share this view? Create a stacked bar chart. Using your plot, do political ideology and views on immigration appear to be independent? Explain your reasoning. Views on the DREAM Act The same survey from Exercise 1 also asked respondents if they support the DREAM Act, a proposed law which would provide a path to citizenship for people brought illegally to the US as children. The data is in the openintro package in the dream data object. Create a mosaic plot. Based on the mosaic plot, are views on the DREAM Act and political ideology independent? Heart transplants The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable transplant indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. Another variable called survived was used to indicate whether or not the patient was alive at the end of the study. The data is in the openintro package and is called heart_transplant. Create a mosaic plot. Based on the mosaic plot, is survival independent of whether or not the patient got a transplant? Explain your reasoning. Using the variable survtime, create side-by-side boxplots for the control and treatment groups. What do the box plots suggest about the efficacy (effectiveness) of transplants? 0.748 represents the proportions of emails with no spam that had a small number in it. From the help menu on the data HTML is coded as a 1 The column proportions with number in the columns will probably be most useful, which makes it easier to see that emails with small numbers are spam about 5.9% of the time (relatively rare). We would also see that about 27.1% of emails with no numbers are spam, and 9.2% of emails with big numbers are spam. Answers may vary a little. The counties with population gains tend to have higher income (median of about $45,000) versus counties without a gain (median of about $40,000). The variability is also slightly larger for the population gain group. This is evident in the IQR, which is about 50% bigger in the gain group. Both distributions show slight to moderate right skew and are unimodal. There is a secondary small bump at about $60,000 for the no gain group, visible in the density plot, that seems out of place. (Looking into the data set, we would find that 8 of these 15 counties are in Alaska and Texas.) The box plots indicate there are many observations far above the median in each group, though we should anticipate that many observations will fall beyond the whiskers when using such a large data set. The side-by-side box plots are especially useful for comparing centers and spreads, while the density plots are more useful for seeing distribution shape, skew, and groups of anomalies. "],["CS2.html", "Chapter 7 Case Study 7.1 Objectives 7.2 Introduction to Probability Models 7.3 Probability models 7.4 Case study 7.5 Homework Problems", " Chapter 7 Case Study 7.1 Objectives Use R to simulate a probabilistic model. Use basic counting methods. 7.2 Introduction to Probability Models In this second block of material we will focus on probability models. We will take two approaches, one is mathematical and the other is computational. In some cases we can use both methods on a problem and in others only the computational approach is feasible. The mathematical approach to probability modeling allows us insight into the problem and the ability to understand the process. Simulation has a much greater ability to generalize but can be time intensive to run and often requires the writing of custom functions. This case study is extensive and may seem overwhelming, do not worry we will discuss these ideas again in the many lessons we have coming up this block. 7.3 Probability models Probability models are an important tool for data analysts. They are used to explain variation in outcomes that cannot be explained by other variables. We will use these ideas in block 3 to help us make decisions about our statistical models. Often probability models are used to answer a question of the form What is the chance that ..? This means that we typically have an experiment or trial where multiple outcomes are possible and we only have an idea of the frequency of those outcomes. We use this frequency as a measure of the probability of a particular outcome. For this block we will focus just on probability models. To apply a probability model we will need to Select the experiment and its possible values. Have probability values for the outcomes which may include parameters that determine the probabilities. Understand the assumptions behind the model 7.4 Case study There is a famous example of a probability question that we will attack in this case study. The question we want to answer is In a room of \\(n\\) people what is the chance that at least two people have the same birthday? Exercise: The typical classroom at USAFA has 18 students in it. What do you think the chance that at least two students have the same birthday?62 7.4.1 Break down the question The first action we should take is to understand what is being asked. What is the experiment or trial? What does it mean to have the same birthday? What about leap years? What about the frequency of births? Are some days less likely than others? Exercise: Discuss these questions and others that you think are relevant.63 The best first step is to make a simple model, often these are the only ones that will have a mathematical solution. For our problem this means we answer the above questions. We have a room of 18 people and we look at their birthdays. We either have two or more birthdays matching or not; thus there are two outcomes. We dont care about the year, only the day and month. Thus two people born on May 16th are a match. We will ignore leap years. We will assume that a person has equal probability of being born on any of the 365 days of the year. At least two means we could have multiple matches on the same day or several different days where multiple people have matching birthdays. 7.4.2 Simulate (computational) Now that we have an idea about the structure of the problem, we next need to think about how we would simulate a single classroom. We have 18 students in the classroom and they all could have any of the 365 days of the year as a birthday. What we need to do is sample birthdays for each of the 18 students. But how do we code the days of the year? An easy solution is to just label the days from 1 to 365. The function seq() does this for us. days &lt;- seq(1,365) Next we need to pick one of the days using the sample function. Note that we set the seed to get repeatable results, this is not required. set.seed(2022) sample(days,1) ## [1] 228 The first person was born on the 228th day of the year. Since R works on vectors, we dont have to write a loop to select 18 days, we just have sample() do it for us. class &lt;- sample(days,size=18,replace = TRUE) class ## [1] 206 311 331 196 262 191 206 123 233 270 248 7 349 112 1 307 288 354 What do we want R to do? Sample from the numbers 1 to 365 with replacement, which means a number can be picked more than once. Notice in our sample we have at least one match, although it is difficult to look at this list and see the match. Lets sort them to make it easier for us to see. sort(class) ## [1] 1 7 112 123 191 196 206 206 233 248 262 270 288 307 311 331 349 354 The next step is to find a way in R for the code to detect that there is a match. Exercise: What idea(s) can we use to determine if a match exists? We could sort the data and look at differences in sequential values and then check if the set of differences contains a zero. This seems to be computationally expensive. Instead we will use the function unique() which gives a vector of unique values in an object. The function length() gives the number of elements in the vector. length(unique(class)) ## [1] 17 Since we only have 17 unique values in a vector of size 18, we have a match. Now lets put this all together to generate another classroom of size 18. length(unique(sample(days,size=18,replace = TRUE))) ## [1] 16 The next problem that needs to be solved is how to repeat the classrooms and keep track of those that have a match. There are several functions we could use to include replicate() but we will use do() from the mosaic package because it returns a data frame so we can use tidyverse verbs to wrangle the data. The do() function allows us to repeat an operation many times. The following template do(n) * {stuff to do} # pseudo-code where {stuff to do} is typically a single R command, but may be something more complicated. Load the libraries. library(mosaic) library(tidyverse) do(5)*length(unique(sample(days,size=18,replace = TRUE))) ## length ## 1 18 ## 2 17 ## 3 17 ## 4 17 ## 5 18 Lets repeat for a larger number of simulated classroom, remember you should be asking yourself: What do I want R to do? What does R need to do this? (do(1000)*length(unique(sample(days,size=18,replace = TRUE)))) %&gt;% mutate(match=if_else(length==18,0,1)) %&gt;% summarise(prob=mean(match)) ## prob ## 1 0.36 This is within 2 decimal places of the mathematical solution we develop shortly. How many classrooms do we need to simulate to get an accurate estimate of the probability of a match? That is a statistical modeling question and it depends on how much variability we can accept. We will discuss these ideas later in the semester. For now, you can run the code multiple times and see how the estimate varies. If computational power is cheap, you can increase the number of simulations. (do(10000)*length(unique(sample(days,size=18,replace = TRUE)))) %&gt;% mutate(match=if_else(length==18,0,1)) %&gt;% summarise(prob=mean(match)) ## prob ## 1 0.3442 7.4.3 Plotting By the way, the method we have used to create the data allows us to summarize the number of unique birthdays using a table or bar chart. Lets do that now. Note that since the first argument in tally() is not data then the pipe operator will not work without some extra effort. We must tell R that the data is the previous argument in the pipeline and thus use the symbol . to denote this. (do(1000)*length(unique(sample(days,size=18,replace = TRUE)))) %&gt;% tally(~length,data=.) ## length ## 14 15 16 17 18 ## 1 7 52 253 687 Figure 7.1 is a plot of the number of unique birthdays in our sample. (do(1000)*length(unique(sample(days,size=18,replace = TRUE)))) %&gt;% gf_bar(~length) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;Number of unique birthdays&quot;,y=&quot;Count&quot;) Figure 7.1: Bar chart of the number of unique birthdays in the sample. Exercise: What does it mean if the length of unique birthdays is 16, in terms of matches?64 7.4.4 Mathematical solution To solve this problem mathematically, we will step through the logic one step at a time. One of the key ideas that we will see many times is the idea of the multiplication rule. This idea is the foundation for permutation and combinations which are counting methods frequently used in probability calculations. The first step that we take is to understand the idea of 2 or more people with the same birthday. With 18 people, there are a great deal of possibilities for 2 or more birthdays. We could have exactly 2 people with the same birthday. We could have 18 people with the same birthday, We could have 3 people with the same birthday and another 2 people with the same birthday but different from the other 3. Accounting for all these possibilities is too large a counting process. Instead, we will take the approach of finding the probability of no one having a matching birthday. Then the probability of at least 2 people having a matching birthday is 1 minus the probability that no one has a matching birthday. This is known as a complementary probability. A simpler example is to think about rolling a single die. The probability of rolling a 6 is equivalent to 1 minus the probability of not rolling a 6. We first need to think about all the different ways we could get 18 birthdays. This is going to be our denominator in the probability calculation. First lets just look at 2 people. The first person could have 365 different days for their birthday. The second person could also have 365 different birthdays. So for each birthday of the first person there could be 365 birthdays for the second. Thus for 2 people there are \\(365^2\\) possible sets of birthdays. This is an example of the multiplication rule. For 18 people there are \\(365^{18}\\) sets of birthdays. That is a large number. Again, this will be our denominator in calculating the probability. The numerator is the number of sets of birthdays with no matches. Again, lets consider 2 people. The first person can have a birthday on any day of the year, so 365 possibilities. Since we dont want a match, the second person can only have 364 possibilities for a birthday. Thus we have \\(365 \\times 364\\) possibilities for two people to have different birthdays. Exercise: What is the number of possibilities for 18 people so that no one has the same birthday. The answer for 18 people is \\(365 \\times 364 \\times 363 ... \\times 349 \\times 348\\). This looks like a truncated factorial. Remember a factorial, written as \\(n!\\) with an explanation point, is the product of successive positive integers. As an example \\(3!\\) is \\(3 \\times 2 \\times 1\\) or 6. We could write the multiplication for the numerator as \\[\\frac{365!}{(365-n)!}\\] As we will learn, the multiplication rule for the numerator is known as a permutation. We are ready to put it all together. For 18 people, the probability of 2 or more people with the same birthday is 1 minus the probability that no one has the same birthday, which is \\[1 - \\frac{\\frac{365!}{(365-18)!}}{365^{18}}\\] or \\[1 - \\frac{\\frac{365!}{347!}}{365^{18}}\\] In R there is a function called factorial() but factorials get large fast and we will overflow the memory. Try factorial(365) in R to see what happens. factorial(365) ## [1] Inf It is returning infinity because the number is too large for the buffer. As is often the case we will have when using a computational method, we must be clever about our approach. Instead of using factorials we can make use of Rs ability to work on vectors. If we provide R with a vector of values, the prod() will perform a product of all the elements. 365*364 ## [1] 132860 prod(365:364) ## [1] 132860 1- prod(365:348)/(365^18) ## [1] 0.3469114 7.4.5 General solution We now have the mathematics to understand the problem. We can easily generalize this to any number of people. To do this, we have to write a function in R. As with everything in R, we save a function as an object. The general format for creating a function is my_function &lt;- function(parameters){ code for function } For this problem we will call the function birthday_prob(). The only parameter we need is the number of people in the room, n. Lets write this function. birthday_prob &lt;- function(n=20){ 1- prod(365:(365-(n-1)))/(365^n) } Notice we assigned the function to the name birthday_prob, we told R to expect one argument to the function, which we are calling n, and then we provide R with the code to find the probability. We set a default value for n in case one is not provided to prevent an error when the function is run. We will learn more about writing functions over this and the next semester. Test the code with a know answer. birthday_prob(18) ## [1] 0.3469114 Now we can determine the probability for any size room. You may have heard that it only takes about 23 people in a room to have a 50% probability of at least 2 people matching birthdays. birthday_prob(23) ## [1] 0.5072972 Lets create a plot of the probability versus number of people in the room. To do this, we need to apply the function to a vector of values. The function sapply() will work or we can also use Vectorize() to alter our existing function. We choose the latter option. First notice what happens if we input a vector into our function. birthday_prob(1:20) ## Warning in 365:(365 - (n - 1)): numerical expression has 20 elements: only the ## first used ## [1] 0.0000000 0.9972603 0.9999925 1.0000000 1.0000000 1.0000000 1.0000000 ## [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## [15] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 It only uses the first value. Lets vectorize it. birthday_prob &lt;- Vectorize(birthday_prob) Now notice what happens. birthday_prob(1:20) ## [1] 0.000000000 0.002739726 0.008204166 0.016355912 0.027135574 0.040462484 ## [7] 0.056235703 0.074335292 0.094623834 0.116948178 0.141141378 0.167024789 ## [13] 0.194410275 0.223102512 0.252901320 0.283604005 0.315007665 0.346911418 ## [19] 0.379118526 0.411438384 We are good to go. Lets create our line plot, Figure 7.2. gf_line(birthday_prob(1:100)~ seq(1,100), xlab=&quot;Number of People&quot;, ylab=&quot;Probability of Match&quot;, title=&quot;Probability of at least 2 people with matching birthdays&quot;) %&gt;% gf_theme(theme_bw()) Figure 7.2: The probability of at least 2 people having mathcing birthdays Is this what you expected the curve to look like? We, the authors, did not expect this. It has a sigmodial shape with a large increase in the middle range and flatten in the tails. 7.4.6 Data science approach The final approach we will take is one based on data, a data science approach. In the mosaicData package is a data set called Births that contains the number of births in the US from 1969 to 1988. This data will allow us to estimate the number of births on any day of the year. This allows us to eliminate the reliance on the assumption that each day is equally likely. Lets first inspect() the data object. inspect(Births) ## Warning: `data_frame()` is deprecated as of tibble 1.1.0. ## Please use `tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## ## categorical variables: ## name class levels n missing ## 1 wday ordered 7 7305 0 ## distribution ## 1 Wed (14.3%), Thu (14.3%), Fri (14.3%) ... ## ## Date variables: ## name class first last min_diff max_diff n missing ## 1 date Date 1969-01-01 1988-12-31 1 days 1 days 7305 0 ## ## quantitative variables: ## name class min Q1 median Q3 max mean sd ## ...1 births integer 6675 8792 9622 10510 12851 9648.940178 1127.315229 ## ...2 year integer 1969 1974 1979 1984 1988 1978.501027 5.766735 ## ...3 month integer 1 4 7 10 12 6.522930 3.448939 ## ...4 day_of_year integer 1 93 184 275 366 183.753593 105.621885 ## ...5 day_of_month integer 1 8 16 23 31 15.729637 8.800694 ## ...6 day_of_week integer 1 2 4 6 7 4.000274 1.999795 ## n missing ## ...1 7305 0 ## ...2 7305 0 ## ...3 7305 0 ## ...4 7305 0 ## ...5 7305 0 ## ...6 7305 0 It could be argued that we could randomly pick one year and use it. Lets see what happens if we just used 1969. Figure 7.3 is a scatter plot of the number of births in 1969 for each day of the year. Births %&gt;% filter(year == 1969) %&gt;% gf_point(births~day_of_year) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;Day of the Year&quot;,y=&quot;Number of Births&quot;) Figure 7.3: The number of births for each day of the year in 1969 Exercise: What patterns do you see in Figure 7.3? What might explain them? There are definitely bands appearing in the data which could be the day of the week; there are less birthdays on the weekend. There is also seasonality with more birthdays in the summer and fall. There is also probably an impact from holidays. Quickly, lets look at the impact of day of the week by using color for day of the week. Figure 7.4 makes it clear that the weekends have less number of births as compared to the work week. Births %&gt;% filter(year == 1969) %&gt;% gf_point(births~day_of_year,color=~factor(day_of_week)) %&gt;% gf_labs(x=&quot;Day of the Year&quot;,col=&quot;Day of Week&quot;) %&gt;% gf_theme(theme_bw()) Figure 7.4: The number of births for each day of the year in 1969 broken down by day of the week By only using one year, this data might give poor results since holidays will fall on certain days of the week and the weekends will also be impacted. Note that we also still have the problem of leap years. Births %&gt;% group_by(year) %&gt;% summarise(n=n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 20 x 2 ## year n ## &lt;int&gt; &lt;int&gt; ## 1 1969 365 ## 2 1970 365 ## 3 1971 365 ## 4 1972 366 ## 5 1973 365 ## 6 1974 365 ## 7 1975 365 ## 8 1976 366 ## 9 1977 365 ## 10 1978 365 ## 11 1979 365 ## 12 1980 366 ## 13 1981 365 ## 14 1982 365 ## 15 1983 365 ## 16 1984 366 ## 17 1985 365 ## 18 1986 365 ## 19 1987 365 ## 20 1988 366 The years 1972, 1976, 1980, 1984, and 1988 are all leap years. At this point, to make the analysis easier, we will drop those years. Births %&gt;% filter(!(year %in% c(1972,1976,1980,1984,1988))) %&gt;% group_by(year) %&gt;% summarise(n=n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 15 x 2 ## year n ## &lt;int&gt; &lt;int&gt; ## 1 1969 365 ## 2 1970 365 ## 3 1971 365 ## 4 1973 365 ## 5 1974 365 ## 6 1975 365 ## 7 1977 365 ## 8 1978 365 ## 9 1979 365 ## 10 1981 365 ## 11 1982 365 ## 12 1983 365 ## 13 1985 365 ## 14 1986 365 ## 15 1987 365 Notice in filter() we used the %in% argument. This is a logical argument checking if year is one of the values. The ! at the front negates this in a sense requiring year not to be one of those values.` We are almost ready to simulate. We need to get the count of births on each day of the year for the non-leap years. birth_data &lt;- Births %&gt;% filter(!(year %in% c(1972,1976,1980,1984,1988))) %&gt;% group_by(day_of_year) %&gt;% summarise(n=sum(births)) head(birth_data) ## # A tibble: 6 x 2 ## day_of_year n ## &lt;int&gt; &lt;int&gt; ## 1 1 120635 ## 2 2 129042 ## 3 3 135901 ## 4 4 136298 ## 5 5 137319 ## 6 6 140044 Lets look at a plot of the number of births versus day of the year. We combined years in Figure 7.5. birth_data %&gt;% gf_point(n~day_of_year, xlab=&quot;Day of the year&quot;, ylab=&quot;Number of births&quot;) %&gt;% gf_theme(theme_bw()) Figure 7.5: Number of births by day of the year for all years. This curve has the seasonal cycling we would expect. The smaller scale cycling is unexpected. Maybe because we are dropping the leap years, we are getting some days appearing in our time interval more frequently on weekends. We leave it to you to investigate this phenomenon. We use these counts as weights in a sampling process. Days with more births will have a higher probability of being selected. Days such as Christmas and Christmas Eve have a lower probability of being selected. Lets save the weights in an object to use in the sample() function. birth_data_weights &lt;- birth_data %&gt;% select(n) %&gt;% pull() The pull() function pulls the vectors of values out of the data frame format into a vector format which the sample() needs. Now lets simulate the problem. The probability of a match should change slightly, maybe go down slightly?, but not much since most of the days have about the same probability or number of occurrences. set.seed(20) (do(1000)*length(unique(sample(days,size=18,replace = TRUE,prob=birth_data_weights)))) %&gt;% mutate(match=if_else(length==18,0,1)) %&gt;% summarise(prob=mean(match)) ## prob ## 1 0.352 We could not solve this problem of varying frequency of birth days using mathematics, at least as far as we know. Cool stuff, lets get to learning more about probability models in the next chapters. 7.5 Homework Problems Exactly 2 people with the same birthday - Simulation. Complete a similar analysis for case where exactly 2 people in a room of 23 people have the same birthday. In this exercise you will use a computational simulation. Create a new R Markdown file and create a report. Yes, we know you could use this file but we want you to practice generating your own report. Simulate having 23 people in the class with each day of the year equally likely. Find the cases where exactly 2 people have the same birthday, you will have to alter the code from the Notes more than changing 18 to 23. Plot the frequency of occurrences as a bar chart. Estimate the probability of exactly two people having the same birthday. Exactly 2 people with the same birthday - Mathematical. Repeat problem 1 but do it mathematically. As a big hint, you will need to use the choose() function. The idea is that with 23 people we need to choose 2 of them to match. We thus need to multiply, the multiplication rule again, by choose(23,2). If you are having trouble, work with a total of 3 people in the room first. Find a formula to determine the exact probability of exactly 2 people in a room of 23 having the same birthday. Generalize your solution to any number n people in the room and create a function. Vectorize the function. Plot the probability of exactly 2 people having the same birthday versus number of people in the room. Comment on the shape of the curve and explain it. The answer is around 34.7%, how close were you? Another question may be What does it mean at least two people have matching birthdays? It is possible that 3 people all have the same birthday or two sets of 2 people have the same birthday but different from the other pair. "],["PROBRULES.html", "Chapter 8 Probability Rules 8.1 Objectives 8.2 Probability vs Statistics 8.3 Basic probability terms 8.4 Probability 8.5 Counting rules 8.6 Homework Problems", " Chapter 8 Probability Rules 8.1 Objectives Define and use properly in context all new terminology related to probability to include but not limited to: outcome, event, sample space, probability. Apply basic probability and counting rules to find probabilities. Describe the basic axioms of probability. Use R to calculate and simulate probabilities of events. 8.2 Probability vs Statistics As a review, remember this course is divided into four general blocks: data collection/summary, probability models, inference and statistical modeling/prediction. This second block, probability, is the study of stochastic (random) processes and their properties. Specifically, we will explore random experiments. As its name suggests, a random experiment is an experiment whose outcome is not predictable with exact certainty. In the statistical models we develop in the last two blocks of this course, we will use other variables to explain the variance of the outcome of interest. Any remaining variance is modeled with probability models. Even though an outcome is determined by chance, this does not mean that we know nothing about the random experiment. Our favorite simple example is that of a coin flip. If we flip a coin, the possible outcomes are heads and tails. We dont know for sure what outcome will occur, but this doesnt mean we dont know anything about the experiment. If we assume the coin is fair, we know that each outcome is equally likely. Also, we know that if we flip the coin 100 times (independently), we are likely, the highest frequency event, to see around 50 heads, and very unlikely to see 10 heads or fewer. It is important to distinguish probability from inference and modeling. In probability, we consider a known random experiment, including knowing the parameters, and answer questions about what we expect to see from this random experiment. In statistics (inference and modeling), we consider data (the results of a mysterious random experiment) and infer about the underlying process. For example, suppose we have a coin and we are unsure whether this coin is fair or unfair, the parameter is unknown. We flipped it 20 times and it landed on heads 14 times. Inferential statistics will help us answer questions about the underlying process (could this coin be unfair?). This block (9 lessons or so) is devoted to the study of random experiments. First, we will explore simple experiments, counting rule problems, and conditional probability. Next, we will introduce the concept of a random variable and the properties of random variables. Following this, we will cover common distributions of discrete and continuous random variables. We will end the block on multivariate probability (joint distributions and covariance). 8.3 Basic probability terms We will start our work with some definitions and examples. 8.3.1 Sample space Suppose we have a random experiment. The sample space of this experiment, \\(S\\), is the set of all possible results of that experiment. For example, in the case of a coin flip, we could write \\(S=\\{H,T\\}\\). Each element of the sample space is considered an outcome. An event is a set of outcomes, it is a subset of the sample space. Example: Lets let R flip a coin for us and record the number of heads and tails. We will have R flip the coin twice. What is the sample space, what is an example of an outcome, and what is an example of an event. We will load the mosaic package as it has a function rflip() that will simulate flipping a coin. library(mosaic) set.seed(18) rflip(2) ## ## Flipping 2 coins [ Prob(Heads) = 0.5 ] ... ## ## H H ## ## Number of Heads: 2 [Proportion Heads: 1] The sample space is \\(S=\\{HH, TH, HT, TT\\}\\), an example of an outcome is \\(HH\\) which we see in the output from R, and finally an example of an event is the number of heads, which in this case takes on the values 0, 1, and 2. Another example of an event is At least one heads. In this case the event would be \\(\\{HH,TH, HT\\}\\). Also notice that \\(TH\\) is different from \\(HT\\) as an outcome; this is because those are different outcomes from flipping a coin twice. Example of Event: Suppose you arrive at a rental car counter and they show you a list of available vehicles, and one is picked for you at random. The sample space in this experiment is \\[ S=\\{\\mbox{red sedan}, \\mbox{blue sedan}, \\mbox{red truck}, \\mbox{grey truck}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}. \\] Each vehicle represents a possible outcome of the experiment. Let \\(A\\) be the event that a blue vehicle is selected. This event contains the outcomes blue sedan and blue SUV. 8.3.2 Union and intersection Suppose we have two events \\(A\\) and \\(B\\). \\(A\\) is considered a subset of \\(B\\) if all of the outcomes of \\(A\\) are also contained in \\(B\\). This is denoted as \\(A \\subset B\\). The intersection of \\(A\\) and \\(B\\) is all of the outcomes contained in both \\(A\\) and \\(B\\). This is denoted as \\(A \\cap B\\). The union of \\(A\\) and \\(B\\) is all of the outcomes contained in either \\(A\\) or \\(B\\), or both. This is denoted as \\(A \\cup B\\). The complement of \\(A\\) is all of the outcomes not contained in \\(A\\). This is denoted as \\(A^C\\) or \\(A&#39;\\). Note: Here we are treating events as sets and the above definitions are basic set operations. It is sometimes helpful when reading probability notation to think of Union as an or and Intersection as an and. Example: Consider our rental car example above. Let \\(A\\) be the event that a blue vehicle is selected, let \\(B\\) be the event that a black vehicle is selected, and let \\(C\\) be the event that an SUV is selected. First, lets list all of the outcomes of each event. \\(A = \\{\\mbox{blue sedan},\\mbox{blue SUV}\\}\\), \\(B=\\{\\mbox{black SUV}\\}\\), and \\(C= \\{\\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}\\). Since all outcomes in \\(B\\) are contained in \\(C\\), we know that \\(B\\) is a subset of \\(C\\), or \\(B\\subset C\\). Also, since \\(A\\) and \\(B\\) have no outcomes in common, \\(A \\cap B = \\emptyset\\). Further, \\(A \\cup C = \\{\\mbox{blue sedan}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}\\). 8.4 Probability Probability is a number assigned to an event or outcome that describes how likely it is to occur. A probability model assigns a probability to each element of the sample space. What makes a probability model is not just the values assigned to each element but the idea this model contains all the information about the outcomes and there are no other explanatory variables involved. A probability model can be thought of as a function that maps outcomes, or events, to a real number in the interval \\([0,1]\\). There are some basic axioms of probability you should know, although this list is not complete. Let \\(S\\) be the sample space of a random experiment and let \\(A\\) be an event where \\(A\\subset S\\). \\(\\mbox{P}(A) \\geq 0\\). \\(\\mbox{P}(S) = 1\\). These two axioms essentially say that probability must be positive, and the probability of all outcomes must sum to 1. 8.4.1 Probability properties Let \\(A\\) and \\(B\\) be events in a random experiment. Most of these can be proven fairly easily. \\(\\mbox{P}(\\emptyset)=0\\) \\(\\mbox{P}(A&#39;)=1-\\mbox{P}(A)\\) We used this in the case study. If \\(A\\subset B\\), then \\(\\mbox{P}(A)\\leq \\mbox{P}(B)\\). \\(\\mbox{P}(A\\cup B) = \\mbox{P}(A)+\\mbox{P}(B)-\\mbox{P}(A\\cap B)\\). This property can be generalized to more than two events. The intersection is subtracted because outcomes in both events \\(A\\) and \\(B\\) get counted twice in the first sum. Law of Total Probability: Let \\(B_1, B_2,...,B_n\\) be mutually exclusive, this means disjoint or no outcomes in common, and exhaustive, this means the union of all the events labeled with a \\(B\\) is the sample space. Then \\[ \\mbox{P}(A)=\\mbox{P}(A\\cap B_1)+\\mbox{P}(A\\cap B_2)+...+\\mbox{P}(A\\cap B_n) \\] A specific application of this law appears in Bayes Rule (more to follow). It says that \\(\\mbox{P}(A)=\\mbox{P}(A \\cap B)+\\mbox{P}(A \\cap B&#39;)\\). Essentially, it points out that \\(A\\) can be partitioned into two parts: 1) everything in \\(A\\) and \\(B\\) and 2) everything in \\(A\\) and not in \\(B\\). Example: Consider rolling a six sided die. Let event \\(A\\) be the number showing is less than 5. Let event \\(B\\) be the number is even. Then \\[\\mbox{P}(A)=\\mbox{P}(A \\cap B) + \\mbox{P}(A \\cap B&#39;)\\] \\[ \\mbox{P}(&lt; 5)=\\mbox{P}(&lt;5 \\cap Even)+\\mbox{P}(&lt;5 \\cap Odd) \\] DeMorgans Laws: \\[ \\mbox{P}((A \\cup B)&#39;)=\\mbox{P}(A&#39; \\cap B&#39;) \\] \\[ \\mbox{P}((A \\cap B)&#39;)=\\mbox{P}(A&#39; \\cup B&#39;) \\] 8.4.2 Equally likely scenarios In some random experiments, outcomes can be defined such that each individual outcome is equally likely. In this case, probability becomes a counting problem. Let \\(A\\) be an event in an experiment where each outcome is equally likely. \\[ \\mbox{P}(A)=\\frac{\\mbox{\\# of outcomes in A}}{\\mbox{\\# of outcomes in S}} \\] Example: Suppose a family has three children, with each child being either a boy (B) or girl (G). Assume that the likelihood of boys and girls are equal and independent, this is the idea that the probability of the gender of the second child does not change based on the gender of the first child. The sample space can be written as: \\[ S=\\{\\mbox{BBB},\\mbox{BBG},\\mbox{BGB},\\mbox{BGG},\\mbox{GBB},\\mbox{GBG},\\mbox{GGB},\\mbox{GGG}\\} \\] What is the probability that the family has exactly 2 girls? This only happens in three ways: BGG, GBG, and GGB. Thus, the probability of exactly 2 girls is 3/8 or 0.375. 8.4.3 Using R (Equally likely scenarios) The previous example above is an example of an Equally Likely scenario, where the sample space of a random experiment contains a list of outcomes that are equally likely. In these cases, we can sometimes use R to list out the possible outcomes and count them to determine probability. We can also use R to simulate. Example: Use R to simulate the family of three children where each child has the same probability of being a boy or a girl. Instead of writing our own function, we can use rflip() in the mosaic package. We will let \\(H\\) stand for girl. First simulate one family. set.seed(73) rflip(3) ## ## Flipping 3 coins [ Prob(Heads) = 0.5 ] ... ## ## T T H ## ## Number of Heads: 1 [Proportion Heads: 0.333333333333333] In this case we got 1 girl. Next we will use the do() function to repeat this simulation. results &lt;- do(10000)*rflip(3) head(results) ## n heads tails prop ## 1 3 1 2 0.3333333 ## 2 3 3 0 1.0000000 ## 3 3 3 0 1.0000000 ## 4 3 3 0 1.0000000 ## 5 3 1 2 0.3333333 ## 6 3 1 2 0.3333333 Next we can visualize the distribution of the number of girls, heads, in Figure 8.1. results %&gt;% gf_bar(~heads) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;NUmber of girls&quot;,y=&quot;Count&quot;) Figure 8.1: Number of girls in a family of size 3. Finally we can estimate the probability of exactly 2 girls. We need the tidyverse library. library(tidyverse) results %&gt;% filter(heads==2) %&gt;% summarize(prob=n()/10000) ## prob ## 1 0.3782 Or slightly different code. results %&gt;% count(heads) %&gt;% mutate(prop=n/sum(n)) ## heads n prop ## 1 0 3723 0.1241 ## 2 1 11358 0.3786 ## 3 2 11346 0.3782 ## 4 3 3573 0.1191 Not a bad estimate of the exact probability. Lets now use an example of cards to simulate some probabilities as well as learning more about counting. The file Cards.csv contains the data for cards from a 52 card deck. Lets read it in and summarize. Cards &lt;- read_csv(&quot;data/Cards.csv&quot;) inspect(Cards) ## ## categorical variables: ## name class levels n missing ## 1 rank character 13 52 0 ## 2 suit character 4 52 0 ## distribution ## 1 10 (7.7%), 2 (7.7%), 3 (7.7%) ... ## 2 Club (25%), Diamond (25%) ... ## ## quantitative variables: ## name class min Q1 median Q3 max ## ...1 probs numeric 0.01923077 0.01923077 0.01923077 0.01923077 0.01923077 ## mean sd n missing ## ...1 0.01923077 0 52 0 head(Cards) ## # A tibble: 6 x 3 ## rank suit probs ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Club 0.0192 ## 2 3 Club 0.0192 ## 3 4 Club 0.0192 ## 4 5 Club 0.0192 ## 5 6 Club 0.0192 ## 6 7 Club 0.0192 We can see 4 suits, and 13 ranks, the value on the face of the card. Example: Suppose we draw one card out of a standard deck. Let \\(A\\) be the event that we draw a Club. Let \\(B\\) be the event that we draw a 10 or a face card (Jack, Queen, King or Ace). We can use R to define these events and find probabilities. Lets find all the Clubs. Cards %&gt;% filter(suit == &quot;Club&quot;) %&gt;% select(rank,suit) ## # A tibble: 13 x 2 ## rank suit ## &lt;chr&gt; &lt;chr&gt; ## 1 2 Club ## 2 3 Club ## 3 4 Club ## 4 5 Club ## 5 6 Club ## 6 7 Club ## 7 8 Club ## 8 9 Club ## 9 10 Club ## 10 J Club ## 11 Q Club ## 12 K Club ## 13 A Club So just by counting, we find the probability of drawing a Club is \\(\\frac{13}{52}\\) or 0.25. We can do this by simulation, this is over kill but gets the idea of simulation across. Remember, ask what do we want R to do and what does R need to do this? results &lt;- do(10000)*sample(Cards,1) head(results) ## # A tibble: 6 x 6 ## rank suit probs orig.id .row .index ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9 Spade 0.0192 47 1 1 ## 2 5 Club 0.0192 4 1 2 ## 3 5 Spade 0.0192 43 1 3 ## 4 7 Heart 0.0192 32 1 4 ## 5 4 Club 0.0192 3 1 5 ## 6 A Spade 0.0192 52 1 6 results %&gt;% filter(suit == &quot;Club&quot;) %&gt;% summarize(prob=n()/10000) ## # A tibble: 1 x 1 ## prob ## &lt;dbl&gt; ## 1 0.243 results %&gt;% count(suit) %&gt;% mutate(prob=n/sum(n)) ## # A tibble: 4 x 3 ## suit n prob ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Club 2432 0.243 ## 2 Diamond 2558 0.256 ## 3 Heart 2417 0.242 ## 4 Spade 2593 0.259 Now lets count the number of outcomes in \\(B\\). Cards %&gt;% filter(rank %in% c(10, &quot;J&quot;, &quot;Q&quot;, &quot;K&quot;, &quot;A&quot;)) %&gt;% select(rank,suit) ## # A tibble: 20 x 2 ## rank suit ## &lt;chr&gt; &lt;chr&gt; ## 1 10 Club ## 2 J Club ## 3 Q Club ## 4 K Club ## 5 A Club ## 6 10 Diamond ## 7 J Diamond ## 8 Q Diamond ## 9 K Diamond ## 10 A Diamond ## 11 10 Heart ## 12 J Heart ## 13 Q Heart ## 14 K Heart ## 15 A Heart ## 16 10 Spade ## 17 J Spade ## 18 Q Spade ## 19 K Spade ## 20 A Spade So just by counting, we find the probability of drawing a 10 or greater is \\(\\frac{20}{52}\\) or 0.3846154. Exercise: Using simulation to estimate the probability of 10 or higher. results &lt;- do(10000)*sample(Cards,1) head(results) ## # A tibble: 6 x 6 ## rank suit probs orig.id .row .index ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 10 Heart 0.0192 35 1 1 ## 2 6 Heart 0.0192 31 1 2 ## 3 8 Spade 0.0192 46 1 3 ## 4 J Heart 0.0192 36 1 4 ## 5 Q Spade 0.0192 50 1 5 ## 6 10 Club 0.0192 9 1 6 results %&gt;% filter(rank %in% c(10, &quot;J&quot;, &quot;Q&quot;, &quot;K&quot;, &quot;A&quot;)) %&gt;% summarize(prob=n()/10000) ## # A tibble: 1 x 1 ## prob ## &lt;dbl&gt; ## 1 0.389 Notice that this code is not robust to change the number of simulations. If we change from 10000, then we have to change the denominator in the summarize() function. We can change this by using mutate() instead of filter(). results %&gt;% mutate(face=rank %in% c(10, &quot;J&quot;, &quot;Q&quot;, &quot;K&quot;, &quot;A&quot;))%&gt;% summarize(prob=mean(face)) ## # A tibble: 1 x 1 ## prob ## &lt;dbl&gt; ## 1 0.389 Notice that in the mutate() function, we are creating a new logical variable called face. This variable takes on the values of TRUE and FALSE. In the next line we use a summarize() command with the function mean(). In R a function that requires numeric input takes a logical variable and converts the TRUE into 1 and the FALSE into 0. Thus the mean() will find the proportion of TRUE values and that is why we report it as a probability. Next, lets find a card that is 10 or greater and a club. Cards %&gt;% filter(rank %in% c(10, &quot;J&quot;, &quot;Q&quot;, &quot;K&quot;, &quot;A&quot;),suit==&quot;Club&quot;) %&gt;% select(rank,suit) ## # A tibble: 5 x 2 ## rank suit ## &lt;chr&gt; &lt;chr&gt; ## 1 10 Club ## 2 J Club ## 3 Q Club ## 4 K Club ## 5 A Club We find the probability of drawing a 10 or greater club is \\(\\frac{5}{52}\\) or 0.0961538. Exercise: Simulate drawing one card and estimate the probability of a club that is 10 or greater. results %&gt;% mutate(face=(rank %in% c(10, &quot;J&quot;, &quot;Q&quot;, &quot;K&quot;, &quot;A&quot;))&amp;(suit==&quot;Club&quot;))%&gt;% summarize(prob=mean(face)) ## # A tibble: 1 x 1 ## prob ## &lt;dbl&gt; ## 1 0.0963 8.4.4 Note We have been using R to count the number of outcomes in an event. This helped us to determine probabilities. We limited the problems to simple ones. In our cards example, it would be more interesting for us to explore more complex events such as drawing 5 cards from a standard deck. Each draw of 5 cards is equally likely, so in order to find the probability of a flush (5 cards of the same suit), we could simply list all the possible flushes and compare that to the sample space. Because of the large number of possible outcomes, this becomes difficult. Thus we need to explore counting rules in more detail to help us solve more complex problems. In this course we will limit our discussion to three basic cases. You should know that there are entire courses on discrete math and counting rules, so we will still be limited in our methods and the type of problems we can solve in this course. 8.5 Counting rules There are three types of counting problems we will consider. In each case, the multiplication rule is being used and all that changes is whether an element is allowed to be reused, replacement, and whether the order of selection matters. This latter question is difficult. Each case will be demonstrated with an example. 8.5.1 Multiplication rule 1: Order matters, sample with replacement The multiplication rule is at the center of each of the three methods. In this first case we are using the idea that order matters and items can be reused. Lets use an example to help. Example: A license plate consists of three numeric digits (0-9) followed by three single letters (A-Z). How many possible license plates exist? We can divide this problem into two sections. In the numeric section, we are selecting 3 objects from 10, with replacement. This means that a number can be used more than once. Order clearly matters because a license plate starting with 432 is distinct from a license plate starting with 234. There are \\(10^3 = 1000\\) ways to select the first three digits; 10 for the first, 10 for the second, and 10 for the third. Why do you multiply and not add?65 In the alphabet section, we are selecting 3 objects from 26, where order matters. Thus, there are \\(26^3=17576\\) ways to select the last three letters of the plate. Combined, there are \\(10^3 \\times 26^3 = 17576000\\) ways to select license plates. Visually, \\[ \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} = 17,576,000 \\] Next we are going to use this new counting method to find a probability. Exercise: What is the probability a license plate starts with the number 8 or 0 and ends with the letter B? In order to find this probability, we simply need to determine the number of ways to select a license plate starting with 8 or 0 and ending with the letter B. We can visually represent this event: \\[ \\underbrace{\\underline{\\quad 2 \\quad }}_\\text{8 or 0} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 10 \\quad }}_\\text{number} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 26 \\quad }}_\\text{letter} \\times \\underbrace{\\underline{\\quad 1 \\quad }}_\\text{B} = 135,200 \\] Dividing this number by the total number of possible license plates yields the probability of this event occurring. denom&lt;-10*10*10*26*26*26 num&lt;-2*10*10*26*26*1 num/denom ## [1] 0.007692308 The probability of obtaining a license plate starting with 8 or 0 and ending with B is 0.0077. Simulating this would be difficult because we would need special functions to check the first number and last letter. This gets into text mining an important subject in data science but unfortunately we dont have much time in this course for the topic. 8.5.2 Multiplication rule 2 (Permutation): Order Matters, Sampling Without Replacement Consider a random experiment where we sample from a group of size \\(n\\), without replacement, and the outcome of the experiment depends on the order of the outcomes. The number of ways to select \\(k\\) objects is given by \\(n(n-1)(n-2)...(n-k+1)\\). This is known as a permutation and is sometimes written as \\[ {}_nP_{k} = \\frac{n!}{(n-k)!} \\] Recall that \\(n!\\) is read as \\(n\\) factorial and represents the number of ways to arrange \\(n\\) objects. Example: Twenty-five friends participate in a Halloween costume party. Three prizes are given during the party: most creative costume, scariest costume, and funniest costume. No one can win more than one prize. How many possible ways can the prizes by distributed? There are \\(k=3\\) prizes to be assigned to \\(n=25\\) people. Once someone is selected for a prize, they are removed from the pool of eligibles. In other words, we are sampling without replacement. Also, order matters. For example, if Tom, Mike, and Jane, win most creative, scariest and funniest costume, respectively, this is a different outcome than if Mike won creative, Jane won scariest and Tom won funniest. Thus, the number of ways the prizes can be distributed is given by \\({}_{25}P_3 = \\frac{25!}{22!} = 13,800\\). A more visually pleasing way to express this would be: \\[ \\underbrace{\\underline{\\quad 25 \\quad }}_\\text{most creative} \\times \\underbrace{\\underline{\\quad 24 \\quad }}_\\text{scariest} \\times \\underbrace{\\underline{\\quad 23 \\quad }}_\\text{funniest} = 13,800 \\] Notice that it is sometime difficult to determine if order matters or not in a problem, but in this example the name of the prize was a hint that indeed order matters. Lets use the idea of a permutation to calculate a probability. Exercise: Assume that all 25 participants are equally likely to win any one of the three prizes. What is the probability that Tom doesnt win any of them? Just like in the previous probability calculation, we simply need to count the number of ways Tom doesnt win any prize. In other words, we need to count the number of ways that prizes are distributed without Tom. So, remove Tom from the group of 25 eligible participants. The number of ways Tom doesnt get a prize is \\({}_{24}P_3 = \\frac{24!}{21!}=12,144\\). Again visually: \\[ \\underbrace{\\underline{\\quad 24 \\quad }}_\\text{most creative} \\times \\underbrace{\\underline{\\quad 23 \\quad }}_\\text{scariest} \\times \\underbrace{\\underline{\\quad 22 \\quad }}_\\text{funniest} = 12,144 \\] The probability Tom doesnt get a prize is simply the second number divided by the first: denom&lt;-factorial(25)/factorial(25-3) # Or, denom&lt;-25*24*23 num&lt;-24*23*22 num/denom ## [1] 0.88 8.5.3 Multiplication rule 3 (Combination): Order Does Not Matter, Sampling Without Replacement Consider a random experiment where we sample from a group of size \\(n\\), without replacement, and the outcome of the experiment does not depend on the order of the outcomes. The number of ways to select \\(k\\) objects is given by \\(\\frac{n!} {(n-k)!k!}\\). This is known as a combination and is written as: \\[ \\binom{n}{k} = \\frac{n!}{(n-k)!k!} \\] This is read as \\(n\\) choose \\(k\\). Take a moment to compare combinations to permutations, discussed in Rule 2. The difference between these two rules is that in a combination, order no longer matters. A combination is equivalent to a permutation divided by \\(k!\\), the number of ways to arrange the \\(k\\) objects selected. Example: Suppose we draw 5 cards out of a standard deck (52 cards, no jokers). How many possible 5 card hands are there? In this example, order does not matter. I dont care if I receive 3 jacks then 2 queens or 2 queens then 3 jacks. Either way, its the same collection of 5 cards. Also, we are drawing without replacement. Once a card is selected, it cannot be selected again. Thus, the number of ways to select 5 cards is given by: \\[ \\binom{52}{5} = \\frac{52!}{(52-5)!5!} = 2,598,960 \\] Example: When drawing 5 cards, what is the probability of drawing a flush (5 cards of the same suit)? Lets determine how many ways to draw a flush. There are four suits (clubs, hearts, diamonds and spades). Each suit has 13 cards. We would like to pick 5 of those 13 cards and 0 of the remaining 39. Lets consider just one of those suits (clubs): \\[ \\mbox{P}(\\mbox{5 clubs})=\\frac{\\binom{13}{5}\\binom{39}{0}}{\\binom{52}{5}} \\] The second part of the numerator (\\(\\binom{39}{0}\\)) isnt necessary, since it simply represents the number of ways to select 0 objects from a group (1 way), but it helps clearly lay out the events. This brings up the point of what \\(0!\\) equals. By definition it is 1. This allows us to use \\(0!\\) in our work. Now, we expand this to all four suits by multiplying by 4, or \\(\\binom{4}{1}\\) since we are selecting 1 suit out of the 4: \\[ \\mbox{P}(\\mbox{flush})=\\frac{\\binom{4}{1}\\binom{13}{5}\\binom{39}{0}}{\\binom{52}{5}} \\] num&lt;-4*choose(13,5)*1 denom&lt;-choose(52,5) num/denom ## [1] 0.001980792 There is a probability of 0.0020 of drawing a flush in a draw of 5 cards from a standard deck of cards. Exercise: When drawing 5 cards, what is the probability of drawing a full house (3 cards of the same rank and the other 2 of the same rank)? This problem uses several ideas from this lesson. We need to pick the rank of the three of a kind. Then pick 3 cards from the 4 possible. Next we pick the rank of the pair from the remaining 12 ranks. Finally pick 2 cards of that rank from the 4 possible. \\[ \\mbox{P}(\\mbox{full house})=\\frac{\\binom{13}{1}\\binom{4}{3}\\binom{12}{1}\\binom{4}{2}}{\\binom{52}{5}} \\] num&lt;-choose(13,1)*choose(4,3)*choose(12,1)*choose(4,2) denom&lt;-choose(52,5) num/denom ## [1] 0.001440576 Why not use \\(\\binom{13}{2}\\) instead of \\(\\binom{13}{1}\\binom{12}{1}\\)?66 We have just determined that a full house has a lower probability of occurring than a flush. This is why in gambling, a flush is valued less than a full house. 8.6 Homework Problems 1. Let \\(A\\), \\(B\\) and \\(C\\) be events such that \\(\\mbox{P}(A)=0.5\\), \\(\\mbox{P}(B)=0.3\\), and \\(\\mbox{P}(C)=0.4\\). Also, we know that \\(\\mbox{P}(A \\cap B)=0.2\\), \\(\\mbox{P}(B \\cap C)=0.12\\), \\(\\mbox{P}(A \\cap C)=0.1\\), and \\(\\mbox{P}(A \\cap B \\cap C)=0.05\\). Find the following: \\(\\mbox{P}(A\\cup B)\\) \\(\\mbox{P}(A\\cup B \\cup C)\\) \\(\\mbox{P}(B&#39;\\cap C&#39;)\\) \\(\\mbox{P}(A\\cup (B\\cap C))\\) \\(\\mbox{P}((A\\cup B \\cup C)\\cap (A\\cap B \\cap C)&#39;)\\) 2. Consider the example of the family in the reading. What is the probability that the family has at least one boy? 3. The Birthday Problem Revisited. Suppose there are \\(n=20\\) students in a classroom. My birthday, the instructor, is April 3rd. What is the probability that at least one student shares my birthday? Assume only 365 days in a year and assume that all birthdays are equally likely. In R, find the probability that at least one other person shares my birthday for each value of \\(n\\) from 1 to 80. Plot these probabilities with \\(n\\) on the \\(x\\)-axis and probability on the \\(y\\)-axis. At what value of \\(n\\) would the probability be at least 50%? 4. Thinking of the cards again. Answer the following questions: Define two events that are mutually exclusive. Define two events that are independent. Define an event and its complement. 5. Consider the license plate example from the reading. What is the probability that a license plate contains exactly one B? What is the probability that a license plate contains at least one B? 6. Consider the party example in the reading. Suppose 8 people showed up to the party dressed as zombies. What is the probability that all three awards are won by people dressed as zombies? What is the probability that zombies win most creative and funniest but not scariest? 7. Consider the cards example from the reading. How many ways can we obtain a two pairs (2 of one number, 2 of another, and the final different)? What is the probability of drawing a four of a kind (four cards of the same value)? 8. Advanced Question: Consider rolling 5 dice. What is the probability of a pour resulting in a full house? Multiplication is repeated adding so in a sense we are adding. However in a more serious tone, for this problem for every first number there are 10 possibilities for the second number and for every second number there are 10 possibilities for the third numbers. This is multiplication. Because this implies the order selection of the ranks does not matter. In other words, this assumes that for example 3 Kings and 2 fours is the same full house as 3 fours and 2 Kings. This is not true so we break the rank selection about essentially making it a permutation. "],["CONDPROB.html", "Chapter 9 Conditional Probability 9.1 Objectives 9.2 Conditional Probability 9.3 Independence 9.4 Bayes Rule 9.5 Homework Problems", " Chapter 9 Conditional Probability 9.1 Objectives Define conditional probability and distinguish it from joint probability. Find a conditional probability using its definition. Using conditional probability, determine whether two events are independent. Apply Bayes Rule mathematically and via simulation. 9.2 Conditional Probability So far, weve covered the basic axioms of probability, the properties of events (set theory) and counting rules. Another important concept, perhaps one of the most important, is conditional probability. Often, we know a certain event or sequence of events has occurred and we are interested in the probability of another event. Example: Suppose you arrive at a rental car counter and they show you a list of available vehicles, and one is picked for you at random. The sample space in this experiment is \\[ S=\\{\\mbox{red sedan}, \\mbox{blue sedan}, \\mbox{red truck}, \\mbox{grey truck}, \\mbox{grey SUV}, \\mbox{black SUV}, \\mbox{blue SUV}\\}. \\] What is the probability that a blue vehicle is selected, given a sedan was selected? Since we know that a sedan was selected, our sample space has been reduced to just red sedan and blue sedan. The probability of selecting a blue vehicle out of this sample space is simply 1/2. In set notation, let \\(A\\) be the event that a blue vehicle is selected. Let \\(B\\) be the event that a sedan is selected. We are looking for \\(\\mbox{P}(A \\mbox{ given } B)\\), which is also written as \\(\\mbox{P}(A|B)\\). By definition, \\[ \\mbox{P}(A|B)=\\frac{\\mbox{P}(A \\cap B)}{\\mbox{P}(B)} \\] It is important to distinguish between the event \\(A|B\\) and \\(A \\cap B\\). This is a common misunderstanding about probability. \\(A \\cap B\\) is the event that an outcome was selected at random from the total sample space, and that outcome was contained in both \\(A\\) and \\(B\\). On the other hand, \\(A|B\\) assumes the \\(B\\) has occurred, and an outcome was drawn from the remaining sample space, and that outcome was contained in \\(A\\). Another common misunderstanding involves the direction of conditional probability. Specifically, \\(A|B\\) is NOT the same event as \\(B|A\\). For example, consider a medical test for a disease. The probability that someone tests positive given they had the disease is different than the probability that someone has the disease given they tested positive. We will explore this example further in our Bayes Rule section. 9.3 Independence Two events, \\(A\\) and \\(B\\), are said to be independent if the probability of one occurring does not change whether or not the other has occurred. We looked at this last lesson but now we have another way of looking at it using conditional probabilities. For example, lets say the probability that a randomly selected student has seen the latest superhero movie is 0.55. What if we randomly select a student and we see that he/she is wearing a black backpack? Does that probability change? Likely not, since movie attendance is probably not related to choice of backpack color. These two events are independent. Mathematically, \\(A\\) and \\(B\\) are considered independent if and only if \\[ \\mbox{P}(A|B)=\\mbox{P}(A) \\] Result: \\(A\\) and \\(B\\) are independent if and only if \\[ \\mbox{P}(A\\cap B)=\\mbox{P}(A)\\mbox{P}(B) \\] This follows from the definition of conditional probability and from above: \\[ \\mbox{P}(A|B)=\\frac{\\mbox{P}(A\\cap B)}{\\mbox{P}(B)}=\\mbox{P}(A) \\] Thus, \\(\\mbox{P}(A\\cap B)=\\mbox{P}(A)\\mbox{P}(B)\\). Example: Consider the example above. Recall events \\(A\\) and \\(B\\). Let \\(A\\) be the event that a blue vehicle is selected. Let \\(B\\) be the event that a sedan is selected. Are \\(A\\) and \\(B\\) independent? No. First, recall that \\(\\mbox{P}(A|B)=0.5\\). The probability of selecting a blue vehicle (\\(\\mbox{P}(A)\\)) is \\(2/7\\) (the number of blue vehicles in our sample space divided by 7, the total number vehicles in \\(S\\)). This value is different from 0.5; thus, \\(A\\) and \\(B\\) are not independent. We could also use the result above to determine whether \\(A\\) and \\(B\\) are independent. Note that \\(\\mbox{P}(A)= 2/7\\). Also, we know that \\(\\mbox{P}(B)=2/7\\). So, \\(\\mbox{P}(A)\\mbox{P}(B)=4/49\\). But, \\(\\mbox{P}(A\\cap B) = 1/7\\), since there is just one blue sedan in the sample space. \\(4/49\\) is not equal to \\(1/7\\); thus, \\(A\\) and \\(B\\) are not independent. 9.4 Bayes Rule As mentioned in the introduction to this section, \\(\\mbox{P}(A|B)\\) is not the same quantity as \\(\\mbox{P}(B|A)\\). However, if we are given information about \\(A|B\\) and \\(B\\), we can use Bayes Rule to find \\(\\mbox{P}(B|A)\\). Let \\(B_1, B_2, ..., B_n\\) be mutually exclusive and exhaustive events and let \\(\\mbox{P}(A)&gt;0\\). Then, \\[ \\mbox{P}(B_k|A)=\\frac{\\mbox{P}(A|B_k)\\mbox{P}(B_k)}{\\sum_{i=1}^n \\mbox{P}(A|B_i)\\mbox{P}(B_i)} \\] Lets use an example to dig into where this comes from. Example: Suppose a doctor has developed a blood test for a certain rare disease (only one out of every 10,000 people have this disease). After careful and extensive evaluation of this blood test, the doctor determined the tests sensitivity and specificity. Sensitivity is the probability of detecting the disease for those who actually have it. Note that this is a conditional probability. Specificity is the probability of correctly identifying no disease for those who do not have it. Again, another conditional probability. In fact, this test had a sensitivity of 100% and a specificity of 99.9%. Now suppose a patient walks in, the doctor administers the blood test, and it returns positive. What is the probability that that patient actually has the disease? This is a classic example of how probability could be misunderstood. Upon reading this question, you might guess that the answer to our question is quite high. After all, this is a nearly perfect test. After exploring the problem more in depth, we find a different result. 9.4.1 Approach using whole numbers Without going directly to the formulaic expression above, lets consider a collection of 100,000 randomly selected people. What do we know? Based on the prevalence of this disease (one out of every 10,000 people have this disease), we know that 10 of them should have the disease. This test is perfectly sensitive. Thus, of the 10 people that have the disease, all of them test positive. This test has a specificity of 99.9%. Of the 99,990 that dont have the disease, \\(0.999*99990\\approx 99890\\) will test negative. The remaining 100 will test positive. Thus, of our 100,000 randomly selected people, 110 will test positive. Of these 110, only 10 actually have the disease. Thus, the probability that someone has the disease given theyve tested positive is actually around \\(10/110 = 0.0909\\). 9.4.2 Mathematical approach Now lets put this in context of Bayes Rule as stated above. First, lets define some events. Let \\(D\\) be the event that someone has the disease. Thus, \\(D&#39;\\) would be the event that someone does not have the disease. Similarly, let \\(T\\) be the event that someone has tested positive. What do we already know? \\[ \\mbox{P}(D) = 0.0001 \\hspace{1cm} \\mbox{P}(D&#39;)=0.9999 \\] \\[ \\mbox{P}(T|D)= 1 \\hspace{1cm} \\mbox{P}(T&#39;|D)=0 \\] \\[ \\mbox{P}(T&#39;|D&#39;)=0.999 \\hspace{1cm} \\mbox{P}(T|D&#39;) = 0.001 \\] We are looking for \\(\\mbox{P}(D|T)\\), the probability that someone has the disease, given he/she has tested positive. By the definition of conditional probability, \\[ \\mbox{P}(D|T)=\\frac{\\mbox{P}(D \\cap T)}{\\mbox{P}(T)} \\] The numerator can be rewritten, again utilizing the definition of conditional probability: \\(\\mbox{P}(D\\cap T)=\\mbox{P}(T|D)\\mbox{P}(D)\\). The denominator can be rewritten using the Law of Total Probability (discussed in Lesson 2) and then the definition of conditional probability: \\(\\mbox{P}(T)=\\mbox{P}(T\\cap D) + \\mbox{P}(T \\cap D&#39;) = \\mbox{P}(T|D)\\mbox{P}(D) + \\mbox{P}(T|D&#39;)\\mbox{P}(D&#39;)\\). So, putting it all together, \\[ \\mbox{P}(D|T)=\\frac{\\mbox{P}(T|D)\\mbox{P}(D)}{\\mbox{P}(T|D)\\mbox{P}(D) + \\mbox{P}(T|D&#39;)\\mbox{P}(D&#39;)} \\] Now we have stated our problem in the context of quantities we know: \\[ \\mbox{P}(D|T)=\\frac{1\\cdot 0.0001}{1\\cdot 0.0001 + 0.001\\cdot 0.9999} = 0.0909 \\] Note that in the original statement of Bayes Rule, we considered \\(n\\) partitions, \\(B_1, B_2,...,B_n\\). In this example, we only have two: \\(D\\) and \\(D&#39;\\). 9.4.3 Simulation To do the simulation, we can think of it as flipping a coin. First lets assume we are pulling 1,000,000 people from the population. The probability that any one person has the disease is 0.0001. We will use rflip() to get the 1,000,000 people and designate as no disease or disease. set.seed(43) results &lt;- rflip(1000000,0.0001,summarize = TRUE) results ## n heads tails prob ## 1 1e+06 100 999900 1e-04 In this case 100 people had the disease. Now lets find the positive test results. Of the 100 with the disease, all will test positive. Of those without disease, there is a 0.001 probability of testing positive. rflip(as.numeric(results[&#39;tails&#39;]),prob=.001,summarize = TRUE) ## n heads tails prob ## 1 999900 959 998941 0.001 Now 959 tested positive. Thus the probability of having the disease given a positive test result is approximately: 100/(100+959) ## [1] 0.09442871 9.5 Homework Problems 1. Consider: \\(A\\), \\(B\\) and \\(C\\) are events such that \\(\\mbox{P}(A)=0.5\\), \\(\\mbox{P}(B)=0.3\\), \\(\\mbox{P}(C)=0.4\\), \\(\\mbox{P}(A \\cap B)=0.2\\), \\(\\mbox{P}(B \\cap C)=0.12\\), \\(\\mbox{P}(A \\cap C)=0.1\\), and \\(\\mbox{P}(A \\cap B \\cap C)=0.05\\). Are \\(A\\) and \\(B\\) independent? Are \\(B\\) and \\(C\\) independent? 2. Suppose I have a biased coin (the probability I flip a heads is 0.6). I flip that coin twice. Assume that the coin is memoryless (flips are independent of one another). What is the probability that the second flip results in heads? What is the probability that the second flip results in heads, given the first also resulted in heads? What is the probability both flips result in heads? What is the probability exactly one coin flip results in heads? Now assume I flip the coin five times. What is the probability the result is 5 heads? What is the probability the result is exactly 2 heads (out of 5 flips)? 3. Suppose there are three assistants working at a company: Moe, Larry and Curly. All three assist with a filing process. Only one filing assistant is needed at a time. Moe assists 60% of the time, Larry assists 30% of the time and Curly assists the remaining 10% of the time. Occasionally, they make errors (misfiles); Moe has a misfile rate of 0.01, Larry has a misfile rate of 0.025, and Curly has a rate of 0.05. Suppose a misfile was discovered, but it is unknown who was on schedule when it occurred. Who is most likely to have committed the misfile? Calculate the probabilities for each of the three assistants. 4. You are playing a game where there are two coins. One coin is fair and the other comes up heads 80% of the time. One coin is flipped 3 times and the result is three heads, what is the probability that the coin flipped is the fair coin? You will need to make an assumption about the probability of either coin being selected. Use Bayes formula to solve this problem. Use simulation to solve this problem. "],["RANDVAR.html", "Chapter 10 Random Variables 10.1 Objectives 10.2 Random variables 10.3 Moments 10.4 Homework Problems", " Chapter 10 Random Variables 10.1 Objectives Define and use properly in context all new terminology. Given a discrete random variable, obtain the pmf and cdf, and use them to obtain probabilities of events. Simulate random variable for a discrete distribution. Find the moments of a discrete random variable. Find the expected value of a linear transformation of a random variable. 10.2 Random variables We have already discussed random experiments. We have also discussed \\(S\\), the sample space for an experiment. A random variable essentially maps the events in the sample space to the real number line. For a formal definition: A random variable \\(X\\) is a function \\(X: S\\rightarrow \\mathbb{R}\\) that assigns exactly one number to each outcome in an experiment. Example: Suppose you flip a coin three times. The sample space, \\(S\\), of this experiment is \\[ S=\\{\\mbox{HHH}, \\mbox{HHT}, \\mbox{HTH}, \\mbox{HTT}, \\mbox{THH}, \\mbox{THT}, \\mbox{TTH}, \\mbox{TTT}\\} \\] Let the random variable \\(X\\) be the number of heads in three coin flips. Whenever introduced to a new random variable, you should take a moment to think about what possible values can \\(X\\) take? When tossing a coin 3 times, we can get no heads, one head, two heads or three heads. The random variable \\(X\\) assigns each outcome in our experiment to one of these values. Visually: \\[ S=\\{\\underbrace{\\mbox{HHH}}_{X=3}, \\underbrace{\\mbox{HHT}}_{X=2}, \\underbrace{\\mbox{HTH}}_{X=2}, \\underbrace{\\mbox{HTT}}_{X=1}, \\underbrace{\\mbox{THH}}_{X=2}, \\underbrace{\\mbox{THT}}_{X=1}, \\underbrace{\\mbox{TTH}}_{X=1}, \\underbrace{\\mbox{TTT}}_{X=0}\\} \\] The sample space of \\(X\\), the support, is the list of numerical values that \\(X\\) can take. \\[ S_X=\\{0,1,2,3\\} \\] Because the sample space of \\(X\\) is a countable list of numbers, we consider \\(X\\) to be a discrete random variable (more on that later). 10.2.1 How does this help? Sticking with our example, we can now frame a problem of interest in the context of our random variable \\(X\\). For example, suppose we wanted to know the probability of at least two heads. Without our random variable, we have to write this as: \\[ \\mbox{P}(\\mbox{at least two heads})= \\mbox{P}(\\{\\mbox{HHH},\\mbox{HHT},\\mbox{HTH},\\mbox{THH}\\}) \\] In the context of our random variable, this simply becomes \\(\\mbox{P}(X\\geq 2)\\). It may not seem important in a case like this, but imagine if we were flipping a coin 50 times and wanted to know the probability of obtaining at least 30 heads. It would be unfeasible to write out all possible ways to obtain at least 30 heads. It is much easier to write \\(\\mbox{P}(X\\geq 30)\\) and explore the distribution of \\(X\\) (more on that in Lesson 6). Essentially, a random variable often helps us reduce a complex random experiment to a simple variable that is easy to characterize. 10.2.2 Discrete vs Continuous A discrete random variable has a sample space that consists of a countable set of values. \\(X\\) in our example above is a discrete random variable. Note that countable does not necessarily mean finite. For example, a random variable with a Poisson distribution (a topic for a later lesson) has a sample space of \\(\\{0,1,2,...\\}\\). This sample space stretches to infinity, but it is considered countably infinite, and thus the random variable would be considered discrete. A continuous random variable has a sample space that is a continuous interval. For example, let \\(Y\\) be the random variable corresponding to the height of a randomly selected individual. \\(Y\\) is a continuous random variable because a person could measure 68.1 inches, 68.2 inches, or perhaps any value in between. Note that when we measure height, our precision is limited by our measuring device, so we are technically discretizing height. However, even in these cases, we typically consider height to be a continuous random variable. A mixed random variable is exactly what it sounds like. It has a sample space that is both discrete and continuous. How could such a thing occur? Consider an experiment where a person rolls a standard six-sided die. If it lands on anything other than one, the result of the die roll is recorded. If it lands on one, the person spins a wheel, and the angle in degrees of the resulting spin, divided by 360, is recorded. If our random variable \\(Z\\) is the number that is recorded in this experiment, the sample space of \\(Z\\) is \\([0,1] \\cup \\{2,3,4,5,6\\}\\). We will not be spending much time on mixed random variables. However they do occur in practice, consider the job of analyzing bomb data error. If the bomb hits within a certain radius, the error is 0. Otherwise it is measured in a radial direction. This data is mixed. 10.2.3 Discrete distribution functions Once we have defined a random variable, we need a way to describe its behavior and we will use probabilities for this purpose. Distribution functions describe the behavior of random variables. We can use these functions to determine the probability that a random variable takes a value or a range of values. For discrete random variables, there are two distribution functions of interest: the probability mass function (pmf) and the cumulative distribution function (cdf). 10.2.4 Probability mass function Let \\(X\\) be a discrete random variable. The probability mass function (pmf) of \\(X\\), given by \\(f_X(x)\\) is a function that assigns probability to each possible outcome of \\(X\\). \\[ f_X(x)=\\mbox{P}(X=x) \\] Note that the pmf is a function. Functions have input and output. The input of a pmf is any real number. The output of a pmf is the probability that the random variable takes the inputted value. The pmf must follow the axioms of probability described in the Probability Rules lesson. Primarily, For all \\(x \\in \\mathbb{R}\\), \\(0 \\leq f_X(x) \\leq 1\\). \\(\\sum_x f_X(x) = 1\\), where the \\(x\\) in the index of the sum simply denotes that we are summing across the entire domain or support of \\(X\\). Example: Recall our example again. You flip a coin three times and let \\(X\\) be the number of heads in those three coin flips. We know that \\(X\\) can only take values 0, 1, 2 or 3. But at what probability does it take these three values? In that example, we had listed out the possible outcomes of the experiment and denoted what value of \\(X\\) corresponds to each outcome. \\[ S=\\{\\underbrace{\\mbox{HHH}}_{X=3}, \\underbrace{\\mbox{HHT}}_{X=2}, \\underbrace{\\mbox{HTH}}_{X=2}, \\underbrace{\\mbox{HTT}}_{X=1}, \\underbrace{\\mbox{THH}}_{X=2}, \\underbrace{\\mbox{THT}}_{X=1}, \\underbrace{\\mbox{TTH}}_{X=1}, \\underbrace{\\mbox{TTT}}_{X=0}\\} \\] Each of these eight outcomes is equally likely (each with a probability of \\(\\frac{1}{8}\\)). Thus, building the pmf of \\(X\\) becomes a matter of counting the number of outcomes associated with each possible value of \\(X\\): \\[ f_X(x)=\\left\\{ \\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} \\frac{1}{8}, &amp; x=0 \\\\ \\frac{3}{8}, &amp; x=1 \\\\ \\frac{3}{8}, &amp; x=2 \\\\ \\frac{1}{8}, &amp; x=3 \\\\ 0, &amp; \\mbox{otherwise} \\end{array} \\right . \\] Note that this function specifies the probability that \\(X\\) takes any of the four values in the sample space (0, 1, 2, and 3). Also, it specifies that the probability that \\(X\\) takes any other value is 0. Graphically, the pmf is not terribly interesting. The pmf is 0 at all values of \\(X\\) except for 0, 1, 2 and 3, Figure 10.1. Figure 10.1: Probability Mass Function of \\(X\\) from Coin Flip Example Example: We can use a pmf to answer questions about an experiment. For example, consider the same context. What is the probability that we flip at least one heads? We can write this in the context of \\(X\\): \\[ \\mbox{P}(\\mbox{at least one heads})=\\mbox{P}(X\\geq 1)=\\mbox{P}(X=1)+\\mbox{P}(X=2)+\\mbox{P}(X=3)=\\frac{3}{8} + \\frac{3}{8}+\\frac{1}{8}=\\frac{7}{8} \\] Alternatively, we can recognize that \\(\\mbox{P}(X\\geq 1)=1-\\mbox{P}(X=0)=1-\\frac{1}{8}=\\frac{7}{8}\\). 10.2.5 Cumulative distribution function Let \\(X\\) be a discrete random variable. The cumulative distribution function (cdf) of \\(X\\), given by \\(F_X(x)\\) is a function that assigns to each value of \\(X\\) the probability that \\(X\\) takes that value or lower: \\[ F_X(x)=\\mbox{P}(X\\leq x) \\] Again, note that the cdf is a function with an input and output. The input of a cdf is any real number. The output of a cdf is the probability that the random variable takes the inputted value or less. If we know the pmf, we can obtain the cdf: \\[ F_X(x)=\\mbox{P}(X\\leq x)=\\sum_{y\\leq x} f_X(y) \\] Like the pmf, the cdf must be between 0 and 1. Also, since the pmf is always non-negative, the cdf must be non-decreasing. Example: Obtain and plot the cdf of \\(X\\) of the previous example. \\[ F_X(x)=\\mbox{P}(X\\leq x)=\\left\\{\\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} 0, &amp; x &lt;0 \\\\ \\frac{1}{8}, &amp; 0\\leq x &lt; 1 \\\\ \\frac{4}{8}, &amp; 1\\leq x &lt; 2 \\\\ \\frac{7}{8}, &amp; 2\\leq x &lt; 3 \\\\ 1, &amp; x\\geq 3 \\end{array}\\right . \\] Visually, the cdf of a discrete random variable has a stairstep appearance. In this example, the cdf takes a value 0 up until \\(X=0\\), at which point the cdf increases to 1/8. It stays at this value until \\(X=1\\), and so on. At and beyond \\(X=3\\), the cdf is equal to 1, Figure 10.2. Figure 10.2: Cumulative Distribution Function of \\(X\\) from Coin Flip Example 10.2.6 Simulating random variables We can simulate values from a random variable using the cdf, we will use a similar idea for continuous random variables. Since the range of the cdf is in the interval \\([0,1]\\) we will generate a random number in that same interval and then use the inverse function to find the value of the random variable. The pseudo code is: 1) Generate a random number, \\(U\\). 2) Find the index \\(k\\) such that \\(\\sum_{j=1}^{k-1}x_{j} \\leq U &lt; \\sum_{j=1}^{k}x_{j}\\) or \\(F_x(k-1) \\leq U &lt; F_{x}(k)\\). Example: Simulate a random variable for the number of heads in flipping a coin three times. First we will create the pmf. pmf &lt;- c(1/8,3/8,3/8,1/8) values &lt;- c(0,1,2,3) pmf ## [1] 0.125 0.375 0.375 0.125 We get the cdf from the cumulative sum. cdf &lt;- cumsum(pmf) cdf ## [1] 0.125 0.500 0.875 1.000 Next, we will generate a random number between 0 and 1. set.seed(1153) ran_num &lt;- runif(1) ran_num ## [1] 0.7381891 Finally, we will find the value of the random variable. We will do each step separately first so you can understand the code. ran_num &lt; cdf ## [1] FALSE FALSE TRUE TRUE which(ran_num &lt; cdf) ## [1] 3 4 which(ran_num &lt; cdf)[1] ## [1] 3 values[which(ran_num &lt; cdf)[1]] ## [1] 2 Lets make this a function. simple_rv &lt;- function(values,cdf){ ran_num &lt;- runif(1) return(values[which(ran_num &lt; cdf)[1]]) } Now lets generate 10000 values from this random variable. results &lt;- do(10000)*simple_rv(values,cdf) inspect(results) ## ## quantitative variables: ## name class min Q1 median Q3 max mean sd n missing ## ...1 simple_rv numeric 0 1 2 2 3 1.5048 0.860727 10000 0 tally(~simple_rv,data=results,format=&quot;proportion&quot;) ## simple_rv ## 0 1 2 3 ## 0.1207 0.3785 0.3761 0.1247 Not a bad approximation. 10.3 Moments Distribution functions are excellent characterizations of random variables. The pmf and cdf will tell you exactly how often the random variables takes particular values. However, distribution functions are often a lot of information. Sometimes, we may want to describe a random variable \\(X\\) with a single value or small set of values. For example, we may want to know the average or some measure of center of \\(X\\). We also may want to know a measure of spread of \\(X\\). Moments are values that summarize random variables with single numbers. Since we are dealing with the population, these moments are population values and not summary statistics as we used in the first block of material. 10.3.1 Expectation At this point, we should define the term expectation. Let \\(g(X)\\) be some function of a discrete random variable \\(X\\). The expected value of \\(g(X)\\) is given by: \\[ \\mbox{E}(g(X))=\\sum_x g(x) \\cdot f_X(x) \\] 10.3.2 Mean The most common moments used to describe random variables are mean and variance. The mean (often referred to as the expected value of \\(X\\)), is simply the average value of a random variable. It is denoted as \\(\\mu_X\\) or \\(\\mbox{E}(X)\\). In the discrete case, the mean is found by: \\[ \\mu_X=\\mbox{E}(X)=\\sum_x x \\cdot f_X(x) \\] The mean is also known as the first moment of \\(X\\) around the origin. It is a weighted sum with the weight being the probability. If each outcome were equally likely, the expected value would just be the average of the values of the random variable since each wight is the reciprocal of the number of values. Example: Find the expected value (or mean) of \\(X\\): the number of heads in three flips of a fair coin. \\[ \\mbox{E}(X)=\\sum_x x\\cdot f_X(x) = 0*\\frac{1}{8} + 1*\\frac{3}{8} + 2*\\frac{3}{8} + 3*\\frac{1}{8}=1.5 \\] We are using \\(\\mu\\) because it is a population parameter. From our simulation above, we can find the mean as an estimate of the expected value. This is really a statistic since our simulation is data from the population and thus will have variance from sample to sample. mean(~simple_rv,data=results) ## [1] 1.5048 10.3.3 Variance Variance is a measure of spread of a random variable. The variance of \\(X\\) is denoted as \\(\\sigma^2_X\\) or \\(\\mbox{Var}(X)\\). It is equivalent to the average squared deviation from the mean: \\[ \\sigma^2_X=\\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2] \\] In the discrete case, this can be evaluated by: \\[ \\mbox{E}[(X-\\mu_X)^2]=\\sum_x (x-\\mu_X)^2f_X(x) \\] Variance is also known as the second moment of \\(X\\) around the mean. The square root of \\(\\mbox{Var}(X)\\) is denoted as \\(\\sigma_X\\), the standard deviation of \\(X\\). The standard deviation is often reported because it is measured in the same units as \\(X\\), while the variance is measured in squared units and is thus harder to interpret. Example: Find the variance of \\(X\\): the number of heads in three flips of a fair coin. \\[ \\mbox{Var}(X)=\\sum_x (x-\\mu_X)^2 \\cdot f_X(x) = (0-1.5)^2 \\times \\frac{1}{8} + (1-1.5)^2 \\times \\frac{3}{8}+(2-1.5)^2 \\times \\frac{3}{8} + (3-1.5)^2\\times \\frac{1}{8} \\] (0-1.5)^2*1/8 + (1-1.5)^2*3/8 + (2-1.5)^2*3/8 + (3-1.5)^2*1/8 ## [1] 0.75 The variance of \\(X\\) is 0.75. We can find the variance of the simulation but R uses the sample variance and this is the population variance. So we need to multiply by \\(\\frac{n-1}{n}\\) var(~simple_rv,data=results)*(10000-1)/10000 ## [1] 0.740777 10.3.4 Mean and variance of Linear Transformations Lemma: Let \\(X\\) be a discrete random variable, and let \\(a\\) and \\(b\\) be constants. Then: \\[ \\mbox{E}(aX+b)=a\\mbox{E}(X)+b \\] and \\[ \\mbox{Var}(aX+b)=a^2\\mbox{Var}(X) \\] The proof of this is left as an Application problem. 10.4 Homework Problems 1. Suppose we are flipping a fair coin, and the result of a single coin flip is either heads or tails. Let \\(X\\) be a random variable representing the number of flips until the first heads. Is \\(X\\) discrete or continuous? What is the domain, support, of \\(X\\)? What values do you expect \\(X\\) to take? What do you think is the average of \\(X\\)? Dont actually do any formal math, just think about if you were flipping a regular coin, how long it would take you to get the first heads. Advanced: In R, generate 10,000 observations from \\(X\\). What is the empirical, from the simulation, pmf? What is the average value of \\(X\\) based on this simulation? Create a bar chart of the proportions. Note: Unlike the example in the Notes, we dont have the pmf, so you will have to simulate the experiment and using R to find the number of flips until the first heads. Note: There are many ways to do this. Below is a description of one approach. It assumes we are extremely unlikely to go past 1000 flips. First, lets sample with replacement from the vector c(H,T), 1000 times with replacement, use sample(). As we did in the reading, use which() and a logical argument to find the first occurrence of a heads. Find the theoretical distribution, use math to come up with a closed for solution for the pmf. 2. Repeat Problem 1,except part d, but with a different random variable, \\(Y\\): the number of coin flips until the fifth heads. 3. Suppose you are a data analyst for a large international airport. Your boss, the head of the airport, is dismayed that this airport has received negative attention in the press for inefficiencies and sluggishness. In a staff meeting, your boss gives you a week to build a report addressing the timeliness at the airport. Your boss is in a big hurry and gives you no further information or guidance on this task. Prior to building the report, you will need to conduct some analysis. To aid you in this, create a list of at least three random variables that will help you address timeliness at the airport. For each of your random variables, Determine whether it is discrete or continuous. Report its domain. What is the experimental unit? Explain how this random variable will be useful in addressing timeliness at the airport. We will provide one example: Let \\(D\\) be the difference between a flights actual departure and its scheduled departure. This is a continuous random variable, since time can be measured in fractions of minutes. A flight can be early or late, so domain is any real number. The experimental unit is each individual (non-canceled) flight. This is a useful random variable because the average value of \\(D\\) will describe whether flights take off on time. We could also find out how often \\(D\\) exceeds 0 (implying late departure) or how often \\(D\\) exceeds 30 minutes, which could indicate a very late departure. 4. Consider the experiment of rolling two fair six-sided dice. Let the random variable \\(Y\\) be the absolute difference between the two numbers that appear upon rolling the dice. What is the domain/support of \\(Y\\)? What values do you expect \\(Y\\) to take? What do you think is the average of \\(Y\\)? Dont actually do any formal math, just think about the experiment. Find the probability mass function and cumulative distribution function of \\(Y\\). Find the expected value and variance of \\(Y\\). Advanced: In R, obtain 10,000 realizations of \\(Y\\). In other words, simulate the roll of two fair dice, record the absolute difference and repeat this 10,000 times. Construct a frequency table of your results (what percentage of time did you get a difference of 0? difference of 1? etc.) Find the mean and variance of your simulated sample of \\(Y\\). Were they close to your answers in part d? 5. Prove the Lemma from the Notes: Let \\(X\\) be a discrete random variable, and let \\(a\\) and \\(b\\) be constants. Show \\(\\mbox{E}(aX + b)=a\\mbox{E}(X)+b\\). 6. In the Notes, we saw that \\(\\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]\\). Show that \\(\\mbox{Var}(X)\\) is also equal to \\(\\mbox{E}(X^2)-[\\mbox{E}(X)]^2\\). "],["CONRANDVAR.html", "Chapter 11 Continuous Random Variables 11.1 Objectives 11.2 Continuous random variables 11.3 Moments 11.4 Homework Problems", " Chapter 11 Continuous Random Variables 11.1 Objectives Define and properly use the new terms to include probability density function (pdf) and cumulative distribution function (cdf) for continuous random variables. Given a continuous random variable, find probabilities using the pdf and/or the cdf. Find the mean and variance of a continuous random variable. 11.2 Continuous random variables In the last lesson, we introduced random variables, and explored discrete random variables. In this lesson, we will move into continuous random variables, their properties, their distribution functions, and how they differ from discrete random variables. Recall that a continuous random variable has a domain that is a continuous interval (or possibly a group of intervals). For example, let \\(Y\\) be the random variable corresponding to the height of a randomly selected individual. While our measurement will necessitate discretizing height to some degree, technically, height is a continuous random variable since a person could measure 67.3 inches or 67.4 inches or anything in between. 11.2.1 Continuous distribution functions So how do we describe the randomness of continuous random variables? In the case of discrete random variables, the probability mass function (pmf) and the cumulative distribution function (cdf) are used to describe randomness. However, recall that the pmf is a function that returns the probability that the random variable takes the inputted value. Due to the nature of continuous random variables, the probability that a continuous random variable takes on any one individual value is technically 0. Thus, a pmf cannot apply to a continuous random variable. Rather, we describe the randomness of continuous random variables with the probability density function (pdf) and the cumulative distribution function (cdf). Note that the cdf has the same interpretation and application as in the discrete case. 11.2.2 Probability density function Let \\(X\\) be a continuous random variable. The probability density function (pdf) of \\(X\\), given by \\(f_X(x)\\) is a function that describes the behavior of \\(X\\). It is important to note that in the continuous case, \\(f_X(x)\\neq \\mbox{P}(X=x)\\), as the probability of \\(X\\) taking any one individual value is 0. The pdf is a function. The input of a pdf is any real number. The output is known as the density. The pdf has three main properties: \\(f_X(x)\\geq 0\\) \\(\\int_{S_X} f_X(x)\\mathop{}\\!\\mathrm{d}x = 1\\) \\(\\mbox{P}(X\\in A)=\\int_{x\\in A} f_X(x)\\mathop{}\\!\\mathrm{d}x\\) or another way to write this \\(\\mbox{P}(a \\leq X \\leq b)=\\int_{a}^{b} f_X(x)\\mathop{}\\!\\mathrm{d}x\\) Properties 2) and 3) imply that the area underneath a pdf represents probability. The pdf is a non-negative function, it cannot have negative values. 11.2.3 Cumulative distribution function The cumulative distribution function (cdf) of a continuous random variable has the same interpretation as it does for a discrete random variable. It is a function. The input of a cdf is any real number, and the output is the probability that the random variable takes a value less than or equal to the inputted value. It is denoted as \\(F\\) and is given by: \\[ F_X(x)=\\mbox{P}(X\\leq x)=\\int_{-\\infty}^x f_x(t) \\mathop{}\\!\\mathrm{d}t \\] Example: Let \\(X\\) be a continuous random variable with \\(f_X(x)=2x\\) where \\(0 \\leq x \\leq 1\\). Verify that \\(f\\) is a valid pdf. Find the cdf of \\(X\\). Also, find the following probabilities: \\(\\mbox{P}(X&lt;0.5)\\), \\(\\mbox{P}(X&gt;0.5)\\), and \\(\\mbox{P}(0.1\\leq X &lt; 0.75)\\). Finally, find the median of \\(X\\). To verify that \\(f\\) is a valid pdf, we simply note that \\(f_X(x) \\geq 0\\) on the range \\(0 \\leq x \\leq 1\\). Also, we note that \\(\\int_0^1 2x \\mathop{}\\!\\mathrm{d}x = x^2\\bigg|_0^1 = 1\\). Using R, we find integrate(function(x)2*x,0,1) ## 1 with absolute error &lt; 1.1e-14 Or we can use the mosaicCalc package to find the anti-derivative. If the package is not installed, you can use the Packages tab in RStudio or type install.packages(\"mosaicCalc\") at the command prompt. Load the library. library(mosaicCalc) (Fx&lt;-antiD(2*x~x)) ## function (x, C = 0) ## 1 * x^2 + C Fx(1)-Fx(0) ## [1] 1 Graphically, the pdf is displayed in Figure 11.1: Figure 11.1: pdf of \\(X\\) The cdf of \\(X\\) is found by \\[ \\int_0^x 2t \\mathop{}\\!\\mathrm{d}t = t^2\\bigg|_0^x = x^2 \\] This is antiD found from the calculations above. So, \\[ F_X(x)=\\left\\{ \\begin{array}{ll} 0, &amp; x&lt;0 \\\\ x^2, &amp; 0\\leq x \\leq 1 \\\\ 1, &amp; x&gt;1 \\end{array}\\right. \\] The plot of the cdf of \\(X\\) is shown in Figure 11.2. Figure 11.2: cdf of \\(X\\) Probabilities are found either by integrating the pdf or using the cdf: \\(\\mbox{P}(X &lt; 0.5)=\\mbox{P}(X\\leq 0.5)=F_X(0.5)=0.5^2=0.25\\). See Figure 11.3. Figure 11.3: Probability represented by shaded area \\(\\mbox{P}(X &gt; 0.5) = 1-\\mbox{P}(X\\leq 0.5)=1-0.25 = 0.75\\) See Figure 11.4. Figure 11.4: Probability represented by shaded area \\(\\mbox{P}(0.1\\leq X &lt; 0.75) = \\int_{0.1}^{0.75}2x\\mathop{}\\!\\mathrm{d}x = 0.75^2 - 0.1^2 = 0.553\\) See Figure 11.5. integrate(function(x)2*x,.1,.75) ## 0.5525 with absolute error &lt; 6.1e-15 Alternatively, \\(\\mbox{P}(0.1\\leq X &lt; 0.75) = \\mbox{P}(X &lt; 0.75) -\\mbox{P}(x \\leq 0.1) = F(0.75)-F(0.1)=0.75^2-0.1^2 =0.553\\) Fx(0.75)-Fx(0.1) ## [1] 0.5525 Notice for a continuous random variable, we are loose with the use of the = sign. This is because for a continuous random variable \\(\\mbox{P}(X=x)=0\\). Do not get sloppy when working with discrete random variables. Figure 11.5: Probability represented by shaded area The median of \\(X\\) is the value \\(x\\) such that \\(\\mbox{P}(X\\leq x)=0.5\\), the area under a single point is 0. So we simply solve \\(x^2=0.5\\) for \\(x\\). Thus, the median of \\(X\\) is \\(\\sqrt{0.5}=0.707\\). Or using R uniroot(function(x)(Fx(x)-.5),c(0,1))$root ## [1] 0.7071067 11.2.4 Simulation As in the case of the discrete random variable, we can simulate a continuous random variable if we have an inverse for the cdf. The range of the cdf is \\([0,1]\\), so we generate a random number in this interval and then apply the inverse cdf to obtain a random variable. In a similar manner, for a continuous random variable, we use the following pseudo code: 1. Generate a random number in the interval \\([0,1]\\), \\(U\\). 2. Find the random variable \\(X\\) from \\(F_{X}^{-1}(U)\\). In R for our example, this looks like the following. sqrt(runif(1)) ## [1] 0.969391 results &lt;- do(10000)*sqrt(runif(1)) inspect(results) ## ## quantitative variables: ## name class min Q1 median Q3 max mean ## ...1 sqrt numeric 0.004158405 0.5037432 0.7091864 0.8690241 0.9999794 0.6688691 ## sd n missing ## ...1 0.2349856 10000 0 Figure 11.6 is a density plot of the simulated density function. results %&gt;% gf_density(~sqrt,xlab=&quot;X&quot;) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;X&quot;,y=&quot;&quot;) Figure 11.6: Density plot of the simulated random varialbe. 11.3 Moments As with discrete random variables, moments can be calculated to summarize characteristics such as center and spread. In the discrete case, expectation is found by multiplying each possible value by its associated probability and summing across the domain (\\(\\mbox{E}(X)=\\sum_x x\\cdot f_X(x)\\)). In the continuous case, the domain of \\(X\\) consists of an infinite set of values. From your calculus days, recall that the sum across an infinite domain is represented by an integral. Let \\(g(X)\\) be any function of \\(X\\). The expectation of \\(g(X)\\) is found by: \\[ \\mbox{E}(g(X)) = \\int_{S_X} g(x)f_X(x)\\mathop{}\\!\\mathrm{d}x \\] 11.3.1 Mean and variance Let \\(X\\) be a continuous random variable. The mean of \\(X\\), or \\(\\mu_X\\), is simply \\(\\mbox{E}(X)\\). Thus, \\[ \\mbox{E}(X)=\\int_{S_X}x\\cdot f_X(x)\\mathop{}\\!\\mathrm{d}x \\] As in the discrete case, the variance of \\(X\\) is the expected squared difference from the mean, or \\(\\mbox{E}[(X-\\mu_X)^2]\\). Thus, \\[ \\sigma^2_X = \\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]= \\int_{S_X} (x-\\mu_X)^2\\cdot f_X(x) \\mathop{}\\!\\mathrm{d}x \\] Recall Application problem 6 from last lesson. In this problem, you showed that \\(\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2\\). Thus, \\[ \\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2 = \\int_{S_X} x^2\\cdot f_X(x)\\mathop{}\\!\\mathrm{d}x - \\mu_X^2 \\] Example: Consider the random variable \\(X\\) from above. Find the mean and variance of \\(X\\). \\[ \\mu_X= \\mbox{E}(X)=\\int_0^1 x\\cdot 2x\\mathop{}\\!\\mathrm{d}x = \\frac{2x^3}{3}\\bigg|_0^1 = \\frac{2}{3}=0.667 \\] Side note: Since the mean of \\(X\\) is smaller than the median of \\(X\\), we say that \\(X\\) is skewed to the left, or negatively skewed. Using R. integrate(function(x)x*2*x,0,1) ## 0.6666667 with absolute error &lt; 7.4e-15 Or using antiD() Ex&lt;-antiD(2*x^2~x) Ex(1)-Ex(0) ## [1] 0.6666667 Using our simulation. mean(~sqrt,data=results) ## [1] 0.6688691 \\[ \\sigma^2_X = \\mbox{Var}(X)= \\mbox{E}(X^2)-\\mbox{E}(X)^2 = \\int_0^1 x^2\\cdot 2x\\mathop{}\\!\\mathrm{d}x - \\left(\\frac{2}{3}\\right)^2 = \\frac{2x^4}{4}\\bigg|_0^1-\\frac{4}{9}=\\frac{1}{2}-\\frac{4}{9}=\\frac{1}{18}=0.056 \\] integrate(function(x)x^2*2*x,0,1)$value-(2/3)^2 ## [1] 0.05555556 or Vx&lt;-antiD(x^2*2*x~x) Vx(1)-Vx(0)-(2/3)^2 ## [1] 0.05555556 var(~sqrt,data=results)*9999/10000 ## [1] 0.05521269 And finally, the standard deviation of \\(X\\) is \\(\\sigma_X = \\sqrt{\\sigma^2_X}=\\sqrt{1/18}=0.236\\). 11.4 Homework Problems 1. Let \\(X\\) be a continuous random variable on the domain \\(-k \\leq X \\leq k\\). Also, let \\(f(x)=\\frac{x^2}{18}\\). Assume that \\(f(x)\\) is a valid pdf. Find the value of \\(k\\). Plot the pdf of \\(X\\). Find and plot the cdf of \\(X\\). Find \\(\\mbox{P}(X&lt;1)\\). Find \\(\\mbox{P}(1.5&lt;X\\leq 2.5)\\). Find the 80th percentile of \\(X\\) (the value \\(x\\) for which 80% of the distribution is to the left of that value). Find the value \\(x\\) such that \\(\\mbox{P}(-x \\leq X \\leq x)=0.4\\). Find the mean and variance of \\(X\\). Simulate 10000 values from this distribution and plot the density. 2. Let \\(X\\) be a continuous random variable. Prove that the cdf of \\(X\\), \\(F_X(x)\\) is a non-decreasing function. (Hint: show that for any \\(a &lt; b\\), \\(F_X(a) \\leq F_X(b)\\).) "],["DISCRETENAMED.html", "Chapter 12 Named Discrete Distributions 12.1 Objectives 12.2 Named distributions 12.3 Homework Problems", " Chapter 12 Named Discrete Distributions 12.1 Objectives Recognize and setup for use common discrete distributions (Uniform, Binomial, Poisson, Hypergeometric) to include parameters, assumptions, and moments. Use R to calculate probabilities and quantiles involving random variables with common discrete distributions. 12.2 Named distributions In the previous two lessons, we introduced the concept of random variables, distribution functions, and expectations. In some cases, the nature of an experiment may yield random variables with common distributions. In these cases, we can rely on easy-to-use distribution functions and built-in R functions in order to calculate probabilities and quantiles. 12.2.1 Discrete uniform distribution The first distribution we will discuss is the discrete uniform distribution. It is not a very commonly used distribution, especially compared to its continuous counterpart. A discrete random variable has the discrete uniform distribution if probability is evenly allocated to each value in the sample space. A variable with this distribution has parameters \\(a\\) and \\(b\\) representing the minimum and maximum of the sample space, respectively. (By default, that sample space is assumed to consist of integers only, but that is by no means always the case.) Example: Rolling a fair die is an example of the discrete uniform. Each side of the die has an equal probability. Let \\(X\\) be a discrete random variable with the uniform distribution. If the sample space is consecutive integers, this distribution is denoted as \\(X\\sim\\textsf{DUnif}(a,b)\\). The pmf of \\(X\\) is given by: \\[ f_X(x)=\\left\\{\\begin{array}{ll}\\frac{1}{b-a+1}, &amp; x \\in \\{a, a+1,...,b\\} \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] For the die: \\[ f_X(x)=\\left\\{\\begin{array}{ll}\\frac{1}{6-1+1} = \\frac{1}{6}, &amp; x \\in \\{1, 2,...,6\\} \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] The expected value of \\(X\\) is found by: \\[ \\mbox{E}(X)=\\sum_{x=a}^b x\\cdot\\frac{1}{b-a+1}= \\frac{1}{b-a+1} \\cdot \\sum_{x=a}^b x=\\frac{1}{b-a+1}\\cdot\\frac{b-a+1}{2}\\cdot (a+b) = \\frac{a+b}{2} \\] Where the sum of consecutive integers is a common result from discrete math, research it for more information. The variance of \\(X\\) is found by: (derivation not included) \\[ \\mbox{Var}(X)=\\mbox{E}[(X-\\mbox{E}(X))^2]=\\frac{(b-a+1)^2-1}{12} \\] Summarizing for the die: Let \\(X\\) be the result of a single roll of a fair die. We will report the distribution of \\(X\\), the pmf, \\(\\mbox{E}(X)\\) and \\(\\mbox{Var}(X)\\). The sample space of \\(X\\) is \\(S_X=\\{1,2,3,4,5,6\\}\\). Since each of those outcomes is equally likely, \\(X\\) follows the discrete uniform distribution with \\(a=1\\) and \\(b=6\\). Thus, \\[ f_X(x)=\\left\\{\\begin{array}{ll}\\frac{1}{6}, &amp; x \\in \\{1,2,3,4,5,6\\} \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] Finally, \\(\\mbox{E}(X)=\\frac{1+6}{2}=3.5\\). Also, \\(\\mbox{Var}(X)=\\frac{(6-1+1)^2-1}{12}=\\frac{35}{12}=2.917\\). 12.2.2 Simulating To simulate the discrete uniform, we use sample(). Example: To simulate rolling a die 4 times, we use sample(). set.seed(61) sample(1:6,4,replace=TRUE) ## [1] 4 2 2 1 Lets roll it 10,000 times and find results&lt;-do(10000)*sample(1:6,1,replace=TRUE) tally(~sample,data=results,format=&quot;percent&quot;) ## sample ## 1 2 3 4 5 6 ## 16.40 16.46 16.83 17.15 16.92 16.24 mean(~sample,data=results) ## [1] 3.5045 var(~sample,data=results)*(10000-1)/10000 ## [1] 2.87598 Again as a reminder, we multiply by \\(\\frac{(10000-1)}{10000}\\) because the function var() is calculating a sample variance using \\(n-1\\) in the denominator but we need the population variance. 12.2.3 Binomial distribution The binomial distribution is extremely common, and appears in many situations. In fact, we have already discussed several examples where the binomial distribution is heavily involved. Consider an experiment involving repeated independent trials of a binary process (two outcomes), where in each trial, there is a constant probability of success (one of the outcomes which is arbitrary). If the random variable \\(X\\) represents the number of successes out of \\(n\\) independent trials, then \\(X\\) is said to follow the binomial distribution with parameters \\(n\\) and \\(p\\) (the probability of a success in each trial). The pmf of \\(X\\) is given by: \\[ f_X(x)=\\mbox{P}(X=x)={n\\choose{x}}p^x(1-p)^{n-x} \\] for \\(x \\in \\{0,1,...,n\\}\\) and 0 otherwise. Lets take a moment to dissect this pmf. We are looking for the probability of obtaining \\(x\\) successes out of \\(n\\) trials. The \\(p^x\\) represents the probability of \\(x\\) successes, using the multiplication rule because of the independence assumption. The term \\((1-p)^{n-x}\\) represents the probability of the remainder of the trials as failures. Finally, the \\(n\\choose x\\) term represents the number of ways to obtain \\(x\\) successes out of \\(n\\) trials. For example, there are three ways to obtain 1 success out of 3 trials (one success followed by two failures; one success, one failure then one success; or two failures followed by a success). The expected value of a binomially distributed random variable is given by \\(\\mbox{E}(X)=np\\) and the variance is given by \\(\\mbox{Var}(X)=np(1-p)\\). Example: Let \\(X\\) be the number of heads out of 20 independent flips of a fair coin. Note that this is a binomial because the trials are independent and the probability of success, in this case a heads, is constant, and there are two outcomes. Find the distribution, mean and variance of \\(X\\). Find \\(\\mbox{P}(X=8)\\). Find \\(\\mbox{P}(X\\leq 8)\\). \\(X\\) has the binomial distribution with \\(n=20\\) and \\(p=0.5\\). The pmf is given by: \\[ f_X(x)=\\mbox{P}(X=x)={20 \\choose x}0.5^x (1-0.5)^{20-x} \\] Also, \\(\\mbox{E}(X)=20*0.5=10\\) and \\(\\mbox{Var}(X)=20*0.5*0.5=5\\). To find \\(\\mbox{P}(X=8)\\), we can simply use the pmf: \\[ \\mbox{P}(X=8)=f_X(8)={20\\choose 8}0.5^8 (1-0.5)^{12} \\] choose(20,8)*0.5^8*(1-0.5)^12 ## [1] 0.1201344 To find \\(\\mbox{P}(X\\leq 8)\\), we would need to find the cumulative probability: \\[ \\mbox{P}(X\\leq 8)=\\sum_{x=0}^8 {20\\choose 8}0.5^x (1-0.5)^{20-x} \\] x&lt;-0:8 sum(choose(20,x)*0.5^x*(1-.5)^(20-x)) ## [1] 0.2517223 12.2.4 Software Functions One of the advantages of using named distributions is that most software packages have built-in functions that compute probabilities and quantiles for common named distributions. Over the course of this lesson, you will notice that each named distribution is treated similarly in R. There are four main functions tied to each distribution. For the binomial distribution, these are dbinom(), pbinom(), qbinom(), and rbinom(). dbinom(): This function is equivalent to the probability mass function. We use this to find \\(\\mbox{P}(X=x)\\) when \\(X\\sim \\textsf{Binom}(n,p)\\). This function takes three inputs: x (the value of the random variable), size (the number of trials, \\(n\\)), and prob (the probability of success, \\(p\\)). So, \\[ \\mbox{P}(X=x)={n\\choose{x}}p^x(1-p)^{n-x}=\\textsf{dbinom(x,n,p)} \\] pbinom(): This function is equivalent to the cumulative distribution function. We use this to find \\(\\mbox{P}(X\\leq x)\\) when \\(X\\sim \\textsf{Binom}(n,p)\\). This function takes the same inputs as dbinom() but returns the cumulative probability: \\[ \\mbox{P}(X\\leq x)=\\sum_{k=0}^x{n\\choose{k}}p^k(1-p)^{n-k}=\\textsf{pbinom(x,n,p)} \\] qbinom(): This is the inverse of the cumulative distribution function and will return a percentile. This function has three inputs: p (a probability), size and prob. It returns the smallest value \\(x\\) such that \\(\\mbox{P}(X\\leq x) \\geq p\\). rbinom(): This function is used to randomly generate values from the binomial distribution. It takes three inputs: n (the number of values to generate), size and prob. It returns a vector containing the randomly generated values. To learn more about these functions, type ? followed the function in the console. Exercise: Use the built-in functions for the binomial distribution to plot the pmf of \\(X\\) from the previous example. Also, use the built-in functions to compute the probabilities from the example. Figure 12.1 gf_dist(&quot;binom&quot;,size=20,prob=.5) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;X&quot;,y=&quot;P(X=x)&quot;) Figure 12.1: The pmf of a binomial random variable ###P(X=8) dbinom(8,20,0.5) ## [1] 0.1201344 ###P(X&lt;=8) pbinom(8,20,0.5) ## [1] 0.2517223 ## or sum(dbinom(0:8,20,0.5)) ## [1] 0.2517223 12.2.5 Poisson distribution The Poisson distribution is very common when considering count or arrival data. Consider a random process where events occur according to some rate over time (think arrivals to a retail register). Often, these events are modeled with the Poisson process. The Poisson process assumes a consistent rate of arrival and a memoryless arrival process (the time until the next arrival is independent of time since the last arrival). If we assume a particular process is a Poisson process, then there are two random variables that take common named distributions. The number of arrivals in a specified amount of time follows the Poisson distribution. Also, the amount of time until the next arrival follows the exponential distribution. We will defer discussion of the exponential distribution until the next lesson. What is random in the Poisson is the number of occurrences while the interval is fixed. That is why it is a discrete distribution. The parameter \\(\\lambda\\) is the average number of occurrences in the specific interval, note that the interval must be the same as is specified in the random variable. Let \\(X\\) be the number of arrivals in a length of time, \\(T\\), where arrivals occur according to a Poisson process with an average of \\(\\lambda\\) arrivals in length of time \\(T\\). Then \\(X\\) follows a Poisson distribution with parameter \\(\\lambda\\): \\[ X\\sim \\textsf{Poisson}(\\lambda) \\] The pmf of \\(X\\) is given by: \\[ f_X(x)=\\mbox{P}(X=x)=\\frac{\\lambda^xe^{-\\lambda}}{x!}, \\hspace{0.5cm} x=0,1,2,... \\] One unique feature of the Poisson distribution is that \\(\\mbox{E}(X)=\\mbox{Var}(X)=\\lambda\\). Example: Suppose fleet vehicles arrive to a maintenance garage at an average rate of 0.4 per day. Lets assume that these vehicles arrive according to a Poisson process. Let \\(X\\) be the number of vehicles that arrive to the garage in a week (7 days). Notice that the time interval has changed! What is the random variable \\(X\\)? What is the distribution (with parameter) of \\(X\\). What are \\(\\mbox{E}(X)\\) and \\(\\mbox{Var}(X)\\)? Find \\(\\mbox{P}(X=0)\\), \\(\\mbox{P}(X\\leq 6)\\), \\(\\mbox{P}(X \\geq 2)\\), and \\(\\mbox{P}(2 \\leq X \\leq 8)\\). Also, find the median of \\(X\\), and the 95th percentile of \\(X\\) (the value of \\(x\\) such that \\(\\mbox{P}(X\\leq x)\\geq 0.95\\)). Further, plot the pmf of \\(X\\). Since vehicles arrive according to a Poisson process, the probability question leads us to define the random variable \\(X\\) as The number of vehicles that arrive in a week. We know that \\(X\\sim \\textsf{Poisson}(\\lambda=0.4*7=2.8)\\). Thus, \\(\\mbox{E}(X)=\\mbox{Var}(X)=2.8\\). The parameter is the average number of vehicles that arrive in a week. \\[ \\mbox{P}(X=0)=\\frac{2.8^0 e^{-2.8}}{0!}=e^{-2.8}=0.061 \\] Alternatively, we can use the built-in R functions for the Poisson distribution: ##P(X=0) dpois(0,2.8) ## [1] 0.06081006 ##P(X&lt;=6) ppois(6,2.8) ## [1] 0.9755894 ## or sum(dpois(0:6,2.8)) ## [1] 0.9755894 ##P(X&gt;=2)=1-P(X&lt;2)=1-P(X&lt;=1) 1-ppois(1,2.8) ## [1] 0.7689218 ## or sum(dpois(2:1000,2.8)) ## [1] 0.7689218 Note that when considering \\(\\mbox{P}(X\\geq 2)\\), we recognize that this is equivalent to \\(1-\\mbox{P}(X\\leq 1)\\). We can use ppois() to find this probability. When considering \\(\\mbox{P}(2\\leq X \\leq 8)\\), we need to make sure we formulate this correctly. Below are two possible methods: ##P(2 &lt;= X &lt;= 8) = P(X &lt;= 8)-P(X &lt;= 1) ppois(8,2.8)-ppois(1,2.8) ## [1] 0.766489 ## or sum(dpois(2:8,2.8)) ## [1] 0.766489 To find the median and the 95th percentiles, we use qpois: qpois(0.5,2.8) ## [1] 3 qpois(0.95,2.8) ## [1] 6 Figure 12.2 is a plot of the pmf of a Poisson random variable. gf_dist(&quot;pois&quot;,lambda=2.8) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;X&quot;,y=&quot;P(X=x)&quot;) Figure 12.2: The pmf of a Poisson random variable. Figure 12.3 is the cdf of the same Poisson random variable in Figure 12.2. gf_dist(&quot;pois&quot;,lambda=2.8,kind=&quot;cdf&quot;) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;X&quot;,y=&quot;P(X&lt;=x)&quot;) Figure 12.3: The cdf of the Poisson random variable in Figure 12.2 12.2.6 Hypergeometric Consider an experiment where \\(k\\) objects are to be selected from a larger, but finite, group consisting of \\(m\\) successes and \\(n\\) failures. This is similar to the binomial process; after all, we are selecting successes and failures. However, in this case, the results are effectively selected without replacement. If the random variable \\(X\\) represents the number of successes selected in our sample of size \\(k\\), then \\(X\\) follows a hypergeometric distribution with parameters \\(m\\), \\(n\\), and \\(k\\). The pmf of \\(X\\) is given by: \\[ f_X(x)=\\frac{{m\\choose x}{n\\choose{k-x}}}{{{m+n}\\choose k}} , \\hspace{0.3cm} x=0,1,...,m \\] Also, \\(\\mbox{E}(X)=\\frac{km}{m+n}\\) and \\(\\mbox{Var}(X)=k\\frac{m}{m+n}\\frac{n}{m+n}\\frac{m+n-k}{m+n-1}\\) If you draw on your knowledge of combinations, you can see why this pmf makes sense. Example: Suppose a bag contains 12 red chips and 8 black chips. I reach in blindly and randomly select 6 chips. What is the probability I select no black chips? All black chips? Between 2 and 5 black chips? First we should identify a random variable that will help us with this problem. Let \\(X\\) be the number of black chips selected when randomly selecting 6 from the bag. Then \\(X\\sim \\textsf{HyperGeom}(8,12,6)\\). We can use R to find these probabilities. First, the plot of the pmf of the hypergeometric is in Figure 12.4. gf_dist(&quot;hyper&quot;,m=8,n=12,k=6) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;X&quot;,y=&quot;P(X=x)&quot;) Figure 12.4: The pmf of a hypergeometric random variable. ##P(X=0) dhyper(0,8,12,6) ## [1] 0.02383901 ##P(X=6) dhyper(6,8,12,6) ## [1] 0.0007223942 ##P(2 &lt;= X &lt;=5) sum(dhyper(2:5,8,12,6)) ## [1] 0.8119711 12.3 Homework Problems For each of the problems below, 1) define a random variable that will help you answer the question, 2) state the distribution and parameters of that random variable; 3) determine the expected value and variance of that random variable, and 4) use that random variable to answer the question. We will demonstrate using 1a and 1b. 1. The T-6 training aircraft is used during UPT. Suppose that on each training sortie, aircraft return with a maintenance-related failure at a rate of 1 per 100 sorties. Find the probability of no maintenance failures in 15 sorties. \\(X\\): the number of maintenance failures in 15 sorties. \\(X\\sim \\textsf{Bin}(n=15,p=0.01)\\) \\(\\mbox{E}(X)=15*0.01=0.15\\) and \\(\\mbox{Var}(X)=15*0.01*0.99=0.1485\\). \\(\\mbox{P}(\\mbox{No mainteance failures})=\\mbox{P}(X=0)={15\\choose 0}0.01^0(1-0.01)^{15}=0.99^{15}\\) 0.99^15 ## [1] 0.8600584 ## or dbinom(0,15,0.01) ## [1] 0.8600584 This probability makes sense, since the expected value is fairly low. Because, on average, only 0.15 failures would occur every 15 trials, 0 failures would be a very common result. Graphically, the pmf looks like Figure 12.5. gf_dist(&quot;binom&quot;,size=15,prob=0.01) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;X&quot;,y=&quot;P(X=x)&quot;) Figure 12.5: The pmf for binomail in Homework Problem 1a. Find the probability of at least two maintenance failures in 15 sorties. We can use the same \\(X\\) as above. Now, we are looking for \\(\\mbox{P}(X\\geq 2)\\). This is equivalent to finding \\(1-\\mbox{P}(X\\leq 1)\\): ## Directly 1-(0.99^15 + 15*0.01*0.99^14) ## [1] 0.009629773 ## or, using R sum(dbinom(2:15,15,0.01)) ## [1] 0.009629773 ## or 1-sum(dbinom(0:1,15,0.01)) ## [1] 0.009629773 ## or 1-pbinom(1,15,0.01) ## [1] 0.009629773 ## or pbinom(1,15,0.01,lower.tail = F) ## [1] 0.009629773 Find the probability of at least 30 successful (no mx failures) sorties before the first failure. Find the probability of at least 50 successful sorties before the third failure. 2. On a given Saturday, suppose vehicles arrive at the USAFA North Gate according to a Poisson process at a rate of 40 arrivals per hour. Find the probability no vehicles arrive in 10 minutes. Find the probability at least 50 vehicles arrive in an hour. Find the probability that at least 5 minutes will pass before the next arrival. 3. Suppose there are 12 male and 7 female cadets in a classroom. I select 5 completely at random (without replacement). Find the probability I select no female cadets. Find the probability I select more than 2 female cadets. "],["CONTNNAMED.html", "Chapter 13 Named Continuous Distributions 13.1 Objectives 13.2 Continuous distributions 13.3 Homework Problems", " Chapter 13 Named Continuous Distributions 13.1 Objectives Recognize when to use common continuous distributions (Uniform, Exponential, Gamma, Normal, Beta), identify parameters, and find moments. Use R to calculate probabilities and quantiles involving random variables with common continuous distributions. Understand the relationship between the Poisson process and the Poisson &amp; Exponential distributions. Know when to apply and then use the memoryless property. 13.2 Continuous distributions In this lesson we will explore continuous distributions. This means we work with probability density functions and use them to find probabilities. Thus we must integrate, either numerically, graphically, or mathematically. The cumulative distribution function will also play an important role in this lesson. There are many more distributions than the ones in this lesson but these are the most common and will set you up to learn and use any others in the future. 13.2.1 Uniform distribution The first continuous distribution we will discuss is the uniform distribution. By default, when we refer to the uniform distribution, we are referring to the continuous version. When referring to the discrete version, we use the full term discrete uniform distribution. A continuous random variable has the uniform distribution if probability density is constant, uniform. The parameters of this distribution are \\(a\\) and \\(b\\), representing the minimum and maximum of the sample space. This distribution is commonly denoted as \\(U(a,b)\\). Let \\(X\\) be a continuous random variable with the uniform distribution. This is denoted as \\(X\\sim \\textsf{Unif}(a,b)\\). The pdf of \\(X\\) is given by: \\[ f_X(x)=\\left\\{\\begin{array}{ll} \\frac{1}{b-a}, &amp; a\\leq x \\leq b \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] The mean of \\(X\\) is \\(\\mbox{E}(X)=\\frac{a+b}{2}\\) and the variance is \\(\\mbox{Var}(X)=\\frac{(b-a)^2}{12}\\). The derivation of the mean is left to the Application. The most common uniform distribution is \\(U(0,1)\\) which we have already used several times in this course. Again, notice in Figure 13.1 that the plot of the pdf is a constant or uniform value. gf_dist(&quot;unif&quot;,title=&quot;Pdf of Uniform random variable&quot;,ylab=&quot;f(x)&quot;) %&gt;% gf_theme(theme_bw()) Figure 13.1: The pdf of Uniform random variable. To check it is a proper pdf, all values must be non-negative and the total probability must be 1. In R the function for probability density will start with the letter d and have some short descriptor for the distribution. For the uniform we use dunif(). integrate(function(x)dunif(x),0,1) ## 1 with absolute error &lt; 1.1e-14 13.2.2 Exponential distribution Recall from the lesson on named discrete distributions, we discussed the Poisson process. If arrivals follow a Poisson process, we know that the number of arrivals in a specified amount of time follows a Poisson distribution, and the time until the next arrival follows the exponential distribution. In the Poisson distribution, the number of arrivals is random and the interval is fixed. In the exponential distribution we change this, the interval is random and the arrivals are fixed at 1. This is a subtle point but worth the time to make sure you understand. Let \\(X\\) be the number of arrivals in a time interval \\(T\\), where arrivals occur according to a Poisson process with an average of \\(\\lambda\\) arrivals per unit time interval. From the previous lesson, we know that \\(X\\sim \\textsf{Poisson}(\\lambda T)\\). Now let \\(Y\\) be the time until the next arrival. Then \\(Y\\) follows the exponential distribution with parameter \\(\\lambda\\) which has units of inverse base time: \\[ Y \\sim \\textsf{Expon}(\\lambda) \\] Note on \\(\\lambda\\): One point of confusion involving the parameters of the Poisson and exponential distributions. The parameter of the Poisson distribution (usually denoted as \\(\\lambda\\)) represents the average number of arrivals in whatever amount of time specified by the random variable. In the case of the exponential distribution, the parameter (also denoted as \\(\\lambda\\)) represents the average number of arrivals per unit time. For example, suppose arrivals follow a Poisson process with an average of 10 arrivals per day. \\(X\\), the number of arrivals in 5 days, follows a Poisson distribution with parameter \\(\\lambda=50\\), since that is the average number of arrivals in the amount of time specified by \\(X\\). Meanwhile, \\(Y\\), the time in days until the next arrival, follows an exponential distribution with parameter \\(\\lambda=10\\) (the average number of arrivals per day). The pdf of \\(Y\\) is given by: \\[ f_Y(y)=\\lambda e^{-\\lambda y}, \\hspace{0.3cm} y&gt;0 \\] The mean and variance of \\(Y\\) are: \\(\\mbox{E}(Y)=\\frac{1}{\\lambda}\\) and \\(\\mbox{Var}(Y)=\\frac{1}{\\lambda^2}\\). You should be able to derive these results but they require integration by parts and can be lengthy algebraic exercises. Example: Suppose at a local retail store, customers arrive to a checkout counter according to a Poisson process with an average of one arrival every three minutes. Let \\(Y\\) be the time (in minutes) until the next customer arrives to the counter. What is the distribution (and parameter) of \\(Y\\)? What are \\(\\mbox{E}(Y)\\) and \\(\\mbox{Var}(Y)\\)? Find \\(\\mbox{P}(Y&gt;5)\\), \\(\\mbox{P}(Y\\leq 3)\\), and \\(\\mbox{P}(1 \\leq Y &lt; 5)\\)? Also, find the median and 95th percentile of \\(Y\\). Finally, plot the pdf of \\(Y\\). Since one arrival shows every three minutes, the average number of arrivals per unit time is 1/3 arrival per minute. Thus, \\(Y\\sim \\textsf{Expon}(\\lambda=1/3)\\). This means that \\(\\mbox{E}(Y)=3\\) and \\(\\mbox{Var}(Y)=9\\). To find \\(\\mbox{P}(Y&gt;5)\\), we could integrate the pdf of \\(Y\\): \\[ \\mbox{P}(Y&gt;5)=\\int_5^\\infty \\frac{1}{3}e^{-\\frac{1}{3}y}\\,\\mathrm{d}y = \\lim_{a \\to +\\infty}\\int_5^a \\frac{1}{3}e^{-\\frac{1}{3}y}\\,\\mathrm{d}y = \\] \\[\\lim_{a \\to +\\infty} -e^{-\\frac{1}{3}y}\\bigg|_5^a=\\lim_{a \\to +\\infty} -e^{-\\frac{a}{3}}-(-e^{-\\frac{5}{3}})= 0 + 0.189 = 0.189 \\] Alternatively, we could use R: ##Prob(Y&gt;5)=1-Prob(Y&lt;=5) 1-pexp(5,1/3) ## [1] 0.1888756 Or using integrate() integrate(function(x)1/3*exp(-1/3*x),5,Inf) ## 0.1888756 with absolute error &lt; 8.5e-05 For the remaining probabilities, we will use R: ##Prob(Y&lt;=3) pexp(3,1/3) ## [1] 0.6321206 ##Prob(1&lt;=Y&lt;5) pexp(5,1/3)-pexp(1,1/3) ## [1] 0.5276557 The median is \\(y\\) such that \\(\\mbox{P}(Y\\leq y)=0.5\\). We can find this by solving the following for \\(y\\): \\[ \\int_0^y \\frac{1}{3}e^{-\\frac{1}{3}y}\\,\\mathrm{d}y = 0.5 \\] Alternatively, we can use qexp in R: ##median qexp(0.5,1/3) ## [1] 2.079442 ##95th percentile qexp(0.95,1/3) ## [1] 8.987197 Figure 13.2: pdf of exponential random varible \\(Y\\) Both from Figure 13.2 and the mean and median, we know that the exponential distribution is skewed to the right. 13.2.3 Memoryless property The Poisson process is known for its memoryless property. Essentially, this means that the time until the next arrival is independent of the time since last arrival. Thus, the probability of an arrival within the next 5 minutes is the same regardless of whether an arrival just occurred or an arrival has not occurred for a long time. To show this lets consider random variable \\(Y\\) ( time until the next arrival in minutes) where \\(Y\\sim\\textsf{Expon}(\\lambda)\\). We will show that, given it has been at least \\(t\\) minutes since the last arrival, the probability we wait at least \\(y\\) additional minutes is equal to the marginal probability that we wait \\(y\\) additional minutes. First, note that the cdf of \\(Y\\), \\(F_Y(y)=\\mbox{P}(Y\\leq y)=1-e^{-\\lambda y}\\), you should be able to derive this. So, \\[ \\mbox{P}(Y\\geq y+t|Y\\geq t) = \\frac{\\mbox{P}(Y\\geq y+t \\cap Y\\geq t)}{\\mbox{P}(Y\\geq t)}=\\frac{\\mbox{P}(Y\\geq y +t)}{\\mbox{P}(Y\\geq t)} = \\frac{1-(1-e^{-(y+t)\\lambda})}{1-(1-e^{-t\\lambda})} \\] \\[ =\\frac{e^{-\\lambda y }e^{-\\lambda t}}{e^{-\\lambda t }}=e^{-\\lambda y} = 1-(1-e^{-\\lambda y})=\\mbox{P}(Y\\geq y). \\blacksquare \\] Lets simulate values for a Poisson. The Poisson is often used in modeling customer service situations such as service at Chipotle. However, some people have the mistaken idea that arrivals will be equally spaced. In fact, arrivals will come in clusters and bunches. Maybe this is the root of the common expression, Bad news comes in threes? Figure 13.3: Simulations of Poisson random variable. In Figure 13.3, the number of events in a box is \\(X\\sim \\textsf{Poisson}(\\lambda = 5)\\). As you can see, some boxes have more than 5 and some less because 5 is the average number of arrivals. Also note that the spacing is not equal. The 8 different runs are just repeated simulations of the same process. We can see spacing and clusters in each run. 13.2.4 Gamma distribution The gamma distribution is a generalization of the exponential distribution. In the exponential distribution, the parameter \\(\\lambda\\) is sometimes referred to as the rate parameter. The gamma distribution is sometimes used to model wait times (as with the exponential distribution), but in cases without the memoryless property. The gamma distribution has two parameters, rate and shape. In some texts, scale (the inverse of rate) is used as an alternative parameter to rate. Suppose \\(X\\) is a random variable with the gamma distribution with shape parameter \\(\\alpha\\) and rate parameter \\(\\lambda\\): \\[ X \\sim \\textsf{Gamma}(\\alpha,\\lambda) \\] \\(X\\) has the following pdf: \\[ f_X(x)=\\frac{\\lambda^\\alpha}{\\Gamma (\\alpha)}x^{\\alpha-1}e^{-\\lambda x}, \\hspace{0.3cm} x&gt;0 \\] and 0 otherwise. The mean and variance of \\(X\\) are \\(\\mbox{E}(X)=\\frac{\\alpha}{\\lambda}\\) and \\(\\mbox{Var}(X)=\\frac{\\alpha}{\\lambda^2}\\). Looking at the pdf, the mean and the variance, one can easily see that if \\(\\alpha=1\\), the resulting distribution is equivalent to \\(\\textsf{Expon}(\\lambda)\\). 13.2.4.1 Gamma function You may have little to no background with the Gamma function (\\(\\Gamma (\\alpha)\\)). This is different from the gamma distribution. The gamma function is simply a function and is defined by: \\[ \\Gamma (\\alpha)=\\int_0^\\infty t^{\\alpha-1}e^{-t}\\,\\mathrm{d}t \\] There are some important properties of the gamma function. Notably, \\(\\Gamma (\\alpha)=(\\alpha-1)\\Gamma (\\alpha -1)\\), and if \\(\\alpha\\) is a non-negative integer, \\(\\Gamma(\\alpha)=(\\alpha-1)!\\). Suppose \\(X \\sim \\textsf{Gamma}(\\alpha,\\lambda)\\). The pdf of \\(X\\) for various values of \\(\\alpha\\) and \\(\\lambda\\) is shown in Figure 13.4. Figure 13.4: pdf of Gamma for various values of alpha and lambda Example: Let \\(X \\sim \\textsf{Gamma}(\\alpha=5,\\lambda=1)\\). Find the mean and variance of \\(X\\). Also, compute \\(\\mbox{P}(X\\leq 2)\\) and \\(\\mbox{P}(1\\leq X &lt; 8)\\). Find the median and 95th percentile of \\(X\\). The mean and variance of \\(X\\) are \\(\\mbox{E}(X)=5\\) and \\(\\mbox{Var}(X)=5\\). To find probabilities and quantiles, integration will be difficult, so its best to use the built-in R functions: ## Prob(X&lt;=2) pgamma(2,5,1) ## [1] 0.05265302 ##Prob(1 &lt;= X &lt; 8) pgamma(8,5,1)-pgamma(1,5,1) ## [1] 0.8967078 ## median qgamma(0.5,5,1) ## [1] 4.670909 ## 95th percentile qgamma(0.95,5,1) ## [1] 9.153519 13.2.5 Weibull distribution The last common distribution we will explore is the Weibull distribution. Like the gamma, the Weibull distribution is a generalization of the exponential distribution and is meant to model wait times. A random variable with the Weibull distribution has parameters \\(\\alpha\\) and \\(\\beta\\). In R, these are referred to as shape and scale respectively. Note that in some resources, these are represented by \\(k\\) and \\(\\lambda\\) or even \\(k\\) and \\(\\theta\\). Let \\(X \\sim \\textsf{Weibull}(\\alpha,\\beta)\\). The pdf of \\(X\\) is given by: \\[ f_X(x)=\\frac{\\alpha}{\\beta} \\left(\\frac{x}{\\beta}\\right)^{\\alpha-1} e^{-\\left(\\frac{x}{\\beta}\\right)^\\alpha}, \\hspace{0.3cm} x\\geq 0 \\] The mean and variance of a random variable with a Weibull distribution can be found by consulting R documentation. Look them up and make sure you can use them. 13.2.6 Normal distribution The normal distribution (also referred to as Gaussian) is a common distribution found in natural processes. You have likely seen a bell curve in various contexts. The bell curve is often indicative of an underlying normal distribution. There are two parameters of the normal distribution: \\(\\mu\\) (the mean of \\(X\\)) and \\(\\sigma\\) (the standard deviation of \\(X\\)). Suppose a random variable \\(X\\) has a normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\). The pdf of \\(X\\) is given by: \\[ f_X(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, \\hspace{0.3cm} -\\infty &lt; x &lt;\\infty \\] Some plots of normal distributions for different parameters are plotted in Figure 13.5. Figure 13.5: pdf of Normal for various values of mu and sigma 13.2.6.1 Standard normal When random variable \\(X\\) is normally distributed with \\(\\mu=0\\) and \\(\\sigma=1\\), \\(X\\) is said to follow the standard normal distribution. Sometimes, the standard normal pdf is denoted by \\(\\phi(x)\\). Note that any normally distributed random variable can be transformed to have the standard normal distribution. Let \\(X \\sim \\textsf{Norm}(\\mu,\\sigma)\\). Then, \\[ Z=\\frac{X-\\mu}{\\sigma} \\sim \\textsf{Norm}(0,1) \\] Partially, one can show this is true by noting that the mean of \\(Z\\) is 0 and the variance (and standard deviation) of \\(Z\\) is 1: \\[ \\mbox{E}(Z)=\\mbox{E}\\left(\\frac{X-\\mu}{\\sigma}\\right)=\\frac{1}{\\sigma}\\left(\\mbox{E}(X)-\\mu\\right)=\\frac{1}\\sigma(\\mu-\\mu)=0 \\] \\[ \\mbox{Var}(Z)=\\mbox{Var}\\left(\\frac{X-\\mu}{\\sigma}\\right)=\\frac{1}{\\sigma^2}\\left(\\mbox{Var}(X)-0\\right)=\\frac{1}{\\sigma^2} \\sigma^2=1 \\] Note that this does not prove that \\(Z\\) follows the standard normal distribution; we have merely shown that \\(Z\\) has a mean of 0 and a variance of 1. We will discuss transformation of random variables in a later lesson. Example: Let \\(X \\sim \\textsf{Norm}(\\mu=200,\\sigma=15)\\). Compute \\(\\mbox{P}(X\\leq 160)\\), \\(\\mbox{P}(180\\leq X &lt; 230)\\), and \\(\\mbox{P}(X&gt;\\mu+\\sigma)\\). Find the median and 95th percentile of \\(X\\). As with the gamma distribution, to find probabilities and quantiles, integration will be difficult, so its best to use the built-in R functions: ## Prob(X&lt;=160) pnorm(160,200,15) ## [1] 0.003830381 ##Prob(180 &lt;= X &lt; 230) pnorm(230,200,15)-pnorm(180,200,15) ## [1] 0.8860386 ##Prob(X&gt;mu+sig) 1-pnorm(215,200,15) ## [1] 0.1586553 ## median qnorm(0.5,200,15) ## [1] 200 ## 95th percentile qnorm(0.95,200,15) ## [1] 224.6728 13.2.7 Beta distribution Another common continuous distribution is the beta distribution. This has a unique application in that the domain of a random variable with the beta distribution is \\([0,1]\\). Thus it is typically used to model proportions. The beta distribution has two parameters, \\(\\alpha\\) and \\(\\beta\\). (In R, these are denoted not so cleverly as shape1 and shape2.) Let \\(X \\sim \\textsf{Beta}(\\alpha,\\beta)\\). The pdf of \\(X\\) is given by: \\[ f_X(x)=\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}, \\hspace{0.3cm} 0\\leq x \\leq 1 \\] Yes, our old friend the Gamma function. In some resources, \\(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\) is written as \\(\\frac{1}{B(\\alpha,\\beta)}\\), where \\(B\\) is known as the beta function. Note that \\(\\mbox{E}(X)=\\frac{\\alpha}{\\alpha+\\beta}\\) and \\(\\mbox{Var}(X)=\\frac{\\alpha \\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\). For various values \\(\\alpha\\) and \\(\\beta\\), the pdf of a beta distributed random variable is shown in Figure 13.6. Figure 13.6: pdf of Beta for various values of alpha and beta Exercise What is the distribution if \\(\\alpha=\\beta=1\\)? It is the uniform. It is easy to verify that \\(\\Gamma(1)=1\\) so that \\(B(1,1)=1\\). 13.3 Homework Problems For problems 1-3 below, 1) define a random variable that will help you answer the question, 2) state the distribution and parameters of that random variable; 3) determine the expected value and variance of that random variable, and 4) use that random variable to answer the question. 1. On a given Saturday, suppose vehicles arrive at the USAFA North Gate according to a Poisson process at a rate of 40 arrivals per hour. Find the probability no vehicles arrive in 10 minutes. Find the probability that at least 5 minutes will pass before the next arrival. Find the probability that the next vehicle will arrive between 2 and 10 minutes from now. Find the probability that at least 7 minutes will pass before the next arrival, given that 2 minutes have already passed. Compare this answer to part (b). This is an example of the memoryless property of the exponential distribution. Fill in the blank. There is a probability of 90% that the next vehicle will arrive within __ minutes. This value is known as the 90% percentile of the random variable. Use the function stripplot() to visualize the arrival of 30 vehicles using a random sample from the appropriate exponential distribution. 2. Suppose time until computer errors on the F-35 follows a Gamma distribution with mean 20 hours and variance 10. Find the probability that 20 hours pass without a computer error. Find the probability that 45 hours pass without a computer error, given that 25 hours have already passed. Does the memoryless property apply to the Gamma distribution? Find \\(a\\) and \\(b\\): There is a 95% probability time until next computer error will be between \\(a\\) and \\(b\\). (note: technically, there are many answers to this question, but find \\(a\\) and \\(b\\) such that each tail has equal probability.) 3. Suppose PFT scores in the cadet wing follow a normal distribution with mean 330 and standard deviation 50. Find the probability a randomly selected cadet has a PFT score higher than 450. Find the probability a randomly selected cadet has a PFT score within 2 standard deviations of the mean. Find \\(a\\) and \\(b\\) such that 90% of PFT scores will be between \\(a\\) and \\(b\\). Find the probability a randomly selected cadet has a PFT score higher than 450 given he/she is among the top 10% of cadets. 4. Let \\(X \\sim \\textsf{Beta}(\\alpha=1,\\beta=1)\\). Show that \\(X\\sim \\textsf{Unif}(0,1)\\). Hint: write out the beta distribution pdf where \\(\\alpha=1\\) and \\(\\beta=1\\). 5. When using R to calculate probabilities related to the gamma distribution, we often use pgamma. Recall that pgamma is equivalent to the cdf of the gamma distribution. If \\(X\\sim\\textsf{Gamma}(\\alpha,\\lambda)\\), then \\[ \\mbox{P}(X\\leq x)=\\textsf{pgamma(x,alpha,lambda)} \\] "],["MULTIDISTS.html", "Chapter 14 Multivariate Distributions 14.1 Objectives 14.2 Multivariate distributions 14.3 Joint probability 14.4 Homework Problems", " Chapter 14 Multivariate Distributions 14.1 Objectives Define (and distinguish between) the terms joint probability mass/density function, marginal pmf/pdf, and conditional pmf/pdf. Given a joint pmf/pdf, obtain the marginal and conditional pmfs/pdfs. Use joint, marginal and conditional pmfs/pdfs to obtain probabilities. 14.2 Multivariate distributions Multivariate situations are the more common in practice. We are often dealing with more than one variable. We have seen this in the previous block of material and will see multivariate distributions in the remainder of the course and through Math 378. The basic idea is that we want to determine the relationship between two or more variables to include variable(s) conditional on variables. 14.3 Joint probability Thus far, we have only considered situations involving one random variable. In some cases, we might be concerned with the behavior of multiple random variables simultaneously. The next three lessons are dedicated to jointly distributed random variables. 14.3.1 Discrete random variables In the discrete case, joint probability is described by the joint probability mass function. In the bivariate case, suppose \\(X\\) and \\(Y\\) are discrete random variables. The joint pmf is given by \\(f_{X,Y}(x,y)\\) and represents \\(\\mbox{P}(X=x,Y=y) = \\mbox{P}(X=x \\cap Y=y)\\). Note: it is common in statistical and probability models to use a comma to represent and, in fact the select() function in tidyverse does this. The same rules of probability apply to the joint pmf. Each value of \\(f\\) must be between 0 and 1, and the total probability must sum to 1: \\[ \\sum_{x\\in S_X}\\sum_{y \\in S_Y} f_{X,Y}(x,y) = 1 \\] This notation means that if we sum the joint probabilities over all values of the random variables \\(X\\) and \\(Y\\) we will get 1. If given a joint pmf, one can obtain the marginal pmf of individual variables. The marginal pmf is simply the mass function of an individual random variable, summing over the possible values of all the other variables. In the bivariate case, the marginal pmf of \\(X\\), \\(f_X(x)\\) is found by: \\[ f_X(x)=\\sum_{y \\in S_Y}f_{X,Y}(x,y) \\] Notice that in the above summation, we summed over only the \\(y\\) values. Similarly, \\[ f_Y(y)=\\sum_{x \\in S_X}f_{X,Y}(x,y) \\] The marginal pmf must be distinguished from the conditional pmf. The conditional pmf describes a discrete random variable given other random variables have taken particular values. In the bivariate case, the conditional pmf of \\(X\\), given \\(Y=y\\), is denoted as \\(f_{X|Y=y}(x)\\) and is found by: \\[ f_{X|Y=y}(x)=\\mbox{P}(X=x|Y=y)=\\frac{\\mbox{P}(X=x,Y=y)}{\\mbox{P}(Y=y)}=\\frac{f_{X,Y}(x,y)}{f_Y(y)} \\] Example: Let \\(X\\) and \\(Y\\) be discrete random variables with joint pmf below. \\[ \\begin{array}{cc|ccc} &amp; &amp; &amp; \\textbf{Y} &amp; \\\\ &amp; &amp; 0 &amp; 1 &amp; 2 \\\\&amp;\\hline0 &amp; 0.10 &amp; 0.08 &amp; 0.11 \\\\\\textbf{X} &amp;2 &amp; 0.18 &amp; 0.20 &amp; 0.12 \\\\&amp;4 &amp; 0.07 &amp; 0.05 &amp; 0.09 \\end{array} \\] Find the marginal pmfs of \\(X\\) and \\(Y\\). Find \\(f_{X|Y=0}(x)\\) and \\(f_{Y|X=2}(y)\\). The marginal pmfs can be found by summing across the other variable. So, to find \\(f_X(x)\\), we simply sum across the rows: \\[ f_X(x)=\\left\\{\\begin{array}{ll} 0.10+0.08+0.11, &amp; x=0 \\\\ 0.18+0.20+0.12, &amp; x=2 \\\\ 0.07+0.05+0.09, &amp; x=4 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.29, &amp; x=0 \\\\ 0.50, &amp; x=2 \\\\ 0.21, &amp; x=4 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] Similarly, \\(f_Y(y)\\) can be found by summing down the columns of the joint pmf: \\[ f_Y(y)=\\left\\{\\begin{array}{ll} 0.35, &amp; y=0 \\\\ 0.33, &amp; y=1 \\\\ 0.32, &amp; y=2 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] To find the conditional pmf of \\(X\\) given \\(Y=0\\), it helps to recognize that once we know that \\(Y=0\\), the overall sample space has changed. Now the only outcomes we consider are in the first column (corresponding to \\(Y=0\\)): We are looking for the distribution of \\(X\\) within the circled area. So, we need to find the proportion of probability assigned to each outcome of \\(X\\). Mathematically: \\[ f_{X|Y=0}(x)=\\mbox{P}(X=x|Y=0)=\\frac{\\mbox{P}(X=x,Y=0)}{\\mbox{P}(Y=0)}=\\frac{f_{X,Y}(x,0)}{f_Y(0)} \\] Above, we found the marginal pmf of \\(Y\\). We know that \\(f_Y(0)=0.35\\). So, \\[ \\renewcommand{\\arraystretch}{1.25} f_{X|Y=0}(x)=\\left\\{\\begin{array}{ll} \\frac{0.10}{0.35}, &amp; x=0 \\\\ \\frac{0.18}{0.35}, &amp; x=2 \\\\ \\frac{0.07}{0.35}, &amp; x=4 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.286, &amp; x=0 \\\\ 0.514, &amp; x=2 \\\\ 0.200, &amp; x=4 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] Note that the probabilities in this pmf sum to 1. It is always wise to confirm this to ensure we did not make a simple computational error along the way. Similarly, we can find \\(f_{Y|X=2}(y)\\). First we recognize that \\(f_X(2)=0.5\\). \\[ \\renewcommand{\\arraystretch}{1.25} f_{Y|X=2}(x)=\\left\\{\\begin{array}{ll} \\frac{0.18}{0.50}, &amp; y=0 \\\\ \\frac{0.20}{0.50}, &amp; y=1 \\\\ \\frac{0.12}{0.50}, &amp; y=2 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.36, &amp; y=0 \\\\ 0.40, &amp; y=1 \\\\ 0.24, &amp; y=2 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] Together, these pmfs can be used to find relevant probabilities. For example, see the practical application exercises. 14.3.2 Continuous random variables Many of the ideas above for discrete random variables also apply to the case of multiple continuous random variables. Suppose \\(X\\) and \\(Y\\) are continuous random variables. Their joint probability is described by the joint probability density function. As in the discrete case, the joint pdf is represented by \\(f_{X,Y}(x,y)\\). Recall that while pmfs return probabilities, pdfs return densities, which are not equivalent to probabilities. In order to obtain probability from a pdf, one has to integrate the pdf across the applicable subset of the domain. The rules of a joint pdf are analogous to the univariate case. For all \\(x\\) and \\(y\\), \\(f_{X,Y}(x,y)\\geq 0\\) and the probability must sum to one: \\[ \\int_{S_X}\\int_{S_Y}f_{X,Y}(x,y)\\,\\mathrm{d}y \\,\\mathrm{d}x = 1 \\] The marginal pdf is the density function of an individual random variable, integrating out all others. In the bivariate case, the marginal pdf of \\(X\\), \\(f_X(x)\\), is found by summing, integrating, across the other variable: \\[ f_X(x)=\\int_{S_Y}f_{X,Y}(x,y)\\,\\mathrm{d}y \\] Similarly, \\[ f_Y(y)=\\int_{S_X}f_{X,Y}(x,y)\\,\\mathrm{d}x \\] The conditional pdf of \\(X\\), given \\(Y=y\\) is denoted as \\(f_{X|Y=y}(x)\\) and is found in the same way as in the discrete case: \\[ f_{X|Y=y}(x)=\\frac{f_{X,Y}(x,y)}{f_Y(y)} \\] Similarly, \\[ f_{Y|X=x}(y)=\\frac{f_{X,Y}(x,y)}{f_X(x)} \\] Note that we are working with the pdf and not probabilities in this case. That is because we cant determine the probability at a point for a continuous random variable. Thus we work with conditional pdfs to find probabilities for conditional statements. Example: Let \\(X\\) and \\(Y\\) be continuous random variables with joint pdf: \\[ f_{X,Y}(x,y)=xy \\] for \\(0\\leq x \\leq 2\\) and \\(0 \\leq y \\leq 1\\). Verify \\(f\\) is a valid joint pdf. We need to ensure the total volume under the pdf is 1. Note that the double integral with constant limits of integration is just like doing single integrals. We just treat the other variable as a constant. In this course we will not work with limits of integration that have variables in them, this is the material of Calc III. For our simple case of constant limits of integration, the order of integration does not matter. We will arbitrarily integrate \\(x\\) first, treating \\(y\\) as a constant. Then integrate with respect to \\(y\\). \\[ \\int_0^1 \\int_0^2 xy \\,\\mathrm{d}x \\,\\mathrm{d}y = \\int_0^1 \\frac{x^2y}{2}\\bigg|_0^2 \\,\\mathrm{d}y = \\int_0^1 2y\\,\\mathrm{d}y = y^2\\bigg|_0^1 = 1 \\] Using R to do this requires a new package cubature. You can install it from RStudio package tab or the command line using install.packages(\"cubature\"). Then we can use it as follows: library(cubature) # load the package &quot;cubature&quot; f &lt;- function(x) { (x[1] * x[2]) } # &quot;x&quot; is vector adaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1, 2)) ## $integral ## [1] 1 ## ## $error ## [1] 0 ## ## $functionEvaluations ## [1] 17 ## ## $returnCode ## [1] 0 Notice the function adaptIntegrate returned four objects. You can read the help menu to learn more about them but we are only interested in the result contained in the object integral. Find \\(\\mbox{P}(X &gt; 1, Y \\leq 0.5)\\). \\[ \\mbox{P}(X&gt;1,Y\\leq 0.5)=\\int_0^{0.5}\\int_1^2 xy \\,\\mathrm{d}x \\,\\mathrm{d}y = \\int_0^{0.5} \\frac{x^2 y}{2}\\bigg|_1^2 \\,\\mathrm{d}y = \\int_0^{0.5}2y - \\frac{y}{2}\\,\\mathrm{d}y \\] \\[ = \\frac{3y^2}{4}\\bigg|_0^{0.5}=0.1875 \\] f &lt;- function(x) { (x[1] * x[2]) } # &quot;x&quot; is vector adaptIntegrate(f, lowerLimit = c(1, 0), upperLimit = c(2, 1/2)) ## $integral ## [1] 0.1875 ## ## $error ## [1] 2.775558e-17 ## ## $functionEvaluations ## [1] 17 ## ## $returnCode ## [1] 0 Find the marginal pdfs of \\(X\\) and \\(Y\\). \\[ f_X(x)=\\int_0^1 xy \\,\\mathrm{d}y = \\frac{xy^2}{2}\\bigg|_0^1=\\frac{x}{2} \\] where \\(0 \\leq x \\leq 2\\). \\[ f_Y(y)=\\int_0^2 xy \\,\\mathrm{d}x = \\frac{x^2y}{2}\\bigg|_0^2= 2y \\] where \\(0 \\leq y \\leq 1\\). Find the conditional pdfs of \\(X|Y=y\\) and \\(Y|X=x\\). \\[ f_{X|Y=y}(x)=\\frac{f_{X,Y}(x,y)}{f_Y(y)}=\\frac{xy}{2y}=\\frac{x}{2} \\] where \\(0 \\leq x \\leq 2\\). Similarly, \\[ f_{Y|X=x}(y)=\\frac{f_{X,Y}(x,y)}{f_X(x)}=\\frac{xy}{\\frac{x}{2}}=2y \\] where \\(0 \\leq y \\leq 1\\). 14.4 Homework Problems 1. Let \\(X\\) and \\(Y\\) be continuous random variables with joint pmf: \\[ f_{X,Y}(x,y)=x + y \\] where \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 1\\). Verify that \\(f\\) is a valid pdf. Find the marginal pdfs of \\(X\\) and \\(Y\\). Find the conditional pdfs of \\(X|Y=y\\) and \\(Y|X=x\\). Find the following probabilities: \\(\\mbox{P}(X&lt;0.5)\\); \\(\\mbox{P}(Y&gt;0.8)\\); \\(\\mbox{P}(X&lt;0.2,Y\\geq 0.75)\\); \\(\\mbox{P}(X&lt;0.2|Y\\geq 0.75)\\); \\(\\mbox{P}(X&lt;0.2|Y= 0.25)\\); Optional - \\(\\mbox{P}(X\\leq Y)\\). 2. In the Notes, we saw an example where \\(f_X(x)=f_{X|Y=y}(x)\\) and \\(f_Y(y)=f_{Y|X=x}(y)\\). This is not common and is important. What does this imply about \\(X\\) and \\(Y\\)? 3. ADVANCED: Recall on an earlier assignment, we came up with random variables to describe timeliness at an airport. Suppose over the course of 210 days, on each day we recorded the number of customer complaints regarding timeliness. Also on each day, we recorded the weather (our airport is located somewhere without snow and without substantial wind). The data are displayed below. \\[ \\begin{array}{cc|cc} &amp; &amp; &amp;\\textbf{Weather Status}\\\\ &amp; &amp; \\mbox{Clear} &amp; \\mbox{Light Rain} &amp; \\mbox{Rain} \\\\ &amp; \\hline0 &amp; 28 &amp; 11 &amp; 4 \\\\ &amp; 1 &amp; 18 &amp; 15 &amp; 8 \\\\ &amp; 2 &amp; 17 &amp; 25 &amp; 12 \\\\ \\textbf{# of complaints} &amp; 3 &amp; 13 &amp; 15 &amp; 16 \\\\ &amp; 4 &amp; 8 &amp; 8 &amp; 10 \\\\ &amp; 5 &amp; 0 &amp; 1 &amp; 1 \\\\ \\end{array} \\] First, define two random variables for this scenario. One of them (# of complaints) is essentially already a random variable. For the other (weather status) you will need to assign a number to each status. Use the table above to build an empirical joint pmf of the two random variables. Find the marginal pmfs of each random variable. Find the probability of fewer than 3 complaints. Find the probability of fewer than 3 complaints given there is no rain. Optional for those of you that like Calc III and want a challenge. 4. Let \\(X\\) and \\(Y\\) be continuous random variables with joint pmf: \\[ f_{X,Y}(x,y)=1 \\] where \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 2x\\). Verify that \\(f\\) is a valid pdf. Find the marginal pdfs of \\(X\\) and \\(Y\\). Find the conditional pdfs of \\(X|Y=y\\) and \\(Y|X=x\\). Find the following probabilities: \\(\\mbox{P}(X&lt;0.5)\\); \\(\\mbox{P}(Y&gt;1)\\); \\(\\mbox{P}(X&lt;0.5,Y\\leq 0.8)\\); Optional \\(\\mbox{P}(X&lt;0.5|Y= 0.8)\\); \\(\\mbox{P}(Y\\leq 1-X)\\). (It would probably help to draw some pictures.) "],["MULTIEXP.html", "Chapter 15 Multivariate Expectation 15.1 Objectives 15.2 Expectation - moments 15.3 Exercises to apply what we learned 15.4 Exercises to apply what we learned 15.5 Covariance/Correlation 15.6 Independence 15.7 Conditional expectation 15.8 Homework Problems", " Chapter 15 Multivariate Expectation 15.1 Objectives Given a joint pmf/pdf, obtain means and variances of random variables and functions of random variables. Define the terms covariance and correlation, and given a joint pmf/pdf, obtain the covariance and correlation between two random variables. Given a joint pmf/pdf, determine whether random variables are independent of one another. Find conditional expectations. 15.2 Expectation - moments Computing expected values of random variables in the joint context is similar to the univariate case. Let \\(X\\) and \\(Y\\) be discrete random variables with joint pmf \\(f_{X,Y}(x,y)\\). Let \\(g(X,Y)\\) be some function of \\(X\\) and \\(Y\\). Then: \\[ \\mbox{E}[g(X,Y)]=\\sum_x\\sum_y g(x,y)f_{X,Y}(x,y) \\] (Note that \\(\\sum\\limits_{x}\\) is shorthand for the sum across all possible values of \\(x\\).) In the case of continuous random variables with a joint pdf \\(f_{X,Y}(x,y)\\), expectation becomes: \\[ \\mbox{E}[g(X,Y)]=\\int_x\\int_y g(x,y)f_{X,Y}(x,y)\\,\\mathrm{d}y \\,\\mathrm{d}x \\] 15.2.1 Expectation of discrete random variables Given a joint pmf, one can find the mean of \\(X\\) by using the joint function or by finding the marginal pmf first and then using that to find \\(\\mbox{E}(X)\\). In the end, both ways are the same. For the discrete case: \\[ \\mbox{E}(X)=\\sum_x\\sum_y xf_{X,Y}(x,y) = \\sum_x x \\sum_y f_{X,Y}(x,y) \\] The \\(x\\) can be moved outside the inner sum since the inner sum is with respect to variable \\(y\\) and \\(x\\) is a constant with respect to \\(y\\). Note that the inner sum is the marginal pmf of \\(X\\). So, \\[ \\mbox{E}(X)=\\sum_x x \\sum_y f_{X,Y}(x,y)=\\sum_x x f_X(x) \\] Example: Let \\(X\\) and \\(Y\\) be discrete random variables with joint pmf below. \\[ \\begin{array}{cc|ccc} &amp; &amp; &amp; \\textbf{Y} &amp; \\\\ &amp; &amp; 0 &amp; 1 &amp; 2 \\\\&amp;\\hline0 &amp; 0.10 &amp; 0.08 &amp; 0.11 \\\\\\textbf{X} &amp;1 &amp; 0.18 &amp; 0.20 &amp; 0.12 \\\\&amp;2 &amp; 0.07 &amp; 0.05 &amp; 0.09 \\end{array} \\] Find \\(\\mbox{E}(X)\\) First we will use the joint pmf directly, then we find the marginal pmf of \\(X\\) and use that as we would in a univariate case. \\[ \\mbox{E}(X)=\\sum_{x=0}^2 \\sum_{y=0}^2 x f_{X,Y}(x,y)=0*0.10+0*0.08+0*0.11+1*0.18+...+2*0.09 = 0.92 \\] The marginal pmf of \\(X\\) is \\[ f_X(x)=\\left\\{\\begin{array}{ll} 0.10+0.08+0.11, &amp; x=0 \\\\ 0.18+0.20+0.12, &amp; x=1 \\\\ 0.07+0.05+0.09, &amp; x=2 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. = \\left\\{\\begin{array}{ll} 0.29, &amp; x=0 \\\\ 0.50, &amp; x=1 \\\\ 0.21, &amp; x=2 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] So, \\(\\mbox{E}(X)=0*0.29+1*0.5+2*0.21=0.92\\). 15.3 Exercises to apply what we learned Exercise: Let \\(X\\) and \\(Y\\) be defined above. Find \\(\\mbox{E}(Y)\\), \\(\\mbox{E}(X+Y)\\), \\(\\mbox{E}(XY)\\), and \\(\\mbox{E}\\left(\\frac{1}{2X+Y+1}\\right)\\). 15.3.1 E(Y) As with \\(\\mbox{E}(X)\\), \\(\\mbox{E}(Y)\\) can be found in two ways. We will use the marginal pmf of \\(Y\\), which we will not derive: \\[ \\mbox{E}(Y)=\\sum_{y=0}^2 y \\cdot f_Y(y)=0*0.35+1*0.33+2*0.32 = 0.97 \\] 15.3.2 E(X+Y) To find \\(\\mbox{E}(X+Y)\\) we will use the joint pmf. In the discrete case, it helps to first identify all of the possible values of \\(X+Y\\) and then figure out what probabilities are associated with each value. This problem is really a transformation problem where we are finding the distribution of \\(X+Y\\). In this example, \\(X+Y\\) can take on values 0, 1, 2, 3, and 4. The value 0 only happens when \\(X=Y=0\\) and the probability of this outcome is 0.10. The value 1 occurs when \\(X=0\\) and \\(Y=1\\) or when \\(X=1\\) and \\(Y=0\\). This occurs with probability 0.08 + 0.18. We continue in this manner: \\[ \\mbox{E}(X+Y)=\\sum_{x=0}^2\\sum_{y=0}^2 (x+y)f_{X,Y}(x,y) = 0*0.1+1*(0.18+0.08)+2*(0.11+0.07+0.20) \\] \\[ +3*(0.12+0.05)+4*0.09 = 1.89 \\] Note that \\(\\mbox{E}(X+Y)=\\mbox{E}(X)+\\mbox{E}(Y)\\). (The proof of this is left to the reader.) 15.3.3 E(XY) \\[ \\mbox{E}(XY)=\\sum_{x=0}^2\\sum_{y=0}^2 xyf_{X,Y}(x,y) = 0*(0.1+0.08+0.11+0.18+0.07)+1*0.20 \\] \\[ +2*(0.12+0.05)+4*0.09= 0.9 \\] Note that \\(\\mbox{E}(XY)\\) is not necessarily equal to \\(\\mbox{E}(X)\\mbox{E}(Y)\\). 15.3.4 E(1/2X+Y+1) \\[ \\mbox{E}\\left(\\frac{1}{2X+Y+1}\\right) = \\sum_{x=0}^2\\sum_{y=0}^2 \\frac{1}{2x+y+1}f_{X,Y}(x,y) = 1*0.1+\\frac{1}{2}*0.08+\\frac{1}{3}*(0.11+0.18) \\] \\[ +\\frac{1}{4}*0.20+\\frac{1}{5}*(0.12+0.07)+\\frac{1}{6}*0.05+\\frac{1}{7}*0.09 = 0.3125 \\] 15.3.5 Expectation of continuous random variables Lets consider an example with continuous random variables where summation is replaced with integration: Example: Let \\(X\\) and \\(Y\\) be continuous random variables with joint pdf: \\[ f_{X,Y}(x,y)=xy \\] for \\(0\\leq x \\leq 2\\) and \\(0 \\leq y \\leq 1\\). 15.4 Exercises to apply what we learned Exercise: Find \\(\\mbox{E}(X)\\), \\(\\mbox{E}(X+Y)\\), \\(\\mbox{E}(XY)\\), and \\(\\mbox{Var}(XY)\\). 15.4.1 E(X) We found the marginal pdf of \\(X\\) in a previous lesson, so we should use that now: \\[ \\mbox{E}(X)=\\int_0^2 x\\frac{x}{2}\\,\\mathrm{d}x = \\frac{x^3}{6}\\bigg|_0^2= \\frac{4}{3} \\] 15.4.2 E(X+Y) To find \\(\\mbox{E}(X+Y)\\), we could use the joint pdf directly, or use the marginal pdf of \\(Y\\) to find \\(\\mbox{E}(Y)\\) and then add the result to \\(\\mbox{E}(X)\\). The reason this is valid is because when we integrate \\(x\\) with the joint pdf, integrating with respect to \\(y\\) first, we can treat \\(x\\) as a constant and bring it out side the integral. Then we are integrating the joint pdf with respect \\(y\\) which results in the marginal pdf of \\(X\\). Well use the joint pdf: \\[ \\mbox{E}(X+Y)=\\int_0^2\\int_0^1 (x+y)xy\\,\\mathrm{d}y \\,\\mathrm{d}x=\\int_0^2\\int_0^1 (x^2y+xy^2)\\,\\mathrm{d}y \\,\\mathrm{d}x = \\int_0^2 \\frac{x^2y^2}{2}+\\frac{xy^3}{3} \\bigg|_{y=0}^{y=1}\\,\\mathrm{d}x \\] \\[ = \\int_0^2 \\frac{x^2}{2}+\\frac{x}{3} \\,\\mathrm{d}x= \\frac{x^3}{6}+\\frac{x^2}{6}\\bigg|_0^2=\\frac{8}{6}+\\frac{4}{6}=2 \\] If we wanted to use simulation to find this expectation, we could simulate variables from the marginal of \\(X\\) and \\(Y\\) and then add them together to create a new variable. The cdf for \\(X\\) is \\(\\frac{x^2}{4}\\) so we simulate a random variable from \\(X\\) by sampling from a random uniform and then taking the inverse of the cdf. For \\(Y\\) the cdf is \\(y^2\\) and do a similar simulation. set.seed(1820) new_data &lt;- data.frame(x=2*sqrt(runif(10000)),y=sqrt(runif(10000))) new_data %&gt;% mutate(z=x+y) %&gt;% summarize(Ex=mean(x),Ey=mean(y),Explusy = mean(z)) ## Ex Ey Explusy ## 1 1.338196 0.6695514 2.007748 We can see that \\(E(X + Y) = E(X) + E(Y)\\). 15.4.3 E(XY) Next, we have \\[ \\mbox{E}(XY)=\\int_0^2\\int_0^1 xy*xy\\,\\mathrm{d}y \\,\\mathrm{d}x = \\int_0^2 \\frac{x^2y^3}{3}\\bigg|_0^1 \\,\\mathrm{d}x = \\int_0^2 \\frac{x^2}{3}\\,\\mathrm{d}x \\] \\[ =\\frac{x^3}{9}\\bigg|_0^2 = \\frac{8}{9} \\] Or by simulating, we have: set.seed(191) new_data &lt;- data.frame(x=2*sqrt(runif(10000)),y=sqrt(runif(10000))) new_data %&gt;% mutate(z=x*y) %&gt;% summarize(Ex=mean(x),Ey=mean(y),Extimesy = mean(z)) ## Ex Ey Extimesy ## 1 1.33096 0.6640436 0.8837552 15.4.4 V(XY) Recall that the variance of a random variable is the expected value of the squared difference from its mean. So, \\[ \\mbox{Var}(XY)=\\mbox{E}\\left[\\left(XY-\\mbox{E}(XY)\\right)^2\\right]=\\mbox{E}\\left[\\left(XY-\\frac{8}{9}\\right)^2\\right] \\] \\[ =\\int_0^2\\int_0^1 \\left(xy-\\frac{8}{9}\\right)^2 xy\\,\\mathrm{d}y \\,\\mathrm{d}x =\\int_0^2\\int_0^1 \\left(x^2y^2-\\frac{16xy}{9}+\\frac{64}{81}\\right)xy\\,\\mathrm{d}y \\,\\mathrm{d}x \\] Yuck!! But we will continue because we are determined to integrate after so much Calculus in our core curriculum. \\[ =\\int_0^2\\int_0^1 \\left(x^3y^3-\\frac{16x^2y^2}{9}+\\frac{64xy}{81}\\right)\\,\\mathrm{d}y \\,\\mathrm{d}x =\\int_0^2 \\frac{x^3y^4}{4}-\\frac{16x^2y^3}{27}+\\frac{32xy^2}{81}\\bigg|_0^1 \\,\\mathrm{d}x = \\int_0^2 \\frac{x^3}{4}-\\frac{16x^2}{27}+\\frac{32x}{81}\\,\\mathrm{d}x \\] \\[ = \\frac{x^4}{16}-\\frac{16x^3}{81}+\\frac{16x^2}{81}\\bigg|_0^2 \\] \\[ =\\frac{16}{16}-\\frac{128}{81}+\\frac{64}{81}=0.2098765 \\] Next we will estimate the variance using a simulation: set.seed(816) new_data &lt;- data.frame(x=2*sqrt(runif(10000)),y=sqrt(runif(10000))) new_data %&gt;% mutate(z=(x*y-8/9)^2) %&gt;% summarize(Var = mean(z)) ## Var ## 1 0.2098769 That was much easier. Notice that we are really just estimating these expectations with the simulations. The mathematical answers are the true population values while our simulations are sample estimates. In a few lessons we will discuss estimators in more detail. 15.5 Covariance/Correlation We have discussed expected values of random variables and functions of random variables in a joint context. It would be helpful to have some kind of consistent measure to describe how two random variables are related to one another. Covariance and correlation do just that. It is important to understand that these are measures of a linear relationship between variables. Consider two random variables \\(X\\) and \\(Y\\). (We could certainly consider more than two, but for demonstration, lets consider only two for now). The covariance between \\(X\\) and \\(Y\\) is denoted as \\(\\mbox{Cov}(X,Y)\\) and is found by: \\[ \\mbox{Cov}(X,Y)=\\mbox{E}\\left[(X-\\mbox{E}(X))(Y-\\mbox{E}(Y))\\right] \\] We can simplify this expression to make it a little more usable: \\[ \\mbox{Cov}(X,Y)=\\mbox{E}\\left[(X-\\mbox{E}(X))(Y-\\mbox{E}(Y))\\right] = \\mbox{E}\\left[XY - Y\\mbox{E}(X) - X\\mbox{E}(Y) + \\mbox{E}(X)\\mbox{E}(Y)\\right] \\] \\[ \\mbox{E}(XY) - \\mbox{E}(Y\\mbox{E}(X)) - \\mbox{E}(X\\mbox{E}(Y)) + \\mbox{E}(X)\\mbox{E}(Y) = \\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)-\\mbox{E}(X)\\mbox{E}(Y)+\\mbox{E}(X)\\mbox{E}(Y) \\] Thus, \\[ \\mbox{Cov}(X,Y)=\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y) \\] This expression is a little easier to use, since its typically straightforward to find each of these quantities. It is important to note that while variance is a positive quantity, covariance can be positive or negative. A positive covariance implies that as the value of one variable increases, the other tends to increase. This is a statement about a linear relationship. Likewise, a negative covariance implies that as the value of one variable increases, the other tends to decrease. Example: An example of positive covariance is human height and weight. As height increase, weight tends to increase. An example of negative covariance is gas mileage and car weight. As car weight increases, gas mileage decreases. Remember that if \\(a\\) and \\(b\\) are constants, \\(\\mbox{E}(aX+b) =a\\mbox{E}(X)+b\\) and \\(\\mbox{Var}(aX+b)=a^2\\mbox{Var}(X)\\). Similarly, if \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are all constants, \\[ \\mbox{Cov}(aX+b,cY+d)=ac\\mbox{Cov}(X,Y) \\] One disadvantage of covariance is its dependence on the scales of the random variables involved. This makes it difficult to compare covariances of multiple sets of variables. Correlation avoids this problem. Correlation is a scaled version of covariance. It is denoted by \\(\\rho\\) and found by: \\[ \\rho = \\frac{\\mbox{Cov}(X,Y)}{\\sqrt{\\mbox{Var}(X)\\mbox{Var}(Y)}} \\] While covariance could take on any real number, correlation is bounded by -1 and 1. Two random variables with a correlation of 1 are said to be perfectly positively correlated, while a correlation of -1 implies perfect negative correlation. Two random variables with a correlation (and thus covariance) of 0 are said to be uncorrelated, that is they do not have a linear relationship but could have a non-linear relationship. This last point is important; random variables with no relationship will have a 0 covariance. However, a 0 covariance only implies that the random variables do not have a linear relationship. Lets look at some plots, Figures 15.1, 15.2, 15.3, and 15.4 of different correlations. Remember that the correlation we are calculating in this section is for the population, while the plots are showing sample points from a population. Figure 15.1: Correlation of 1 Figure 15.2: Correlation of .8 Figure 15.3: Correlation of .5 Figure 15.4: Correlation of 0 15.5.1 Variance of sums Suppose \\(X\\) and \\(Y\\) are two random variables. Then, \\[ \\mbox{Var}(X+Y)=\\mbox{E}\\left[(X+Y-\\mbox{E}(X+Y))^2\\right]=\\mbox{E}[(X+Y)^2]-\\left[\\mbox{E}(X+Y)\\right]^2 \\] In the last step, we are using the alternative expression for variance (\\(\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2\\)). Evaluating: \\[ \\mbox{Var}(X+Y)=\\mbox{E}(X^2)+\\mbox{E}(Y^2)+2\\mbox{E}(XY)-\\mbox{E}(X)^2-\\mbox{E}(Y)^2-2\\mbox{E}(X)\\mbox{E}(Y) \\] Regrouping the terms: \\[ =\\mbox{E}(X^2)-\\mbox{E}(X)^2+\\mbox{E}(Y^2)-\\mbox{E}(Y)^2+2\\left(\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)\\right)=\\mbox{Var}(X)+\\mbox{Var}(Y)+2\\mbox{Cov}(X,Y) \\] Example: Let \\(X\\) and \\(Y\\) be defined as above. Find \\(\\mbox{Cov}(X,Y)\\), \\(\\rho\\), and \\(\\mbox{Var}(X+Y)\\). \\[ \\mbox{Cov}(X,Y)=\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)=0.9-0.92*0.97=0.0076 \\] \\[ \\rho=\\frac{\\mbox{Cov}(X,Y)}{\\sqrt{\\mbox{Var}(X)\\mbox{Var}(Y)}} \\] Quickly, \\(\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2= 1.34-0.92^2 =0.4936\\) and \\(\\mbox{Var}(Y)=0.6691\\). So, \\[ \\rho=\\frac{0.0076}{\\sqrt{0.4936*0.6691}}=0.013 \\] With such a low \\(\\rho\\), we would say that \\(X\\) and \\(Y\\) are only slightly positively correlated. \\[ \\mbox{Var}(X+Y)=\\mbox{Var}(X)+\\mbox{Var}(Y)+2\\mbox{Cov}(X,Y)=0.4936+0.6691+2*0.0076=1.178 \\] 15.6 Independence Two random variables \\(X\\) and \\(Y\\) are said to be independent if their joint pmf/pdf is the product of their marginal pmfs/pdfs: \\[ f_{X,Y}(x,y)=f_X(x)f_Y(y) \\] If \\(X\\) and \\(Y\\) are independent, then \\(\\mbox{Cov}(X,Y) = 0\\). The converse is not necessarily true, however because they could have a non-linear relationship. For a discrete distribution, you must check that each cell, joint probabilities, are equal to the product of the marginal probability. Back to our joint pmf from above: \\[ \\begin{array}{cc|ccc} &amp; &amp; &amp; \\textbf{Y} &amp; \\\\ &amp; &amp; 0 &amp; 1 &amp; 2 \\\\&amp;\\hline0 &amp; 0.10 &amp; 0.08 &amp; 0.11 \\\\\\textbf{X} &amp;1 &amp; 0.18 &amp; 0.20 &amp; 0.12 \\\\&amp;2 &amp; 0.07 &amp; 0.05 &amp; 0.09 \\end{array} \\] The marginal pmf of \\(X\\) is \\[ f_X(x) = \\left\\{\\begin{array}{ll} 0.29, &amp; x=0 \\\\ 0.50, &amp; x=1 \\\\ 0.21, &amp; x=2 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] The marginal pmf of \\(Y\\) is \\[ f_Y(y) = \\left\\{\\begin{array}{ll} 0.35, &amp; y=0 \\\\ 0.33, &amp; y=1 \\\\ 0.32, &amp; y=2 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] Checking the first cell, we immediately see that \\(f_{X,Y}(x=0,y=0) \\neq f_{X}(x=0)\\cdot f_{Y}(y=0)\\) so \\(X\\) and \\(Y\\) are not independent. An easy way to determine if continuous variables are independent is to first check that the domain only contains constants, it is rectangular, and second that the joint pdf can be written as a product of a function of \\(X\\) only and a function of \\(Y\\) only. Thus for our examples above even though the domains were rectangular, in \\(f(x,y)=xy\\), \\(X\\) and \\(Y\\) were independent while \\(f(x,y)=x+y\\) they were not. 15.7 Conditional expectation An important idea in graph theory, network analysis, Bayesian networks, and queuing theory is conditional expectation. We will only briefly introduce the ideas here so that you have a basic understanding. This does not imply it is not an important topic. Lets start with a simple example to illustrate the ideas. Example: Sam will read either one chapter of his history book or one chapter of his philosophy book. If the number of misprints in a chapter of his history book is Poisson with mean 2 and if the number of misprints in a chapter of his philosophy book is Poisson with mean 5, then assuming Sam is equally likely to choose either book, what is the expected number of misprints that Sam will find? Note: in the next lesson we are working with transformations and could attack the problem using that method. First lets use simulation to get an idea what value the answer should be and then use algebraic techniques and definitions we have learned in this course. Simulate 5000 reads from the history book and 5000 from philosophy and combine: set.seed(2011) my_data&lt;-data.frame(misprints=c(rpois(5000,2),rpois(5000,5))) head(my_data) ## misprints ## 1 1 ## 2 1 ## 3 2 ## 4 1 ## 5 2 ## 6 4 dim(my_data) ## [1] 10000 1 Figure 15.5 is a histogram of the data. gf_histogram(~misprints,data=my_data,breaks=seq(-0.5, 15.5, by=1)) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Number of Misprints&quot;) Figure 15.5: Misprints from combined history and philosphy books. Or as a bar chart in Figure 15.6 gf_bar(~misprints,data=my_data)%&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Number of Misprints&quot;) Figure 15.6: Misprints from combined history and philosphy books as a bar chart. And now find the average. mean(~misprints,data=my_data) ## [1] 3.4968 Now for a mathematical solution. Let \\(X\\) stand for the number of misprints and \\(Y\\) for the book, to make \\(Y\\) a random variable lets call 0 history and 1 philosophy. Then \\(E[X|Y]\\) is the expected number of misprints given the book. This is a conditional expectation. For example \\(E[X|Y=0]\\) is the expected number of misprints in the history book. Now here is the tricky part, without specifying a value of \\(Y\\), called a realization, this expectation function is a random variable that depends on \\(Y\\). In other words, if we dont know the book, the expected number of misprints depends on \\(Y\\) and thus is a random variable. If we take the expected value of this random variable we get \\[E[X]=E[E[X|Y]]\\] The inner expectation in the right hand side is for the conditional distribution and the outer is for the marginal with respect to \\(Y\\). This seems confusing, so lets go back to our example. \\[E[X]=E[E[X|Y]]\\] \\[=E[X|Y=0] \\cdot \\mbox{P}(Y=0)+E[X|Y=1] \\cdot \\mbox{P}(Y=1)\\] \\[=2*\\frac{1}{2}+5*\\frac{1}{2}=\\frac{7}{2}=3.5\\] These ideas are going to be similar for continuous random variables. Note that we can also use conditional expectations to find probabilities but that is beyond the scope of this course. 15.8 Homework Problems 1. Let \\(X\\) and \\(Y\\) be continuous random variables with joint pdf: \\[ f_{X,Y}(x,y)=x + y \\] where \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 1\\). Find \\(\\mbox{E}(X)\\) and \\(\\mbox{E}(Y)\\). Find \\(\\mbox{Var}(X)\\) and \\(\\mbox{Var}(Y)\\). Find \\(\\mbox{Cov}(X,Y)\\) and \\(\\rho\\). Are \\(X\\) and \\(Y\\) independent? Find \\(\\mbox{Var}(3X+2Y)\\). 2. Optional - not difficult but does have small Calc III idea. Let \\(X\\) and \\(Y\\) be continuous random variables with joint pmf: \\[ f_{X,Y}(x,y)=1 \\] "],["TRANS.html", "Chapter 16 Transformations 16.1 Objectives 16.2 Transformations 16.3 Homework Problems", " Chapter 16 Transformations 16.1 Objectives Given a discrete random variable, determine the distribution of a transformation of that random variable. Given a continuous random variable, use the cdf method to determine the distribution of a transformation of that random variable. Use simulation methods to find the distribution of a transform of single or multivariate random variables. 16.2 Transformations Throughout our coverage of random variables, we have mentioned transformations of random variables. These have been in the context of linear transformations. We have discussed expected value and variance of linear transformations. Recall that \\(\\mbox{E}(aX+b)=a\\mbox{E}(X)+b\\) and \\(\\mbox{Var}(aX+b)=a^2\\mbox{Var}(X)\\). In this lesson, we will discuss transformations of random variables in general, beyond the linear case. 16.2.1 Transformations of discrete random variables Let \\(X\\) be a discrete random variable and let \\(g\\) be a function. The variable \\(Y=g(X)\\) is a discrete random variable with pmf: \\[ f_Y(y)=\\mbox{P}(Y=y)=\\sum_{g(x)=y}\\mbox{P}(X=x)=\\sum_{g(x)=y}f_X(x) \\] An example would help since the notation can be confusing. Example: Suppose \\(X\\) is a discrete random variable with pmf: \\[ f_X(x)=\\left\\{\\begin{array}{ll} 0.05, &amp; x=-2 \\\\ 0.10, &amp; x=-1 \\\\ 0.35, &amp; x=0 \\\\ 0.30, &amp; x=1 \\\\ 0.20, &amp; x=2 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] Find the pmf for \\(Y=X^2\\). It helps to identify the domain of \\(Y\\). Since the domain of \\(X\\) is \\(S_X=\\{-2,-1,0,1,2\\}\\), the domain of \\(Y\\) is \\(S_Y=\\{0,1,4\\}\\). \\[ f_Y(0)=\\sum_{x^2=0}f_X(x)=f_X(0)=0.35 \\] \\[ f_Y(1)=\\sum_{x^2=1}f_X(x)=f_X(-1)+f_X(1)=0.1+0.3=0.4 \\] \\[ f_Y(4)=\\sum_{x^2=4}f_X(x)=f_X(-2)+f_X(2)=0.05+0.2=0.25 \\] So, \\[ f_Y(y)=\\left\\{\\begin{array}{ll} 0.35, &amp; y=0 \\\\ 0.4, &amp; y=1 \\\\ 0.25, &amp; y=4 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] It also helps to confirm that these probabilities add to one, which they do. This is the pmf of \\(Y=X^2\\). The key idea is to find the domain of the new random variable and then go back to the original random variable and sum all the probabilities that get mapped into that new domain element. 16.2.2 Transformations of continuous random variables The methodology above will not work directly in the case of continuous random variables. This is because in the continuous case, the pdf, \\(f_X(x)\\), represents density and not probability. 16.2.3 The cdf method The cdf method can be used for transformations of continuous random variables. The idea is to find the cdf of the new random variable and then by way of the fundamental theorem of calculus. Suppose \\(X\\) is a continuous random variable with cdf \\(F_X(x)\\). Let \\(Y=g(X)\\). We can find the cdf of \\(Y\\) as: \\[ F_Y(y)=\\mbox{P}(Y\\leq y)=\\mbox{P}(g(X)\\leq y)=\\mbox{P}(X\\leq g^{-1}(y))=F_X(g^{-1}(y)) \\] To get the pdf of \\(Y\\), we would need to take the derivative of the cdf. This method requires the transformation function to have an inverse. Sometimes, we break the domain of the original random variables into regions where an inverse of the transformation function exists. Example: Let \\(X\\sim \\textsf{Unif}(0,1)\\) and let \\(Y=X^2\\). Find the pdf of \\(Y\\). Before we start, lets think about this. We are randomly taking numbers between 0 and 1 and then squaring them. Squaring a positive number less than 1 makes it even smaller. We thus suspect the pdf of \\(Y\\) will have larger density near 0 than 1. The shape is hard to determine so lets do some math. Since \\(X\\) has the uniform distribution, we know that \\(F_X(x)=x\\) for \\(0\\leq x \\leq 1\\). So, \\[ F_Y(y)=\\mbox{P}(Y\\leq y)=\\mbox{P}(X^2\\leq y)=\\mbox{P}(X\\leq \\sqrt{y})=F_X\\left(\\sqrt{y}\\right)=\\sqrt{y} \\] Taking the derivative of this yields: \\[ f_Y(y)=\\frac{1}{2\\sqrt{y}} \\] for \\(0 &lt; y \\leq 1\\) and 0 otherwise. Notice we cant have \\(y=0\\) since we would be dividing by zero. This is not a problem since we have a continuous distribution. We could verify this a proper pdf by determining if the pdf integrates to 1 over the domain: \\[ \\int_0^1 \\frac{1}{2\\sqrt{y}} \\,\\mathrm{d}y = \\sqrt{y}\\bigg|_0^1 = 1 \\] We can also do this using R but we first have to create a function that can take vector input. y_pdf &lt;- function(y) { 1/(2*sqrt(y)) } y_pdf&lt;- Vectorize(y_pdf) integrate(y_pdf,0,1) ## 1 with absolute error &lt; 2.9e-15 Notice that since the domain of the original random variable was non-negative, the squared function had an inverse. The pdf of the random variable \\(Y\\) is plotted in Figure 16.1. gf_line(y_pdf(seq(0.01,1,.01))~seq(0.01,1,.01),xlab=&quot;Y&quot;,ylab=expression(f(y))) %&gt;% gf_theme(theme_bw()) Figure 16.1: The pdf of the transformed random variable \\(Y\\). We can see that the density is much larger at we approach 0. 16.2.4 The pdf method - Optional The cdf method of transforming continuous random variables also yields to another method called the pdf method. Recall that the cdf method tells us that if \\(X\\) is a continuous random variable with cdf \\(F_X\\), and \\(Y=g(X)\\), then \\[ F_Y(y)=F_X(g^{-1}(y)) \\] We can find the pdf of \\(Y\\) by differentiating the cdf: \\[ f_Y(y)=\\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}y}F_Y(y)=\\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}y} F_X(g^{-1}(y)) = f_X(g^{-1}(y))\\bigg| \\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}y} g^{-1}(y) \\bigg| \\] So, as long as \\(g^{-1}\\) is differentiable, we can use this method to directly obtain the pdf of \\(Y\\). Note that in some texts, the portion of this expression \\(\\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}y} g^{-1}(y)\\) is sometimes referred to as the Jacobian. We need to take the absolute value of the transformation function \\(g(x)\\) because if it is a decreasing function, we have \\[ F_Y(y)=\\mbox{P}(Y\\leq y)=\\mbox{P}(g(X) \\leq y)=\\mbox{P}(X \\geq g^{-1}(y))= 1 - F_X(g^{-1}(y)) \\] Exercise: Repeat the previous example using the pdf method. Since \\(X\\) has the uniform distribution, we know that \\(f_X(x)=1\\) for \\(0\\leq x \\leq 1\\). Also, \\(g(x)=x^2\\) and \\(g^{-1}(y)=\\sqrt{y}\\), which is differentiable. So, \\[ f_Y(y)=f_X(\\sqrt{y})\\bigg|\\frac{\\,\\mathrm{d}}{\\,\\mathrm{d}y} \\sqrt{y}\\bigg| = \\frac{1}{2\\sqrt{y}} \\] 16.2.5 Simulation We can also get an estimate of the distribution by simulating the random variable. If we have the cdf and can find its inverse, then just like we did in an earlier lesson, we sample from a uniform distribution and apply the inverse to get the distribution. In an earlier lesson we had Let \\(X\\) be a continuous random variable with \\(f_X(x)=2x\\) where \\(0 \\leq x \\leq 1\\). Now lets find the distribution of \\(Y = \\ln{X}\\). The cdf of \\(X\\) is \\(F_X(x)=x^2\\) where \\(0 \\leq x \\leq 1\\). We will draw a uniform random variable and then take the square root. We will replicate this 10,000 times. In R our code, which we have done before, is: results &lt;- do(10000)*sqrt(runif(1)) Remember, we are using the square root because we want the inverse of the cdf and not, for this method, the inverse of the transformation function as when we were using the mathematical method. This can be a point of confusion. inspect(results) ## ## quantitative variables: ## name class min Q1 median Q3 max mean ## ...1 sqrt numeric 0.003822289 0.5034669 0.7075768 0.8659581 0.9998788 0.6682975 ## sd n missing ## ...1 0.2343216 10000 0 Figure 16.2 is a density plot of the simulated original random variable. results %&gt;% gf_density(~sqrt,xlab=&quot;X&quot;) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(y=&quot;&quot;) Figure 16.2: The density plot of the original using simulation. Now to find the distribution of \\(Y\\) we just apply the transformation. y_results &lt;- results %&gt;% transmute(y=log(sqrt)) Figure 16.3 is the density plot of the transformed random variable from the simulation. y_results %&gt;% gf_density(~y,xlab=&quot;X&quot;) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(y=&quot;&quot;) Figure 16.3: The density plot of the transformed random variable from the simulation. inspect(y_results) ## ## quantitative variables: ## name class min Q1 median Q3 max ## ...1 y numeric -5.566906 -0.6862373 -0.3459092 -0.1439187 -0.0001212101 ## mean sd n missing ## ...1 -0.4957383 0.4968033 10000 0 16.2.6 Multivariate Transformations Heres the scenario. Suppose \\(X\\) and \\(Y\\) are independent random variables, both uniformly distributed on \\([5,6]\\). \\[ X\\sim \\textsf{Unif}(5,6)\\hspace{1.5cm} Y\\sim \\textsf{Unif}(5,6) \\] Let \\(X\\) be your arrival time for dinner and \\(Y\\) your friends arrival time. We picked 5 to 6 because this is the time in the evening we want to meet. Assume you travel independently. Define \\(Z\\) as a transformation of \\(X\\) and \\(Y\\) such that \\(Z=|X-Y|\\). Thus \\(Z\\) is the absolute value of the difference between your arrival times. The units for \\(Z\\) are hours. We would like to explore the distribution of \\(Z\\). We could do this via calc III methods but we will simulate instead. We can use R to obtain simulated values from \\(X\\) and \\(Y\\) (and thus find \\(Z\\)). First, simulate 100,000 observations from the uniform distribution with parameters 5 and 6. Assign those random observations to a variable. Next, repeat that process, assigning those to a different variable. These two vectors represent your simulated values from \\(X\\) and \\(Y\\). Finally, obtain your simulated values of \\(Z\\) by taking the absolute value of the difference. Exercise: Complete the code on your own before looking at the code below. set.seed(354) results &lt;- do(100000)*abs(diff(runif(2,5,6))) head(results) ## abs ## 1 0.03171229 ## 2 0.77846706 ## 3 0.29111599 ## 4 0.06700434 ## 5 0.08663187 ## 6 0.40622840 Now, plot the estimated distribution. Figure 16.4 is the result. results %&gt;% gf_density(~abs) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;|X-Y|&quot;,y=&quot;&quot;) Figure 16.4: The density of the absolute value of the difference in random variables. Or as a histogram in Figure 16.5. results %&gt;% gf_histogram(~abs)%&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;|X-Y|&quot;,y=&quot;&quot;) Figure 16.5: Histogram of the absolute value of the difference in random variables. inspect(results) ## ## quantitative variables: ## name class min Q1 median Q3 max mean ## ...1 abs numeric 1.265667e-06 0.133499 0.2916012 0.4990543 0.9979459 0.332799 ## sd n missing ## ...1 0.2358863 100000 0 Exercise: Now suppose whomever arrives first will only wait 5 minutes and then leave. What is the probability you eat together? data.frame(results) %&gt;% summarise(mean(abs&lt;=5/60)) ## mean(abs &lt;= 5/60) ## 1 0.15966 Exercise: How long should the first person wait so that there is at least a 50% probability of you eating together? Lets write a function to find the cdf. z_cdf &lt;- function(x) { mean(results$abs&lt;=x) } z_cdf&lt;- Vectorize(z_cdf) Now test for 5 minutes to make sure our function is correct since we determined above that this value should be 0.15966. z_cdf(5/60) ## [1] 0.15966 Lets plot to see what the cdf looks like. gf_line(z_cdf(seq(0,1,.01))~seq(0,1,.01),xlab=&quot;Time Difference&quot;,ylab=&quot;CDF&quot;) %&gt;% gf_theme(theme_bw()) It looks like some where around 15 minutes, a quarter of an hour. But we will find a better answer by finding the root. In the code that follows we want to find where the cdf equals 0.5. The function uniroot() solves the given equations for roots so we want to put in the cdf minus 0.5. In other words, uniroot() want to solve \\(f(x)=0\\) for x. uniroot(function(x)z_cdf(x)-.5,c(.25,35))$root ## [1] 0.2916077 So it is actually 0.292 hours, 17.5 minutes. So round up and wait 18 minutes. 16.3 Homework Problems 1. Let \\(X\\) be a random variable and let \\(g\\) be a function. By this point, it should be clear that \\(\\mbox{E}[g(X)]\\) is not necessarily equal to \\(g(\\mbox{E}[X])\\). Let \\(X\\sim \\textsf{Expon}(\\lambda=0.5)\\) and \\(g(X)=X^2\\). We know that \\(\\mbox{E}(X)=\\frac{1}{0.5}=2\\) so \\(g(\\mbox{E}(X))=\\mbox{E}(X)^2=4\\). Use R to find \\(\\mbox{E}[g(X)]\\). Make use of the fact that R has rexp() built into it, so you dont have to create your own random variable generator. 2. Let \\(X\\sim \\textsf{Binom}(n,\\pi)\\). What is the pmf for \\(Y = X+3\\)? Make sure you specify the domain of \\(Y\\). [Note, we have used \\(p\\) for the probability of success in a binomial distribution in past chapters but some references use \\(\\pi\\) instead.] 3. Let \\(X\\sim \\textsf{Expon}(\\lambda)\\). Let \\(Y=X^2\\). Find the pdf of \\(Y\\). 4. OPTIONAL: In exercise 3, you found the pdf of \\(Y=X^2\\) when \\(X\\sim \\textsf{Expon}(\\lambda)\\). Rearrange the pdf to show that \\(Y\\sim \\textsf{Weibull}\\) and find the parameters of that distribution. 5. You are on a team of two. You are both tasked to complete an exercise. The time it takes you, \\(T_1\\), and likewise, your teammate, \\(T_2\\), to complete the exercise are independent random variables. Exercise completion time, in minutes, is distributed with the following pdf: \\[ f_T(t)= \\frac{-t}{200}+\\frac{3}{20}; 10 \\leq t \\leq30 \\] "],["EST.html", "Chapter 17 Estimation Methods 17.1 Objectives 17.2 Transition 17.3 Estimation 17.4 Method of Moments 17.5 Maximum likelihood 17.6 Homework Problems", " Chapter 17 Estimation Methods 17.1 Objectives Obtain a method of moments estimate of a parameter or set of parameters. Given a random sample from a distribution, obtain the likelihood function. Obtain a maximum likelihood estimate of a parameter or set of parameters. 17.2 Transition We started this course with descriptive models of data and then moved onto probability models. In these probability models, we have been characterizing experiments and random processes using both theory and simulation. These models are using a model about a random event to make decisions about data. These models are about the population and are used to make decisions about samples and data. For example, suppose we flip a fair coin 10 times, and record the number of heads. The population is the collection of all possible outcomes of this experiment. In this case, the population is infinite, as we could run this experiment repeatedly without limit. If we assume, model, the number of heads as a binomial distribution, we know the exact distribution of the outcomes. For example, we know that exactly 24.61% of the time, we will obtain 5 heads out of 10 flips of a fair coin. We can also use the model to characterize the variance, that is when it does not equal 5 and how much different from 5 it will be. However, these probability models are highly dependent on the assumptions and the values of the parameters. From this point on in the course, we will focus on statistical models. Statistical models describe one or more variables and their relationships. We use these models to make decisions about the population, to predict future outcomes, or both. Often we dont know the true underlying process; all we have is a sample of observations and perhaps some context. Using inferential statistics, we can draw conclusions about the underlying process. For example, suppose we are given a coin and we dont know whether it is fair. So, we flip it a number of times to obtain a sample of outcomes. We can use that sample to decide whether the coin could be fair. In some sense, weve already explored some of these concepts. In our simulation examples, we have drawn observations from a population of interest and used those observations to estimate characteristics of another population or segment of the experiment. For example, we explored random variable \\(Z\\), where \\(Z=|X - Y|\\) and \\(X\\) and \\(Y\\) were both uniform random variables. Instead of dealing with the distribution of \\(Z\\) directly, we simulated many observations from \\(Z\\) and used this simulation to describe the behavior of \\(Z\\). Statistical models and probability models are not separate. In statistical models we find relationships, the explained portion of variation, and use probability models for the remaining random variation. In Figure 17.1, we demonstrate this relationship between the two types of models. In the first part of our studies, we will use univariate data in statistical models to estimate the parameters of a probability model. From there we will develop more sophisticated models to include multivariate models. Figure 17.1: A graphical representation of probability and statistics. In probability, we describe what we expect to happen if we know that underlying process; in statistics, we dont know the underlying process, and must infer based on representative samples. 17.3 Estimation Recall that in probability models, we have complete information about the population and we use that to describe the expected behavior of samples from that population. In statistics we are given a sample from a population about which we know little or nothing. In this lesson, we will discuss estimation. Given a sample, we would like to estimate population parameters. There are several ways to do that. We will discuss two methods: method of moments and maximum likelihood. 17.4 Method of Moments Recall earlier we discussed moments. We can refer to \\(\\mbox{E}(X) = \\mu\\) as the first moment or mean. Further, we can refer to \\(\\mbox{E}(X^k)\\) as the \\(k\\)th central moment and \\(\\mbox{E}[(X-\\mu)^k]\\) as the \\(k\\) moment around the mean. The second moment around the mean is also known as variance. It is important to point out that these are POPULATION moments and are typically some function of the parameters of a probability model. Suppose \\(X_1,X_2,...,X_n\\) is a sequence of independent, identically distributed random variables with some distribution and parameters \\(\\boldsymbol{\\theta}\\). When provided with a random sample of data, we will not know the population moments. However, we can obtain sample moments. The \\(k\\)th central sample moment is denoted by \\(\\hat{\\mu}_k\\) and is given by \\[ \\hat{\\mu}_k = \\frac{1}{n}\\sum_{i=1}^n x_i^k \\] The \\(k\\)th sample moment around the mean is denoted by \\(\\hat{\\mu}&#39;_k\\) and is given by \\[ \\hat{\\mu}&#39;_k=\\frac{1}{n} \\sum_{i=1}^n (x_i-\\bar{x})^k \\] The value \\(\\hat{\\mu}\\) is read mu-hat. The hat denotes that the value is an estimate. We can use the sample moments to estimate the population moments since the population moments are usually functions of a distributions parameters, \\(\\boldsymbol{\\theta}\\). Thus, we can solve for the parameters to obtain method of moments estimates of \\(\\boldsymbol{\\theta}\\). This is all technical, so lets look at an example. Example: Suppose \\(x_1,x_2,...,x_n\\) is an iid, independent and identically distributed, sample from a uniform distribution \\(\\textsf{Unif}(0,\\theta)\\), and we dont know \\(\\theta\\). That is, our data consists of positive random numbers but we dont know the upper bound. Find the method of moments estimator for \\(\\theta\\), the upper bound. We know that if \\(X\\sim \\textsf{Unif}(a,b)\\), then \\(\\mbox{E}(X)=\\frac{a+b}{2}\\). So, in this case, \\(\\mbox{E}(X)={\\theta \\over 2}\\). This is the first population moment. We can estimate this with the first sample moment, which is just the sample mean: \\[ \\hat{\\mu}_1=\\frac{1}{n}\\sum_{i=1}^n x_i = \\bar{x} \\] Our best guess for the first population moment (\\(\\theta/2\\)) is the first sample moment (\\(\\bar{x}\\)). From a common sense perspective, we are hoping that the sample moment will be close in value to the population moment, so we can set them equal and solve for the unknown population parameter. This is essentially what we were doing in our simulations of probability models. Solving for \\(\\theta\\) yields our method of moments estimator for \\(\\theta\\): \\[ \\hat{\\theta}_{MoM}=2\\bar{x} \\] Note that we could have used the second moments about the mean as well. This is less intuitive but still applicable. In this case we know that if \\(X\\sim \\textsf{Unif}(a,b)\\), then \\(\\mbox{Var}(X)=\\frac{(b - a)^2}{12}\\). So, in this case, \\(\\mbox{Var}(X)=\\frac{\\theta ^2}{ 12}\\). We use the second sample moment about the mean \\(\\hat{\\mu}&#39;_2=\\frac{1}{n} \\sum_{i=1}^n (x_i-\\bar{x})^2\\) which is not quite the sample variance. In fact, the sample variance is related to the second sample moment about the mean by \\(\\hat{\\mu}&#39;_2 = s^2 \\frac{n}{n-1}\\). Setting the population moment and sample moment equal and solving we get \\[ \\hat{\\theta}_{MoM}=\\sqrt{\\frac{12n}{n-1}}s \\] To decide which is better we need a criteria of comparison. This is beyond the scope of this class, but some common criteria are unbiased and minimum variance. The method of moments can be used to estimate more than one parameter as well. We simply would have to incorporate higher order moments. Example: Suppose we take an iid sample from the normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\). Find method of moments estimates of \\(\\mu\\) and \\(\\sigma\\). First, we remember that we know two population moments for the normal distribution: \\[ \\mbox{E}(X)=\\mu \\hspace{1cm} \\mbox{Var}(X)=\\mbox{E}[(X-\\mu)^2]=\\sigma^2 \\] Setting these equal to the sample moments yields: \\[ \\hat{\\mu}_{MoM}=\\bar{x} \\hspace{1cm} \\hat{\\sigma}_{MoM} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar{x})^2} \\] Again, we notice that the estimate for \\(\\sigma\\) is different from sample standard deviation discussed earlier in the semester. The reason for this is a property of estimators called unbiased. Notice that if we treat the data points as random variables then the estimators are random variables. We can then take the expected value of the estimator and if this equals the parameter being estimated, then it is unbiased. Mathematically, this is written \\[ E(\\hat{\\theta})=\\theta \\] Unbiased is not a required property for an estimated but many practitioners find it desirable. In words, unbiased means that on average the estimator will equal the true value. Sample variance using \\(n-1\\) in the denominator is an unbiased estimate of the population variance. Exercise: You shot 25 free throws and make 21. Assuming a binomial model fits. Find an estimate of the probability of making a free throw. There are two ways to approach this problem depending on how we define the random variable. In the first case we will use a binomial random variable, \\(X\\) the number of made free throws in 25 attempts. In this case, we only ran the experiment once and have the observed result of 21. Recall for the binomial \\(E(X)=np\\) where \\(n\\) is the number of attempts and \\(p\\) is the probability of success. The sample mean is 21 since we only have one data point. Using the method of moments, we set the first population mean equal to the first sample mean \\(np=\\sum{x_i}\\) or \\(25 \\hat{p} = 21\\). Thus \\(\\hat{p} = \\frac{21}{25}\\). A second approach is to let \\(X_i\\) be a single free throw, we have a Bernoulli random variable. This variable takes on the values of 0 if we miss and 1 if we make the free throw. Thus we have 25 data points. For a Bernoulli random variable \\(E(X)=p\\). The sample is \\(\\bar{x} = \\frac{21}{25}\\). Using the method of moments, we set the sample mean equal to the population mean. We have \\(E(X) = \\hat{p} = \\bar{x} = \\frac{21}{25}\\). This is a natural estimate; we estimate our probability of success as the number of made free throws divided by the number of shots. As a side note, this is an unbiased estimator since \\[ E(\\hat{p})=E\\left( \\sum{\\frac{X_i}{n}} \\right) \\] \\[ = \\sum{E\\left( \\frac{X_i}{n} \\right)}= \\sum{ \\frac{E\\left(X_i\\right)}{n}}=\\sum{\\frac{p}{n}}=\\frac{np}{n}=p \\] 17.5 Maximum likelihood Recall that using method of moments involves finding values of the parameters that cause the population moments to be equal to the sample moments. Solving for the parameters yields method of moments estimates. Next we will discuss one more estimation method, maximum likelihood estimation. In this method, we are finding values of parameters that would make the observed data most likely. In order to do this, we first need to introduce the likelihood function. 17.5.1 Likelihood Function Suppose \\(x_1,x_2,...,x_n\\) is an iid random sample from a distribution with mass/density function \\(f_{X}(x;\\boldsymbol{\\theta})\\) where \\(\\boldsymbol{\\theta}\\) are the parameters. Lets take a second to explain this notation. We are using a bold symbol for \\(\\boldsymbol{\\theta}\\) to indicate it is a vector, that it can be one or more values. However, in the pmf/pdf \\(x\\) is not bold since it is a scalar variable. In our probability models we know \\(\\boldsymbol{\\theta}\\) and then use to model to make decision about the random variable \\(X\\). The likelihood function is denoted as \\(L(\\boldsymbol{\\theta};x_1,x_2,...,x_n) = L(\\boldsymbol{\\theta};\\boldsymbol{x})\\). Now we have multiple instances of the random variable, we use \\(\\boldsymbol{x}\\). Since our random sample is iid, independent and identically distributed, we can write the likelihood function as a product of the pmfs/pdfs: \\[ L(\\boldsymbol{\\theta};\\boldsymbol{x})=\\prod_{i=1}^n f_X(x_i;\\boldsymbol{\\theta}) \\] The likelihood function is really the pmf/pdf except instead of the variables being random and the parameter(s) fixed, the values of the variable are known and the parameter(s) are unknown. A note on notation, we are using the semicolon in the pdf and likelihood function to denote what is known or given. In the pmf/pdf the parameters are known and thus follow the semicolon. The opposite is the case in the likelihood function. Lets do an example to help understand these ideas. Example: Suppose we are presented with a coin and are unsure of its fairness. We toss the coin 50 times and obtain 18 heads and 32 tails. Let \\(\\pi\\) be the probability that a coin flip results in heads, we could use \\(p\\) but we are getting you used to the two different common ways to represent a binomial parameter. What is the likelihood function of \\(\\pi\\)? This is a binomial process, but each individual coin flip can be thought of as a Bernoulli experiment. That is, \\(x_1,x_2,...,x_{50}\\) is an iid sample from \\(\\textsf{Binom}(1,\\pi)\\) or, in other words, \\(\\textsf{Bernoulli}(\\pi)\\). Each \\(x_i\\) is either 1 or 0. The pmf of \\(X\\), a Bernoulli random variable, is simply: \\[ f_X(x;\\pi)= \\binom{1}{x} \\pi^x(1-\\pi)^{1-x} = \\pi^x(1-\\pi)^{1-x} \\] Notice this makes sense \\[ f_X(1)=P(X=1)= \\pi^1(1-\\pi)^{1-1}=\\pi \\] and \\[ f_X(0)=P(X=0)= \\pi^0(1-\\pi)^{1-0}=(1-\\pi) \\] Generalizing for any sample size \\(n\\), the likelihood function is: \\[ L(\\pi;\\boldsymbol{x})=\\prod_{i=1}^{n} \\pi^{x_i}(1-\\pi)^{1-x_i} = \\pi^{\\sum_{i=1}^{n} x_i}(1-\\pi)^{n-\\sum_{i=1}^{n} x_i} \\] For our example \\(n=50\\) and the \\[ L(\\pi;\\boldsymbol{x})=\\prod_{i=1}^{50} \\pi^{x_i}(1-\\pi)^{1-x_i} = \\pi^{18}(1-\\pi)^{32} \\] which makes sense because we had 18 successes, heads, and 32 failures, tails. The likelihood function is a function of the unknown parameter \\(\\pi\\). 17.5.2 Maximum Likelihood Estimation Once we have a likelihood function \\(L(\\boldsymbol{\\theta},\\boldsymbol{x})\\), we need to figure out which value of \\(\\boldsymbol{\\theta}\\) makes the data most likely. In other words, we need to maximize \\(L\\) with respect to \\(\\boldsymbol{\\theta}\\). Most of the time (but not always), this will involve simple optimization through calculus (i.e., take the derivative with respect to the parameter, set to 0 and solve for the parameter). When maximizing the likelihood function through calculus, it is often easier to maximize the log of the likelihood function, denoted as \\(l\\) and often referred to as the log-likelihood function: \\[ l(\\boldsymbol{\\theta};\\boldsymbol{x})= \\log L(\\boldsymbol{\\theta};\\boldsymbol{x}) \\] Note that since logarithm is one-to-one, onto and increasing, maximizing the log-likelihood function is equivalent to maximizing the likelihood function, and the maximum will occur at the same values of the parameters. We are using log because now we can take the derivative of a sum instead of a product, thus making it much easier. Example: Continuing our example. Find the maximum likelihood estimator for \\(\\pi\\). Recall that our likelihood function is \\[ L(\\pi;\\boldsymbol{x})= \\pi^{\\sum x_i}(1-\\pi)^{n-\\sum x_i} \\] Figure 17.2 is a plot of the likelihood function as a function of the unknown parameter \\(\\pi\\). ## Warning: geom_vline(): Ignoring `mapping` because `xintercept` was provided. Figure 17.2: Likelihood function for 18 successes in 50 trials By visual inspection, the value of \\(\\pi\\) that makes our data most likely, maximizes the likelihood function, is something a little less than 0.4, the actual value is 0.36 as indicated by the blue line in Figure 17.2. To maximize by mathematical methods, we need to take the derivative of the likelihood function with respect to \\(\\pi\\). We can do this because the likelihood function is a continuous function. Even though the binomial is a discrete random variable, its likelihood is a continuous function. We can find the derivative of the likelihood function by applying the product rule: \\[ {\\,\\mathrm{d}L(\\pi;\\boldsymbol{x})\\over \\,\\mathrm{d}\\pi} = \\left(\\sum x_i\\right) \\pi^{\\sum x_i -1}(1-\\pi)^{n-\\sum x_i} + \\pi^{\\sum x_i}\\left(\\sum x_i -n\\right)(1-\\pi)^{n-\\sum x_i -1} \\] We could simplify this, set to 0, and solve for \\(\\pi\\). However, it may be easier to use the log-likelihood function: \\[ l(\\pi;\\boldsymbol{x})=\\log L(\\pi;\\boldsymbol{x})= \\log \\left(\\pi^{\\sum x_i}(1-\\pi)^{n-\\sum x_i}\\right) = \\sum x_i \\log \\pi + (n-\\sum x_i)\\log (1-\\pi) \\] Now, taking the derivative does not require the product rule: \\[ {\\,\\mathrm{d}l(\\pi;\\boldsymbol{x})\\over \\,\\mathrm{d}\\pi}= {\\sum x_i \\over \\pi} - {n-\\sum x_i\\over (1-\\pi)} \\] Setting equal to 0 yields: \\[ {\\sum x_i \\over \\pi} ={n-\\sum x_i\\over (1-\\pi)} \\] Solving for \\(\\pi\\) yields \\[ \\hat{\\pi}_{MLE}={\\sum x_i \\over n} \\] Note that technically, we should confirm that the function is concave down at our critical value, ensuring that \\(\\hat{\\pi}_{MLE}\\) is, in fact, a maximum: \\[ {\\,\\mathrm{d}^2 l(\\pi;\\boldsymbol{x})\\over \\,\\mathrm{d}\\pi^2}= {-\\sum x_i \\over \\pi^2} - {n-\\sum x_i\\over (1-\\pi)^2} \\] This value is negative for all relevant values of \\(\\pi\\), so \\(l\\) is concave down and \\(\\hat{\\pi}_{MLE}\\) is a maximum. In the case of our example (18 heads out of 50 trials), \\(\\hat{\\pi}_{MLE}=18/50=0.36\\). This seems to make sense. Our best guess for the probability of heads is the number of observed heads divided by our number of trials. That was a great deal of algebra and calculus for what appears to be an obvious answer. However, in more difficult problems, it is not as obvious what to use for a MLE. 17.5.3 Numerical Methods When obtaining MLEs, there are times when analytical methods (calculus) are not feasible or not possible. In the Pruim book (Pruim 2011), there is a good example regarding data from Old Faithful at Yellowstone National Park. We need to load the fastR2 package for this example. library(fastR2) The faithful data set is preloaded into R and contains 272 observations of 2 variables: eruption time in minutes and waiting time until next eruption. If we plot eruption durations, we notice that the distribution appears bimodal, see Figure 17.3. Figure 17.3: Histogram of eruption durations of Old Faithful. Within each section, the distribution appears somewhat bell-curve-ish so well model the eruption time with a mixture of two normal distributions. In this mixture, a proportion \\(\\alpha\\) of our eruptions belong to one normal distribution and the remaining \\(1-\\alpha\\) belong to the other normal distribution. The density function of eruptions is given by: \\[ \\alpha f(x;\\mu_1,\\sigma_1)+(1-\\alpha)f(x;\\mu_2,\\sigma_2) \\] where \\(f\\) is the pdf of the normal distribution with parameters specified. We have five parameters to estimate: \\(\\alpha, \\mu_1, \\mu_2, \\sigma_1, \\sigma_2\\). Obviously, estimation through differentiation is not feasible and thus we will use numerical methods. This code is less in the spirit of tidyverse but we want you to see the example. Try to work your way through the code below: # Define function for pdf of eruptions as a mixture of normals dmix&lt;-function(x,alpha,mu1,mu2,sigma1,sigma2){ if(alpha &lt; 0) dnorm(x,mu2,sigma2) if(alpha &gt; 1) dnorm(x,mu1,sigma1) if(alpha &gt;= 0 &amp;&amp; alpha &lt;=1){ alpha*dnorm(x,mu1,sigma1)+(1-alpha)*dnorm(x,mu2,sigma2) } } Next write a function for the log-likelihood function. R is a vector based programming language so we send theta into the function as a vector argument. # Create the log-likelihood function loglik&lt;-function(theta,x){ alpha=theta[1] mu1=theta[2] mu2=theta[3] sigma1=theta[4] sigma2=theta[5] density&lt;-function(x){ if(alpha&lt;0) return (Inf) if(alpha&gt;1) return (Inf) if(sigma1&lt;0) return (Inf) if(sigma2&lt;0) return (Inf) dmix(x,alpha,mu1,mu2,sigma1,sigma2) } sum(log(sapply(x,density))) } Find the sample mean and standard deviation of the eruption data to use as starting points in the optimization routine. m&lt;-mean(faithful$eruptions) s&lt;-sd(faithful$eruptions) Use the function nlmax() to maximize the non-linear log-likelihood function. mle&lt;-nlmax(loglik,p=c(0.5,m-1,m+1,s,s),x=faithful$eruptions)$estimate mle ## [1] 0.3484040 2.0186065 4.2733410 0.2356208 0.4370633 So, according to our MLEs, about 34.84% of the eruptions belong to the first normal distribution (the one on the left). Furthermore the parameters of that first distribution are a mean of 2.019 and a standard deviation of 0.236. Likewise, 65.16% of the eruptions belong to the second normal with mean of 4.27 and standard deviation of 0.437. Plotting the density atop the histogram shows a fairly good fit: dmix2&lt;-function(x) dmix(x,mle[1],mle[2],mle[3],mle[4],mle[5]) #y_old&lt;-dmix2(seq(1,6,.01)) #x_old&lt;-seq(1,6,.01) #dens_data&lt;-data.frame(x=x_old,y=y_old) #faithful%&gt;% #gf_histogram(~eruptions,fill=&quot;cyan&quot;,color = &quot;black&quot;) %&gt;% # gf_curve(y~x,data=dens_data)%&gt;% # gf_theme(theme_bw()) %&gt;% # gf_labs(x=&quot;Duration in minutes&quot;,y=&quot;Count&quot;) hist(faithful$eruptions,breaks=40,freq=F,main=&quot;&quot;,xlab=&quot;Duration in minutes.&quot;) curve(dmix2,from=1,to=6,add=T) Figure 17.4: Histogram of eruption duration with estimated mixture of normals plotted on top. This is a fairly elaborate example but it is cool. You can see the power of the method and the software. 17.6 Homework Problems 1. In the Notes, we found that if we take a sample from the uniform distribution \\(\\textsf{Unif}(0,\\theta)\\), the method of moments estimate of \\(\\theta\\) is \\(\\hat{\\theta}_{MoM}=2\\bar{x}\\). Suppose our sample consists of the following values: \\[ 0.2 \\hspace{0.4cm} 0.9 \\hspace{0.4cm} 1.9 \\hspace{0.4cm} 2.2 \\hspace{0.4cm} 4.7 \\hspace{0.4cm} 5.1 \\] What is \\(\\hat{\\theta}_{MoM}\\) for this sample? What is an wrong with this estimate? Show that this estimator is unbiased. ADVANCED: Use simulation in R to find out how often the method of moment estimator is less the maximum observed value, (\\(\\hat{\\theta}_{MoM} &lt; \\max x\\)). Report an answer for various sizes of samples. You can just pick an arbitrary value for \\(\\theta\\) when you sample from the uniform. However, the minimum must be 0. 2. Let \\(x_1,x_2,...,x_n\\) be a simple random sample from an exponentially distributed population with parameter \\(\\lambda\\). Find \\(\\hat{\\lambda}_{MoM}\\). 3. Let \\(x_1,x_2,...,x_n\\) be an iid random sample from an exponentially distributed population with parameter \\(\\lambda\\). Find \\(\\hat{\\lambda}_{MLE}\\). 4. It is mathematically difficult to determine if the estimators found in questions 2 and 3 are unbiased. Since the sample mean is in the denominator; mathematically we may have to work with the joint pdf. So instead, use simulation to get an sense of whether the method of moments estimator for the exponential distribution is unbiased. 5. Find a maximum likelihood estimator for \\(\\theta\\) when \\(X\\sim\\textsf{Unif}(0,\\theta)\\). Compare this to the method of moments estimator we found. Hint: Do not take the derivative of the likelihood function. References "],["CS3.html", "Chapter 18 Hypothesis Testing Case Study 18.1 Objectives 18.2 Introduction 18.3 Foundation for inference 18.4 Randomization case study: gender discrimination 18.5 Homework Problems", " Chapter 18 Hypothesis Testing Case Study 18.1 Objectives Define and use properly in context all new terminology. Conduct a hypothesis test using a permutation test to include all 4 steps. 18.2 Introduction We now have the foundation to move onto statistical modeling. First we will begin with inference where we use the ideas of estimation and the variance of estimates to make decisions about the population. We will also briefly introduce the ideas of prediction. Then in the final block of material, we will examine some common linear models and use them both in prediction situations as well as inference. 18.3 Foundation for inference Suppose a professor randomly splits the students in class into two groups: students on the left and students on the right. If \\(\\hat{p}_{_L}\\) and \\(\\hat{p}_{_R}\\) represent the proportion of students who own an Apple product on the left and right, respectively, would you be surprised if \\(\\hat{p}_{_L}\\) did not exactly equal \\(\\hat{p}_{_R}\\)? While the proportions would probably be close to each other, they are probably not exactly the same. We would probably observe a small difference due to chance. Exercise: If we dont think the side of the room a person sits on in class is related to whether the person owns an Apple product, what assumption are we making about the relationship between these two variables?67 Studying randomness of this form is a key focus of statistical modeling. In this block, well explore this type of randomness in the context of several applications, and well learn new tools and ideas that can be applied to help make decisions from data. 18.4 Randomization case study: gender discrimination We consider a study investigating gender discrimination in the 1970s, which is set in the context of personnel decisions within a bank.68 The research question we hope to answer is, Are females discriminated against in promotion decisions made by male managers? 18.4.1 Variability within data The participants in this study were 48 male bank supervisors attending a management institute at the University of North Carolina in 1972. They were asked to assume the role of the personnel director of a bank and were given a personnel file to judge whether the person should be promoted to a branch manager position. The files given to the participants were identical, except that half of them indicated the candidate was male and the other half indicated the candidate was female. These files were randomly assigned to the subjects. Exercise: Is this an observational study or an experiment? How does the type of study impact what can be inferred from the results?69 For each supervisor we recorded the gender associated with the assigned file and the promotion decision. Using the results of the study summarized in the table below, we would like to evaluate if females are unfairly discriminated against in promotion decisions. In this study, a smaller proportion of females are promoted than males (0.583 versus 0.875), but it is unclear whether the difference provides convincing evidence that females are unfairly discriminated against. \\[ \\begin{array}{cc|ccc} &amp; &amp; &amp;\\textbf{Decision}\\\\ &amp; &amp; \\mbox{Promoted} &amp; \\mbox{Not Promoted} &amp; \\mbox{Total} \\\\ &amp; \\hline \\mbox{male} &amp; 21 &amp; 3 &amp; 24 \\\\ \\textbf{Gender}&amp; \\mbox{female} &amp; 14 &amp; 10 &amp; 24 \\\\ &amp; \\mbox{Total} &amp; 35 &amp; 13 &amp; 48 \\\\ \\end{array} \\] Thought Question: Statisticians are sometimes called upon to evaluate the strength of evidence. When looking at the rates of promotion for males and females in this study, why might we be tempted to immediately conclude that females are being discriminated against? The large difference in promotion rates (58.3% for females versus 87.5% for males) suggest there might be discrimination against women in promotion decisions. Most people come to this conclusion because they think these sample statistics are the actual population parameters. We cannot yet be sure if the observed difference represents discrimination or is just from random variability. Generally there is fluctuation in sample data; if we conducted the experiment again, we would get different values. We also wouldnt expect the sample proportions to be exactly equal, even if the truth was that the promotion decisions were independent of gender. To make a decision, we must understand the random variability and compare it with the observed difference. This question is a reminder that the observed outcomes in the sample may not perfectly reflect the true relationships between variables in the underlying population. The table shows there were 7 fewer promotions in the female group than in the male group, a difference in promotion rates of 29.2% \\(\\left( \\frac{21}{24} - \\frac{14}{24} = 0.292 \\right)\\). This observed difference is what we call a point estimate of the true effect. The point estimate of the difference is large, but the sample size for the study is small, making it unclear if this observed difference represents discrimination or whether it is simply due to chance. We label these two competing claims, chance or discrimination, as \\(H_0\\) and \\(H_A\\): \\(H_0\\): Null hypothesis. The variables gender and decision are independent. They have no relationship, and the observed difference between the proportion of males and females who were promoted, 29.2%, was due to chance. \\(H_A\\): Alternative hypothesis. The variables gender and decision are not independent. The difference in promotion rates of 29.2% was not due to chance, and equally qualified females are less likely to be promoted than males. Hypothesis testing These hypotheses are part of what is called a hypothesis test. A hypothesis test is a statistical technique used to evaluate competing claims using data. Often times, the null hypothesis takes a stance of no difference or no effect and thus is skeptical of the research claim. If the null hypothesis and the data notably disagree, then we will reject the null hypothesis in favor of the alternative hypothesis. Dont worry if you arent a master of hypothesis testing at the end of this lesson. Well discuss these ideas and details many times in this block. What would it mean if the null hypothesis, which says the variables gender and decision are unrelated, is true? It would mean each banker would decide whether to promote the candidate without regard to the gender indicated on the file. That is, the difference in the promotion percentages would be due to the way the files were randomly divided to the bankers, and the randomization just happened to give rise to a relatively large difference of 29.2%. Consider the alternative hypothesis: bankers were influenced by which gender was listed on the personnel file. If this was true, and especially if this influence was substantial, we would expect to see some difference in the promotion rates of male and female candidates. If this gender bias was against females, we would expect a smaller fraction of promotion recommendations for female personnel files relative to the male files. We will choose between these two competing claims by assessing if the data conflict so much with \\(H_0\\) that the null hypothesis cannot be deemed reasonable. If this is the case, and the data support \\(H_A\\), then we will reject the notion of independence and conclude that these data provide strong evidence of discrimination. Again, we will do this by determining how much difference in promotion rates would happen by random variation and compare this with the observed difference. We will make a decision based on probability considerations. 18.4.2 Simulating the study The table of data shows that 35 bank supervisors recommended promotion and 13 did not. Now, suppose the bankers decisions were independent of gender, that is the null hypothesis is true. Then, if we conducted the experiment again with a different random assignment of files, differences in promotion rates would be based only on random fluctuation. We can actually perform this randomization, which simulates what would have happened if the bankers decisions had been independent of gender but we had distributed the files differently.70 We will walk through the steps next. First lets import the data. discrim &lt;- read_csv(&quot;data/discrimination_study.csv&quot;) inspect(discrim) ## ## categorical variables: ## name class levels n missing ## 1 gender character 2 48 0 ## 2 decision character 2 48 0 ## distribution ## 1 female (50%), male (50%) ## 2 promoted (72.9%), not_promoted (27.1%) tally(~gender+decision,discrim,margins=TRUE) ## decision ## gender not_promoted promoted Total ## female 10 14 24 ## male 3 21 24 ## Total 13 35 48 Lets do some categorical data cleaning. To get the tally() results to look like our table, we need to change to factors and reorder the levels. We will use mutate_if() to convert characters to factors and fct_relevel() to change levels. discrim &lt;- discrim %&gt;% mutate_if(is.character,as.factor) %&gt;% mutate(gender=fct_relevel(gender,&quot;male&quot;), decision=fct_relevel(decision,&quot;promoted&quot;)) head(discrim) ## # A tibble: 6 x 2 ## gender decision ## &lt;fct&gt; &lt;fct&gt; ## 1 female not_promoted ## 2 female not_promoted ## 3 male promoted ## 4 female promoted ## 5 female promoted ## 6 female promoted tally(~gender+decision,discrim,margins = TRUE) ## decision ## gender promoted not_promoted Total ## male 21 3 24 ## female 14 10 24 ## Total 35 13 48 Now that we have the data in form that we want, we are ready to conduct the permutation test. To think about this simulation, imagine we actually had the personnel files. We thoroughly shuffle 48 personnel files, 24 labeled male and 24 labeled female, and deal these files into two stacks. We will deal 35 files into the first stack, which will represent the 35 supervisors who recommended promotion. The second stack will have 13 files, and it will represent the 13 supervisors who recommended against promotion. Remember that the files are identical except for the listed gender. This simulation then assumes that gender is not important and thus we can randomly assign the files to any of the supervisors. Then, as we did with the original data, we tabulate the results and determine the fraction of male and female who were promoted. Since we dont actually physically have the files, we will do this shuffle via computer code. Since the randomization of files in this simulation is independent of the promotion decisions, any difference in the two fractions is entirely due to chance. The following code shows the results of such a simulation. set.seed(101) tally(~shuffle(gender)+decision,discrim,margins = TRUE) ## decision ## shuffle(gender) promoted not_promoted Total ## male 18 6 24 ## female 17 7 24 ## Total 35 13 48 The shuffle() function randomly rearranges the gender column while keeping the decision column the same. It is really a sampling without replacement. Exercise: What is the difference in promotion rates between the two simulated groups? How does this compare to the observed difference 29.2% from the actual study?71 Calculating by hand will not help in a simulation, so we must write a function or use an existing one. We will use diffprop from the mosiac package. The code to find the difference for the original data is: (obs&lt;-diffprop(decision~gender,data=discrim)) ## diffprop ## -0.2916667 Notice that this is subtracting proportion of males promoted from the proportion of females. This does not impact our results as this is an arbitrary decision. We just need to be consistent in our analysis. If we prefer to use positive values we can adjust the order easily. diffprop(decision~fct_relevel(gender,&quot;female&quot;),data=discrim) ## diffprop ## 0.2916667 Notice that what we have done here, we developed a single number metric to measure the relationship between gender and decision. This single value metric is called the test statistic. We could have used a number of different metrics to include just the difference in males and females. The key idea in hypothesis testing is that once you decide on a test statistic, you need to find the distribution of that test statistic assuming the null hypothesis is true. 18.4.3 Checking for independence We computed one possible difference under the null hypothesis in the exercise above, which represents one difference due to chance. Repeating the simulation, we get another difference due to chance: -0.042. And another: 0.208. And so on until we repeat the simulation enough times that we have a good idea of what represents the distribution of differences from chance alone. That is the difference if there really is no relationship between gender and the promotion decision. We are using a simulation when there is actually a finite number of permutations of the gender label. From our lesson on counting, we have 48 labels of which 24 are male and 24 are female. Thus the total number of ways to arrange the labels differently is: \\[ \\frac{48!}{24!\\cdot24!} \\approx 3.2 \\cdot 10^{13} \\] factorial(48)/(factorial(24)*factorial(24)) ## [1] 3.22476e+13 This number of permutations is too large to find by hand or even via code and thus we will use a simulation. Lets simulate the experiment and plot the simulated values of the difference in the proportions of male and female files recommended for promotion. set.seed(2022) results &lt;- do(10000)*diffprop(decision~shuffle(gender),data=discrim) In Figure 18.1, we will insert a vertical line at the value of our observed difference. results %&gt;% gf_histogram(~diffprop) %&gt;% gf_vline(xintercept =-0.2916667 ) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Difference in proportions&quot;,y=&quot;Counts&quot;, title=&quot;Gender discrimination in hiring permutation test&quot;, subtitle=&quot;Test statistic is difference in promotion for female and male&quot;) Figure 18.1: Distribution of test statistic. Note that the distribution of these simulated differences is centered around 0 and is roughly symmetrical. It is centered on zero because we simulated differences in a way that made no distinction between men and women. This makes sense: we should expect differences from chance alone to fall around zero with some random fluctuation for each simulation under the assumption of the null hypothesis. The histogram also looks like a normal distribution; this is not a coincidence, it is a result of what is called the Central Limit Theorem which we will learn about in this block. Example: How often would you observe a difference of at least -29.2% (-0.292) according to the figure? (Often, sometimes, rarely, or never?) It appears that a difference of at least -29.2% due to chance alone would only happen rarely. We can estimate the probability using the results object. results %&gt;% summarise(p_value = mean(diffprop&lt;=obs)) ## p_value ## 1 0.0257 In our simulations, only 2.8% of the simulated test statistics were less than or equal to the observed test statistic, more extreme relative to the null hypothesis. Such a low probability indicates that observing such a large difference in proportions from chance alone is rare. This probability is known as a p-value. The p-value is a conditional probability, the probability of the observed value or more extreme given that the null hypothesis is true. The observed difference of -29.2% is a rare event if there really is no impact from listing gender in the candidates files, which provides us with two possible interpretations of the study results: \\(H_0\\): Null hypothesis. Gender has no effect on promotion decision, and we observed a difference that is so large that it would only happen rarely. \\(H_A\\): Alternative hypothesis. Gender has an effect on promotion decision, and what we observed was actually due to equally qualified women being discriminated against in promotion decisions, which explains the large difference of -29.2%. When we conduct formal studies, we reject a skeptical position if the data strongly conflict with that position.72 In our analysis, we determined that there was only a ~ 2% probability of obtaining a test statistic where the difference between female and male promotion proportions was 29.2% or larger assuming gender had no impact. So we conclude the data provide evidence of gender discrimination against women by the supervisors. In this case, we reject the null hypothesis in favor of the alternative. Statistical inference is the practice of making decisions and conclusions from data in the context of uncertainty. Errors do occur, just like rare events, and the data set at hand might lead us to the wrong conclusion. While a given data set may not always lead us to a correct conclusion, statistical inference gives us tools to control and evaluate how often these errors occur. Lets summarize what we did in this case study. We had a research question and some data to test the question. We then performed 4 steps: State the null and alternative hypotheses. Compute a test statistic. Determine the p-value. Draw a conclusion. We decided to use a randomization, a permutation test, to answer the question. When creating a randomization distribution, we attempted to satisfy 3 guiding principles. Be consistent with the null hypothesis. We need to simulate a world in which the null hypothesis is true. If we dont do this, we wont be testing our null hypothesis. In our problem, we assumed gender and promotion were independent. Use the data in the original sample. The original data should shed light on some aspects of the distribution that are not determined by null hypothesis. For our problem we used the difference in promotion rates. The data does not give us the distribution direction, but it gives us an idea that there is a large difference. Reflect the way the original data were collected. There were 48 files and 48 supervisors. A total of 35 files indicated promote. We keep this the same in our simulation. The remainder of this block expands on the ideas of this case study. 18.5 Homework Problems 1. Side effects of Avandia Rosiglitazone is the active ingredient in the controversial type~2 diabetes medicine Avandia and has been linked to an increased risk of serious cardiovascular problems such as stroke, heart failure, and death. A common alternative treatment is pioglitazone, the active ingredient in a diabetes medicine called Actos. In a nationwide retrospective observational study of 227,571 Medicare beneficiaries aged 65 years or older, it was found that 2,593 of the 67,593 patients using rosiglitazone and 5,386 of the 159,978 using pioglitazone had serious cardiovascular problems. These data are summarized in the contingency table below. \\[ \\begin{array}{cc|ccc} &amp; &amp; &amp;\\textit{Cardiovascular problems}\\\\ &amp; &amp; \\text{Yes} &amp; \\text{No} &amp; \\textbf{Total} \\\\ &amp; \\hline \\text{Rosiglitazone} &amp; 2,593 &amp; 65,000 &amp; 67,593 \\\\ \\textit{Treatment}&amp; \\text{Pioglitazone} &amp; 5,386 &amp; 154,592 &amp; 159,978 \\\\ &amp; \\textbf{Total} &amp; 7,979 &amp; 219,592 &amp; 227,571 \\\\ \\end{array} \\] Determine if each of the following statements is true or false. If false, explain why. The reasoning may be wrong even if the statements conclusion is correct. In such cases, the statement should be considered false. Since more patients on pioglitazone had cardiovascular problems (5,386 vs. 2,593), we can conclude that the rate of cardiovascular problems for those on a pioglitazone treatment is higher. The data suggest that diabetic patients who are taking rosiglitazone are more likely to have cardiovascular problems since the rate of incidence was (2,593 / 67,593 = 0.038) 3.8% for patients on this treatment, while it was only (5,386 / 159,978 = 0.034) 3.4% for patients on pioglitazone. The fact that the rate of incidence is higher for the rosiglitazone group proves that rosiglitazone causes serious cardiovascular problems. Based on the information provided so far, we cannot tell if the difference between the rates of incidences is due to a relationship between the two variables or due to chance. 2. Heart transplants The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. Another variable called was used to indicate whether or not the patient was alive at the end of the study. In the study, of the 34 patients in the control group, 4 were alive at the end of the study. Of the 69 patients in the treatment group, 24 were alive. The contingency table below summarizes these results. \\[ \\begin{array}{cc|ccc} &amp; &amp; &amp;\\textit{Group}\\\\ &amp; &amp; \\text{Control} &amp; \\text{Treatment} &amp; \\textbf{Total} \\\\ &amp; \\hline \\text{Alive} &amp; 4 &amp; 24 &amp; 28 \\\\ \\textit{Outcome}&amp; \\text{Dead} &amp; 30 &amp; 45 &amp; 75 \\\\ &amp; \\textbf{Total} &amp; 34 &amp; 69 &amp; 103\\\\ \\end{array} \\] The data is in a file called Stanford_heart_study.csv. Read the data in and answer the following questions. What proportion of patients in the treatment group and what proportion of patients in the control group died? One approach for investigating whether or not the treatment is effective is to use a randomization technique. What are the claims being tested? Use the same null and alternative hypothesis notation used in the lesson notes. The paragraph below describes the set up for such approach, if we were to do it without using statistical software. Fill in the blanks with a number or phrase, whichever is appropriate. We write alive on _______ cards representing patients who were alive at the end of the study, and dead on _______ cards representing patients who were not. Then, we shuffle these cards and split them into two groups: one group of size _______ representing treatment, and another group of size _______ representing control. We calculate the difference between the proportion of cards in the control and treatment groups (control - treatment), this is just so we have positive observed value, and record this value. We repeat this many times to build a distribution centered at _______. Lastly, we calculate the fraction of simulations where the simulated differences in proportions are _______ or _______. If this fraction of simulations, the empirical p-value, is low, we conclude that it is unlikely to have observed such an outcome by chance and that the null hypothesis should be rejected in favor of the alternative. Next we will perform the simulation and use results to decide the effectiveness of the transplant program. Find observed value of the test statistic, which we decided to use the difference in proportions. Simulate 1000 values of the test statistic by using shuffle() on the variable group. Plot distribution of results. Include a vertical line for the observed value. Clean up the plot as if you were presenting to a decision maker. Find p-value. Think carefully about what more extreme would mean. Decide if the treatment is effective. We would be assuming that these two variables are independent, meaning they are unrelated. Rosen B and Jerdee T. 1974. Influence of sex role stereotypes on personnel decisions. Journal of Applied Psychology 59(1):9-14. The study is an experiment, as subjects were randomly assigned a male file or a female file. Since this is an experiment, the results can be used to evaluate a causal relationship between gender of a candidate and the promotion decision. The test procedure we employ in this section is formally called a permutation test. \\(18/24 - 17/24=0.042\\) or about 4.2% in favor of the men. This difference due to chance is much smaller than the difference observed in the actual groups. This reasoning does not generally extend to anecdotal observations. Each of us observes incredibly rare events every day, events we could not possibly hope to predict. However, in the non-rigorous setting of anecdotal evidence, almost anything may appear to be a rare event, so the idea of looking for rare events in day-to-day activities is treacherous. For example, we might look at the lottery: there was only a 1 in 176 million chance that the Mega Millions numbers for the largest jackpot in history (March 30, 2012) would be (2, 4, 23, 38, 46) with a Mega ball of (23), but nonetheless those numbers came up! However, no matter what numbers had turned up, they would have had the same incredibly rare odds. That is, any set of numbers we could have observed would ultimately be incredibly rare. This type of situation is typical of our daily lives: each possible event in itself seems incredibly rare, but if we consider every alternative, those outcomes are also incredibly rare. We should be cautious not to misinterpret such anecdotal evidence. "],["HYPOTEST.html", "Chapter 19 Hypothesis Testing 19.1 Objectives 19.2 Decision making under uncertainty 19.3 Introduction 19.4 Hypothesis testing 19.5 Two-sided hypothesis test 19.6 Homework Problems", " Chapter 19 Hypothesis Testing 19.1 Objectives Know and properly use the terminology of a hypothesis test. Conduct all four steps of a hypothesis test using randomization. Discuss and explain the ideas of decision errors, one-sided versus two-sided, and choice of statistical significance. 19.2 Decision making under uncertainty At this point, it is useful to take a look at where we have been in this course and where we are going. We did this in the case study, but we want to discuss it again in a little more detail. We first looked at descriptive models to help us understand our data. This also required us to get familiar with software. We learned about graphical summaries, data collection methods, and summary metrics. Next we learned about probability models. These models allowed us to use assumptions and a small number of parameters to make statements about data and also to simulate data. We found that there is a close tie between probability models and statistical models. In our first efforts at statistical modeling, we started to use data to create estimates for parameters of a probability model. This work resulted in point estimates via method of moments and maximum likelihood. Now we are moving more in depth into statistical models. This is going to tie all the ideas together. We are going to use data from a sample and ideas of randomization to make conclusions about a population. This will require probability models, descriptive models, and some new ideas and terminology. We will generate point estimates for a metric designed to answer the research question and then find ways to determine the variability in the metric. Computational/Mathematical and hypothesis testing/confidence intervals context We are going to be using data from a sample of the population to make decisions about the population. There are many approaches and techniques for this. In this course we will be introducing and exploring different approaches; we are establishing foundations. As you can imagine, these ideas are varied, subtle, and at times difficult. We will just be exposing you to the foundational ideas. We want to make sure you understand that to become an accomplished practitioner, you must master the fundamentals and continue to learn the advanced ideas after the course. Historically there have been two approaches to statistical decision making, hypothesis testing and confidence intervals. At their mathematical foundation, they are equivalent but sometimes in practice they offer different perspectives on the problem. We will learn about both of these. The engines that drive the numeric results of a decision making model are either mathematical or computational. In reality, computational methods have mathematics behind them, and mathematical methods often require computer computations. The real distinction between them is the assumptions we are making about our population. Mathematical solutions typically have stricter assumptions thus leading to a tractable mathematical solution to the sampling distribution of the test statistic while computational models relax assumptions but may require extensive computational power. Like all problems, there is a trade off when one is better than the other. There is no one universal best method, some methods perform better in certain contexts. Do not think that computational methods such as the bootstrap are all you need to know. 19.3 Introduction In this lesson we will introduce hypothesis testing. It is really an extension of our last lesson, the case study. We will put more emphasis on terms and core concepts. In this lesson we will use a computational solution but this will lead us into thinking of mathematical solutions.73 The role of the analyst is always key regardless of the perceived power of the computer. The analyst must take the research question and translate it into a numeric metric for evaluation. The analyst must decide on the type of data and its collection to evaluate the question. The analyst must evaluate the variability in the metric and determine what that means in relation to the original research question. The analyst must propose an answer. 19.4 Hypothesis testing We will continue to emphasize the ideas of hypothesis testing through a data-driven example but also via analogy to the US court system. So lets begin our journey. Example: You are annoyed by TV commercials. You suspect that there were more commercials in the basic TV channels, typically the local area channels, than in the premium channels you pay extra for. To test this claim, hypothesis, you want to collect some data and decide. How would you collect his data? Here is one approach, we watch 20 random half hour shows of TV. Ten of those hours are basic TV and the other 10 are premium. In each case you record the total length of commercials in each show. Exercise: Is this enough data? You decide to have your friends help you, so you actually only watch 5 hours and got the rest of the data from your friends. Is this a problem? We cannot determine if this is enough data without some type of subject matter knowledge. First we need to decide on what metric to use to determine if a difference exists, more to come on this, and second how big of a difference from a practical standpoint is of interest. Is a loss of 1 minute of TV show enough to say there is a difference? How about 5 minutes? These are not statistical questions, but depend on the context of the problem and often need subject matter expertise to answer. Often data is collected without thought to these considerations. There are several courses here at USAFA that attempt to answer these questions. It is called a sample size calculation. For the second question, the answer depends on the protocol and operating procedures used. If your friends are trained on how to measure the length of commercials, what counts as an ad, and their skills verified, then it is probably not a problem to use them to collect data. Consistency in measurement is the key. The file ads.csv contains the data. Lets read the data into R and start to summarize. Remember to load the appropriate R packages. ads&lt;-read_csv(&quot;data/ads.csv&quot;) ads ## # A tibble: 10 x 2 ## basic premium ## &lt;dbl&gt; &lt;dbl&gt; ## 1 6.95 3.38 ## 2 10.0 7.8 ## 3 10.6 9.42 ## 4 10.2 4.66 ## 5 8.58 5.36 ## 6 7.62 7.63 ## 7 8.23 4.95 ## 8 10.4 8.01 ## 9 11.0 7.8 ## 10 8.52 9.58 glimpse(ads) ## Rows: 10 ## Columns: 2 ## $ basic &lt;dbl&gt; 6.950, 10.013, 10.620, 10.150, 8.583, 7.620, 8.233, 10.350,... ## $ premium &lt;dbl&gt; 3.383, 7.800, 9.416, 4.660, 5.360, 7.630, 4.950, 8.013, 7.8... Notice that this data is not tidy, for example does each row represent a single observations? Lets clean up, tidy, our data. Remember to ask yourself What do I want R to do? and What does it need to do this? We want one column that specifies the channel type and the other to specify length. We need R to put, pivot, the data into a longer form. We need the function pivot_longer(). For more information type vignette(\"pivot\") at the command prompt in R. ads &lt;- ads %&gt;% pivot_longer(cols=everything(),names_to=&quot;channel&quot;,values_to = &quot;length&quot;) ads ## # A tibble: 20 x 2 ## channel length ## &lt;chr&gt; &lt;dbl&gt; ## 1 basic 6.95 ## 2 premium 3.38 ## 3 basic 10.0 ## 4 premium 7.8 ## 5 basic 10.6 ## 6 premium 9.42 ## 7 basic 10.2 ## 8 premium 4.66 ## 9 basic 8.58 ## 10 premium 5.36 ## 11 basic 7.62 ## 12 premium 7.63 ## 13 basic 8.23 ## 14 premium 4.95 ## 15 basic 10.4 ## 16 premium 8.01 ## 17 basic 11.0 ## 18 premium 7.8 ## 19 basic 8.52 ## 20 premium 9.58 Looks good. Lets summarize the data. inspect(ads) ## ## categorical variables: ## name class levels n missing ## 1 channel character 2 20 0 ## distribution ## 1 basic (50%), premium (50%) ## ## quantitative variables: ## name class min Q1 median Q3 max mean sd n ## ...1 length numeric 3.383 7.4525 8.123 9.68825 11.016 8.03215 2.121412 20 ## missing ## ...1 0 This is not what we want, since we want to break it down by channel type. favstats(length~channel,data=ads) ## channel min Q1 median Q3 max mean sd n missing ## 1 basic 6.950 8.30375 9.298 10.30000 11.016 9.2051 1.396126 10 0 ## 2 premium 3.383 5.05250 7.715 7.95975 9.580 6.8592 2.119976 10 0 Exercise: Visualize the data using a boxplot. ads %&gt;% gf_boxplot(channel~length) %&gt;% gf_labs(title=&quot;Commercial Length&quot;,subtitle = &quot;Random 30 minute shows for 2 channel types&quot;, x=&quot;Length&quot;,y=&quot;Channel Type&quot; ) %&gt;% gf_theme(theme_bw) It appears that the premium channels are skewed to the left. A density plot may help us compare the distributions and see the skewness, Figure 19.1. ads %&gt;% gf_dens(~length,color = ~channel)%&gt;% gf_labs(title=&quot;Commercial Length&quot;,subtitle = &quot;Random 30 minute shows for 2 channel types&quot;, x=&quot;Length&quot;,y=&quot;Density&quot;,color=&quot;Channel Type&quot; ) %&gt;% gf_theme(theme_bw) Figure 19.1: Commercial length broken down by channel type. From this data, it looks like there is a difference between the two type of channels, but we must put the research question into a metric that will allow us to reach a decision. We will do this in a hypothesis test. As a reminder, the steps are State the null and alternative hypotheses. Compute a test statistic. Determine the p-value. Draw a conclusion. Before doing this, lets visit an example of hypothesis testing that has become common knowledge for us, the US criminal trial system, we could also use the cadet honor system. This analogy allows to remember and apply the steps. 19.4.1 Hypothesis testing in the US court system A US court considers two possible claims about a defendant: she is either innocent or guilty. Imagine you are the prosecutor. If we set these claims up in a hypothesis framework, the null hypothesis is that the defendant is innocent and the alternative is guilty. Your job as the prosecutor is to use evidence to demonstrate to the jury that the alternative hypothesis is the reasonable conclusion. The jury considers whether the evidence under the null hypothesis, innocence, is so convincing (strong) that there is no reasonable doubt regarding the persons guilt. That is, the skeptical perspective (null hypothesis) is that the person is innocent until evidence is presented that convinces the jury that the person is guilty (alternative hypothesis). Jurors examine the evidence under the assumption of innocence to see whether the evidence is so unlikely that it convincingly shows a defendant is guilty. Notice that if a jury finds a defendant not guilty, this does not necessarily mean the jury is confident in the persons innocence. They are simply not convinced of the alternative that the person is guilty. This is also the case with hypothesis testing: even if we fail to reject the null hypothesis, we typically do not accept the null hypothesis as truth. Failing to find strong evidence for the alternative hypothesis is not equivalent to providing evidence that the null hypothesis is true. There are two types of mistakes, letting a guilty person go free and sending an innocent person to jail. The criteria for making the decision, reasonable doubt, establishes the likelihood of those errors. Now back to our problem. 19.4.2 Step 1- State the null and alternative hypotheses The first step is to translate the research question into hypotheses. As a reminder, our research question is do premium channels have less ad time than basic channels? In collecting the data, we already decided the total length of time of commercials in a 30 minute shows was the correct data for answering this question. We believe that premium channels have less commercial time. However, the null hypothesis, the straw man, has to be the default case that makes it possible to generate a sampling distribution. \\(H_0\\): Null hypothesis. The distribution of length of commercials in premium and basic channels is the same. \\(H_A\\): Alternative hypothesis. The distribution of length of commercials in premium and basic channels is different. These hypotheses are vague, what does it mean to be different and how do we measure and summarize this? Lets move to the second step and then come back and modify our hypotheses. Notice that the null states the distributions are the same. When we generate our sampling distribution of the test statistic, we will sample under this null. 19.4.3 Step 2 - Compute a test statistic. Exercise: What type of metric could we use to test for a difference in commercials between the two channels? There are many ways for the distributions of lengths of commercials to differ. The easiest is to think of the summary statistics such as mean, median, standard deviation, or some combination of all of these. Historically, for mathematical reasons, it has been common to look at differences in measures of centrality, mean or median. The second consideration is what kind of difference? For example a ratio or an actual difference, subtraction. Again for historical reasons, the difference in means has been used as a measure. To keep things interesting, and to force those with some high school stats experience to think about this problem differently, we are going to use a different metric than has historically been used and taught. This also requires us to write some of our own code. Later we will ask you to complete the same analysis with a different test statistic either with your own code or using code from mosaic. Our metric is the ratio of the median length of commercials in basic channels to premium. Thus our hypotheses are now \\(H_0\\): Null hypothesis. The distribution of length of commercials in premium and basic channels is the same. \\(H_A\\): Alternative hypothesis. The distribution of length of commercials in premium and basic channels are different because the median length of basic channels ads is bigger than premium. First lets calculate the median length of commercials for your data. median(length~channel,data=ads) ## basic premium ## 9.298 7.715 so the ratio is median(length~channel,data=ads)[1]/median(length~channel,data=ads)[2] ## basic ## 1.205185 Lets put this into a function. metric &lt;- function(x){ temp&lt;-x[1]/x[2] names(temp) &lt;- &quot;test_stat&quot; return(temp) } metric(median(length~channel,data=ads) ) ## test_stat ## 1.205185 Now the observed value of the test statistic is saved in an object. obs&lt;-metric(median(length~channel,data=ads) ) obs ## test_stat ## 1.205185 Here is what we have done; we needed a single number metric to use in evaluating the null and alternative hypotheses. The null is that they have the same distribution and the alternative is that they dont. To measure the alternative we decided to use a ratio of the medians. If the number is close to 1 then the medians are not different. There may be other ways in which the distributions are different but we have decided on the ratio of medians. 19.4.4 Step 3 - Determine the p-value. As a reminder, the p-value is the probability of our observed test statistic or more extreme given the null hypothesis. Since our null hypothesis is that the distributions are the same we can use a randomization, permutation, test. We will shuffle the channel labels since under the null they are irrelevant. Here is the code for one run. set.seed(371) metric(median(length~shuffle(channel),data=ads)) ## test_stat ## 0.9957097 Lets generate our empirical sampling distribution of the test statistic we developed. results &lt;- do(1000)*metric(median(length~shuffle(channel),data=ads)) Next we create a plot of the distribution of the ratio of medians commercial length in basic and premium channels assuming they come from the same population, Figure 19.2. results %&gt;% gf_histogram(~test_stat) %&gt;% gf_vline(xintercept =obs ) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;Test statistic&quot;) Figure 19.2: Historgram of the sampling distribution by an approxiamte permutation test Notice that this distribution is centered on 1 and appears to be roughly symmetrical. The vertical line is our observed value of the test statistic. It seems to be in the tail, larger than expected if the channels came from the same distribution. Lets calculate the p-value. results %&gt;% summarise(p_value = mean(test_stat&gt;=obs)) ## p_value ## 1 0.026 Before proceeding, we have a technical question: Should we include the observed data in the calculation of the p-value? The answer is that most people would conclude that the original data is one of the possible permutations and thus include it. This practice will also insure that the p-value from a randomization test is never zero. In practice, this simply means adding 1 to both the numerator and denominator. The mosaic package has done this with the prop1() function. prop1(~(test_stat&gt;=obs),data=results) ## prop_TRUE ## 0.02697303 The test we performed is called a one-sided test since we only checked if the median length for the basic channels is larger than that of the premium. In this case of a one-sided test, more extreme meant a number much bigger than 1. A two-sided test is also common, in fact it is the more common, and is used if we did not apriori think one channel had longer commercials than the other. In this case we find the p-value by doubling the single-sided value. This is the case because more extreme could have happened in either tail. 19.4.5 Step 4 - Draw a conclusion In the last lesson we encountered a study from the 1970s that explored whether there was strong evidence that women were less likely to be promoted than men. The research question  are females discriminated against in promotion decisions made by male managers?  was framed in the context of hypotheses: \\(H_0\\): Gender has no effect on promotion decisions. \\(H_A\\):] Women are discriminated against in promotion decisions. We used a difference in promotion proportions as our test statistic. The null hypothesis (\\(H_0\\)) was a perspective of no difference. The data provided a point estimate of a -29.2% difference in recommended promotion rates between men and women. We determined that such a difference from chance alone would be rare: it would only happen about 2 in 100 times. When results like these are inconsistent with \\(H_0\\), we reject \\(H_0\\) in favor of \\(H_A\\). Here, we concluded there was evidence of discrimination against women. The 2-in-100 chance is the p-value, which is a probability quantifying the strength of the evidence against the null hypothesis and in favor of the alternative. When the p-value is small, i.e. less than a previously set threshold, we say the results are statistically significant. This means the data provide such strong evidence against \\(H_0\\) that we reject the null hypothesis in favor of the alternative hypothesis. The threshold, called the significance level and often represented by the Greek letter \\(\\alpha\\), is typically set to \\(\\alpha = 0.05\\), but can vary depending on the field or the application. Using a significance level of \\(\\alpha = 0.05\\) in the discrimination study, we can say that the data provided statistically significant evidence against the null hypothesis. We say that the data provide statistically significant evidence against the null hypothesis if the p-value is less than some reference value, usually \\(\\alpha=0.05\\). If the null hypothesis is true, unknown to us, the significance level \\(\\alpha\\) defines the probability that we will make a Type 1 Error, we will define errors in the next section. Side note: Whats so special about 0.05? We often use a threshold of 0.05 to determine whether a result is statistically significant. But why 0.05? Maybe we should use a bigger number, or maybe a smaller number. If youre a little puzzled, that probably means youre reading with a critical eye  good job! There are many video clips that explain the use of 0.05. Sometimes its also a good idea to deviate from the standard and it depends on the risk that the decision maker wants in terms of the two types of errors. Exercise: Using our p-value make a decision. Based on our data, if there were really no difference in the distribution of lengths of commercials in 30 minute shows between basic and premium channels then the probability of finding our observed ratio of medians is 0.027. Since this is less than our significance level of 0.05, we reject the null in favor of the alternative that the basic channel has longer commercials. 19.4.6 Decision errors Hypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, data can point to the wrong conclusion. However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion. There are two competing hypotheses: the null and the alternative. In a hypothesis test, we make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized below. \\[ \\begin{array}{cc|cc} &amp; &amp; &amp;\\textbf{Test Conclusion}\\\\ &amp; &amp; \\text{do not reject } H_0 &amp; \\text{reject } H_0 \\text{ in favor of }H_A \\\\ &amp; \\hline H_0 \\text{ true} &amp; \\text{okay} &amp; \\text{Type~1 Error} \\\\ \\textbf{Truth}&amp; H_A \\text{true} &amp; \\text{Type 2 Error} &amp; \\text{okay} \\\\ \\end{array} \\] A Type 1 Error, also called a false positive, is rejecting the null hypothesis when \\(H_0\\) is actually true. Since we rejected the null hypothesis in the gender discrimination and the commercial length studies, it is possible that we made a Type 1 Error in one or both of those studies. A Type 2 Error, also called a false negative, is failing to reject the null hypothesis when the alternative is actually true. Example: In a US court, the defendant is either innocent (\\(H_0\\)) or guilty (\\(H_A\\)). What does a Type 1 Error represent in this context? What does a Type 2 Error represent? If the court makes a Type 1 Error, this means the defendant is innocent (\\(H_0\\) true) but wrongly convicted. A Type 2 Error means the court failed to reject \\(H_0\\) (i.e. failed to convict the person) when she was in fact guilty (\\(H_A\\) true). Exercise: Consider the commercial length study where we concluded basic channels had longer commercials than premium channels. What would a Type 1 Error represent in this context?74 Exercise: How could we reduce the Type 1 Error rate in US courts? What influence would this have on the Type 2 Error rate? To lower the Type 1 Error rate, we might raise our standard for conviction from beyond a reasonable doubt to beyond a conceivable doubt so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type 2 Errors. Exercise: How could we reduce the Type 2 Error rate in US courts? What influence would this have on the Type 1 Error rate? To lower the Type 2 Error rate, we want to convict more guilty people. We could lower the standards for conviction from beyond a reasonable doubt to beyond a little doubt. Lowering the bar for guilt will also result in more wrongful convictions, raising the Type 1 Error rate. Think about the cadet honor system, its metric of evaluation, and the impact on the types of errors. These exercises provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type for given amount of data, information. 19.4.7 Choosing a significance level Choosing a significance level for a test is important in many contexts, and the traditional level is 0.05. However, it is sometimes helpful to adjust the significance level based on the application. We may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test. If making a Type 1 Error is dangerous or especially costly, we should choose a small significance level (e.g. 0.01 or 0.001). Under this scenario, we want to be very cautious about rejecting the null hypothesis, so we demand very strong evidence favoring the alternative \\(H_A\\) before we would reject \\(H_0\\). If a Type 2 Error is relatively more dangerous or much more costly than a Type 1 Error, then we should choose a higher significance level (e.g. 0.10). Here we want to be cautious about failing to reject \\(H_0\\) when the null is actually false. The significance level selected for a test should reflect the real-world consequences associated with making a Type 1 or Type 2 Error. 19.4.8 Introducing two-sided hypotheses So far we have explored whether women were discriminated against and whether commercials were longer depending on the type of channel. In these two case studies, weve actually ignored some possibilities: What if men are actually discriminated against? What if ads on premium channels are actually longer? These possibilities werent considered in our hypotheses or analyses. This may have seemed natural since the data pointed in the directions in which we framed the problems. However, there are two dangers if we ignore possibilities that disagree with our data or that conflict with our worldview: Framing an alternative hypothesis simply to match the direction that the data point will generally inflate the Type 1 Error rate. After all the work weve done (and will continue to do) to rigorously control the error rates in hypothesis tests, careless construction of the alternative hypotheses can disrupt that hard work. If we only use alternative hypotheses that agree with our worldview, then were going to be subjecting ourselves to confirmation bias, which means we are looking for data that supports our ideas. Thats not very scientific, and we can do better! The previous hypotheses weve seen are called one-sided hypothesis tests because they only explored one direction of possibilities. Such hypotheses are appropriate when we are exclusively interested in the single direction, but usually we want to consider all possibilities. To do so, lets discuss two-sided hypothesis tests in the context of a new study that examines the impact of using blood thinners on patients who have undergone CPR. 19.5 Two-sided hypothesis test It is important to distinguish between a two-sided hypothesis test and a one-sided test. In a two-sided test, we are concerned with whether or not the population parameter could take a particular value. For parameter \\(\\theta\\), a set of two-sided hypotheses looks like: \\[ H_0: \\theta=\\theta_0 \\hspace{0.75cm} H_1: \\theta\\neq \\theta_0 \\] In a one-sided test, we are concerned with whether a parameter exceeds or does not exceed a specific value. A set of one-sided hypotheses looks like: \\[ H_0: \\theta = \\theta_0 \\hspace{0.75cm} H_1:\\theta&gt;\\theta_0 \\] or \\[ H_0: \\theta = \\theta_0 \\hspace{0.75cm} H_1:\\theta&lt;\\theta_0 \\] In some texts, one-sided null hypotheses include an inequality (\\(\\geq\\) or \\(\\leq\\)). We have demonstrated one-sided tests and in the next example we will use a two-sided test. 19.5.1 Example CPR Cardiopulmonary resuscitation (CPR) is a procedure used on individuals suffering a heart attack when other emergency resources are unavailable. This procedure is helpful in providing some blood circulation to keep a person alive, but CPR chest compressions can also cause internal injuries. Internal bleeding and other injuries that can result from CPR complicate additional treatment efforts. For instance, blood thinners may be used to help release a clot that is causing the heart attack once a patient arrives in the hospital. However, blood thinners negatively affect internal injuries. Here we consider an experiment with patients who underwent CPR for a heart attack and were subsequently admitted to a hospital.75 Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours. 19.5.2 Step 1- State the null and alternative hypotheses Exercise: Form hypotheses for this study in plain and statistical language. Let \\(p_c\\) represent the true survival rate of people who do not receive a blood thinner (corresponding to the control group) and \\(p_t\\) represent the survival rate for people receiving a blood thinner (corresponding to the treatment group). We want to understand whether blood thinners are helpful or harmful. Well consider both of these possibilities using a two-sided hypothesis test. \\(H_0\\): Blood thinners do not have an overall survival effect, experimental treatment is independent of survival rate. \\(p_c - p_t = 0\\). \\(H_A\\): Blood thinners have an impact on survival, either positive or negative, but not zero. \\(p_c - p_t \\neq 0\\). Notice here that we accelerated the process by already defining our test statistic, our metric, in the hypothesis. It is the difference in survival rates for the control and treatment groups. This is a similar metric to what we used in the case study. We could use others but this will allow us to use functions from the mosaic package and will also help us to understand metrics for mathematically derived sampling distributions. There were 50 patients in the experiment who did not receive a blood thinner and 40 patients who did. The study results are in the file blood_thinner.csv. thinner &lt;- read_csv(&quot;data/blood_thinner.csv&quot;) thinner ## # A tibble: 90 x 2 ## group outcome ## &lt;chr&gt; &lt;chr&gt; ## 1 treatment survived ## 2 control survived ## 3 control died ## 4 control died ## 5 control died ## 6 treatment survived ## 7 control died ## 8 control died ## 9 treatment died ## 10 treatment survived ## # ... with 80 more rows Lets put it in a table. tally(~group+outcome,data=thinner,margins = TRUE) ## outcome ## group died survived Total ## control 39 11 50 ## treatment 26 14 40 ## Total 65 25 90 19.5.3 Step 2 - Compute a test statistic. The test statistic we have selected is the difference in survival rate in the control group versus the treatment group. The following R finds the observed proportions. tally(outcome~group,data=thinner,margins = TRUE,format=&quot;proportion&quot;) ## group ## outcome control treatment ## died 0.78 0.65 ## survived 0.22 0.35 ## Total 1.00 1.00 Notice the formula we used to get the correct variable in the column for the summary proportions. The observed test statistic can now be found.76 obs&lt;-diffprop(outcome~group,data=thinner) obs ## diffprop ## -0.13 Based on the point estimate, for patients who have undergone CPR outside of the hospital, an additional 13% of these patients survive when they are treated with blood thinners. However, we wonder if this difference could be easily explainable by chance. 19.5.4 Step 3 - Determine the p-value. As we did in our past two studies, we will simulate what type of differences we might see from chance alone under the null hypothesis. By randomly assigning simulated treatment and simulated control stickers to the patients files, we get a new grouping. If we repeat this simulation 10,000 times, we can build a null distribution of the differences, this is our empirical sampling distribution. set.seed(655) results &lt;- do(10000)*diffprop(outcome~shuffle(group),data=thinner) Figure 19.3 is a histogram of the estimated sampling distribution. results %&gt;% gf_histogram(~diffprop) %&gt;% gf_vline(xintercept =obs ) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;Test statistic&quot;) Figure 19.3: Histogram of the estiamted sampling distribution. Notice how it is centered on zero, the assumption of no difference. Also notice that it is unimodal and symmetrical. We will use this when we develop mathematical sampling distributions. prop1(~(diffprop&lt;=obs),data=results) ## prop_TRUE ## 0.1283872 The left tail area is about 0.13. (Note: it is only a coincidence that we also have \\(\\hat{p}_c - \\hat{p}_t= - 0.13\\).) However, contrary to how we calculated the p-value in previous studies, the p-value of this test is not 0.13! The p-value is defined as the chance we observe a result at least as favorable to the alternative hypothesis as the result (i.e. the difference) we observe. In this case, any differences greater than or equal to 0.13 would also provide equally strong evidence favoring the alternative hypothesis as a difference of - 0.13. A difference of 0.13 would correspond to 13% higher survival rate in the treatment group than the control group. There is something different in this study than in the past studies: in this study, we are particularly interested in whether blood thinners increase or decrease the risk of death in patients who undergo CPR before arriving at the hospital.77 For a two-sided test, take the single tail (in this case, 0.13) and double it to get the p-value: 0.26. 19.5.5 Step 4 - Draw a conclusion Since this p-value is larger than 0.05, we do not reject the null hypothesis. That is, we do not find statistically significant evidence that the blood thinner has any influence on survival of patients who undergo CPR prior to arriving at the hospital. Once again, we can discuss the causal conclusion since this is an experiment. Default to a two-sided test We want to be rigorous and keep an open mind when we analyze data and evidence. Use a one-sided hypothesis test only if you truly have interest in only one direction. Computing a p-value for a two-sided test First compute the p-value for one tail of the distribution, then double that value to get the two-sided p-value. Thats it! It is never okay to change two-sided tests to one-sided tests after observing the data. Hypothesis tests should be set up before seeing the data After observing data, it is tempting to turn a two-sided test into a one-sided test. Avoid this temptation. Hypotheses should be set up before observing the data. 19.5.6 How to use a hypothesis test This is a summary of the general framework for using hypothesis testing. It is the same steps with just slightly different wording. Frame the research question in terms of hypotheses. Hypothesis tests are appropriate for research questions that can be summarized in two competing hypotheses. The null hypothesis (\\(H_0\\)) usually represents a skeptical perspective or a perspective of no difference. The alternative hypothesis (\\(H_A\\)) usually represents a new view or a difference. Collect data with an observational study or experiment. If a research question can be formed into two hypotheses, we can collect data to run a hypothesis test. If the research question focuses on associations between variables but does not concern causation, we would run an observational study. If the research question seeks a causal connection between two or more variables, then an experiment should be used. Analyze the data. Choose an analysis technique appropriate for the data and identify the p-value. So far, weve only seen one analysis technique: randomization. Well encounter several new methods suitable for many other contexts. Form a conclusion. Using the p-value from the analysis, determine whether the data provide statistically significant evidence against the null hypothesis. Also, be sure to write the conclusion in plain language so casual readers can understand the results. 19.6 Homework Problems 1. Repeat the analysis of the commercial length in the notes. This time use a different test statistic. State the null and alternative hypotheses. Compute a test statistic. Determine the p-value. Draw a conclusion. 2. Is yawning contagious? An experiment conducted by the , a science entertainment TV program on the Discovery Channel, tested if a person can be subconsciously influenced into yawning if another person near them yawns. 50 people were randomly assigned to two groups: 34 to a group where a person near them yawned (treatment) and 16 to a group where there wasnt a person yawning near them (control). The following table shows the results of this experiment. \\[ \\begin{array}{cc|ccc} &amp; &amp; &amp;\\textbf{Group}\\\\ &amp; &amp; \\text{Treatment } &amp; \\text{Control} &amp; \\text{Total} \\\\ &amp; \\hline \\text{Yawn} &amp; 10 &amp; 4 &amp; 14 \\\\ \\textbf{Result} &amp; \\text{Not Yawn} &amp; 24 &amp; 12 &amp; 36 \\\\ &amp;\\text{Total} &amp; 34 &amp; 16 &amp; 50 \\\\ \\end{array} \\] The data is in the file yawn.csv. What are the hypotheses? Calculate the observed difference between the yawning rates under the two scenarios. Yes we are giving you the test statistic. Estimate the p-value using randomization. Plot the empirical sampling distribution. Determine the conclusion of the hypothesis test. The traditional belief is that yawning is contagious  one yawn can lead to another yawn, which might lead to another, and so on. In this exercise, there was the option of selecting a one-sided or two-sided test. Which would you recommend (or which did you choose)? Justify your answer in 1-3 sentences. How did you select your level of significance? Explain in 1-3 sentences. In our opinion, this is how things developed historically. However, since computational tools prior to machine computers, humans in most cases, were limited and expensive, there was a shift to mathematical solutions. The relatively recent increase and availability in machine computational power has lead to a shift back to computational methods. Thus some people think mathematical methods predate computational but that is not the case. Making a Type 1 Error in this context would mean that there is no difference in commercial length between basic and premium channels, despite the strong evidence (the data suggesting otherwise) found in the observational study. Notice that this does not necessarily mean something was wrong with the data or that we made a computational mistake. Sometimes data simply point us to the wrong conclusion, which is why scientific studies are often repeated to check initial findings. Replication is part of the scientific method. Efficacy and safety of thrombolytic therapy after initially unsuccessful cardiopulmonary resuscitation: a prospective clinical trial. The Lancet, 2001. Observed control survival rate: \\(p_c = \\frac{11}{50} = 0.22\\). Treatment survival rate: \\(p_t = \\frac{14}{40} = 0.35\\). Observed difference: \\(\\hat{p}_c - \\hat{p}_t = 0.22 - 0.35 = - 0.13\\). Realistically, we probably are interested in either direction in the past studies as well, and so we should have used the approach we now discuss in this section. However, for simplicity and the sake of not introducing too many concepts at once, we skipped over these details in earlier sections. "],["PVALUES.html", "Chapter 20 Empirical p-values 20.1 Objective 20.2 Hypothesis testing using probability models 20.3 Tappers and listeners 20.4 Cardiopulmonary resuscitation (CPR) 20.5 Golf Balls 20.6 Repeat with a different test statistic 20.7 Summary 20.8 Homework Problems", " Chapter 20 Empirical p-values 20.1 Objective Conduct all four steps of a hypothesis test using probability models. 20.2 Hypothesis testing using probability models As a lead into the central limit theorem and mathematical sampling distributions, we will look at a class of hypothesis testing where the null hypothesis specifies a probability model. In some cases we can get an exact answer and in others we will use simulation to get an empirical p-value. By the way, a permutation test is an exact test; by this we mean we are finding all the possible permutations in the calculation of the p-value. However, since the complete enumeration of all permutations is often difficult, we approximate it with randomization, simulation. Thus the p-value from a randomization test is an approximation of the exact test. Lets use three examples to illustrate the ideas of this lesson. 20.3 Tappers and listeners Heres a game you can try with your friends or family: pick a simple, well-known song, tap that tune on your desk, and see if the other person can guess the song. In this simple game, you are the tapper, and the other person is the listener. A Stanford University graduate student named Elizabeth Newton conducted an experiment using the tapper-listener game.78 In her study, she recruited 120 tappers and 120 listeners into the study. About 50% of the tappers expected that the listener would be able to guess the song. Newton wondered, is 50% a reasonable expectation? 20.3.1 Step 1- State the null and alternative hypotheses Newtons research question can be framed into two hypotheses: \\(H_0\\): The tappers are correct, and generally 50% of the time listeners are able to guess the tune. \\(p = 0.50\\) \\(H_A\\): The tappers are incorrect, and either more than or less than 50% of listeners will be able to guess the tune. \\(p \\neq 0.50\\) Exercise: Is this a two-sided or one-sided hypothesis test? How many variables are in this model? The tappers think that listeners will guess the song 50% of the time, so this is a two-sided test since we dont know before hand if listeners will be better or worse than this value. There is only one variable, is the listener correct? 20.3.2 Step 2 - Compute a test statistic. In Newtons study, only 42, (we changed the number to make this problem more interesting from an educational perspective) out of 120 listeners (\\(\\hat{p} = 0.35\\)) were able to guess the tune! From the perspective of the null hypothesis, we might wonder, how likely is it that we would get this result from chance alone? That is, whats the chance we would happen to see such a small fraction if \\(H_0\\) were true and the true correct-guess rate is 0.50? Now before we use simulation, lets frame this as a probability model. The random variable \\(X\\) is the number of correct out of 120. If the observations are independent and the probability of success is constant then we could use a binomial model. We cant answer the validity of these assumptions without knowing more about the experiment, the subjects, and the data collection. For educational purposes, we will assume they are valid. Thus our test statistic is the number of successes in 120 trials. The observed value is 42. 20.3.3 Step 3 - Determine the p-value. We now want to find the p-value as \\(2 \\cdot \\mbox{P}(X \\leq 42)\\) where \\(X\\) is a binomial with \\(p=0.5\\) and \\(n=120\\). Again, the p-value is the probability of the data or more extreme given the null hypothesis is true. Here the null hypothesis being true implies that the probability of success is 0.50. We will use R to get the one-sided p-value and then double to get the p-value for the problem. We selected \\(\\mbox{P}(X \\leq 42)\\) because more extreme means the observed values and values further from the value you would get if the null hypothesis were true, which is 60 for this problem. 2*pbinom(42,120,prob=0.5) ## [1] 0.001299333 That is a small p-value. 20.3.4 Step 4 - Draw a conclusion Based on our data, if the listeners were guessing correct 50% of the time, there is less than a \\(0.0013\\) probability that only 42 or less or 78 or more listeners would get it right. This is much less than 0.05, so we reject that the listeners are guessing correctly half of the time. This decision region looks like the pmf in Figure 20.1, any observed values inside the red boundary lines would be consistent with the null hypothesis. Any values at the red line or more extreme would be in the rejection region. gf_dist(&quot;binom&quot;,size=120,prob=.5,xlim=c(50,115)) %&gt;% gf_vline(xintercept = c(42,78),color=&quot;red&quot;) %&gt;% gf_theme(theme_bw) %&gt;% gf_labs(title=&quot;Binomial pmf&quot;,subtitle=&quot;Probability of success is 0.5&quot;,y=&quot;Probability&quot;) Figure 20.1: Binomial pmf 20.3.5 Repeat using simulation We will repeat the analysis using an empirical p-value. Step 1 is the same. 20.3.6 Step 2 - Compute a test statistic. We will use the proportion of listeners that get the song correct instead of the number, this is a minor change since we are simply dividing by 120. obs&lt;-42/120 obs ## [1] 0.35 20.3.7 Step 3 - Determine the p-value. To simulate 120 games under the null hypothesis where \\(p = 0.50\\), we could flip a coin 120 times. Each time the coin came up heads, this could represent the listener guessing correctly, and tails would represent the listener guessing incorrectly. For example, we can simulate 5 tapper-listener pairs by flipping a coin 5 times: \\[ \\begin{array}{ccccc} H &amp; H &amp; T &amp; H &amp; T \\\\ Correct &amp; Correct &amp; Wrong &amp; Correct &amp; Wrong \\\\ \\end{array} \\] After flipping the coin 120 times, we got 56 heads for \\(\\hat{p}_{sim} = 0.467\\). As we did with the randomization technique, seeing what would happen with one simulation isnt enough. In order to evaluate whether our originally observed proportion of 0.35 is unusual or not, we should generate more simulations. Here weve repeated this simulation 10000 times: results &lt;- rbinom(10000, 120, 0.5) / 120 Note, we could simulate it a number of ways. Here is a way using do() that will look like how we have coded for other randomization tests. set.seed(604) results&lt;-do(10000)*mean(sample(c(0,1),size=120,replace = TRUE)) head(results) ## mean ## 1 0.4250000 ## 2 0.5250000 ## 3 0.5916667 ## 4 0.5000000 ## 5 0.5250000 ## 6 0.5083333 results %&gt;% gf_histogram(~mean,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_vline(xintercept =c(obs,1-obs),color=&quot;red&quot;) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;Test statistic&quot;) Figure 20.2: The estimated sampling distribution. Notice in Figre 20.2 how the sampling distribution is centered at 0.5 and looks symmetrical. The p-value is found using the prop1 function, in this problem we really need the observed case added back in to prevent a p-value of zero. 2*prop1(~(mean&lt;=obs),data=results) ## prop_TRUE ## 0.00119988 20.3.8 Step 4 - Draw a conclusion In these 10,000 simulations, we see very few results close to 0.35. Based on our data, if the listeners were guessing correct 50% of the time, there is less than a \\(0.0012\\) probability that only 35% or less or 65% or more listeners would get it right. This is much less than 0.05, so we reject that the listeners are guessing correctly half of the time. Exercise: In the context of the experiment, what is the p-value for the hypothesis test?79 Exercise: Do the data provide statistically significant evidence against the null hypothesis? State an appropriate conclusion in the context of the research question.80 20.4 Cardiopulmonary resuscitation (CPR) Lets return to the CPR example from last lesson. As a reminder, we will repeat the background material. Cardiopulmonary resuscitation (CPR) is a procedure used on individuals suffering a heart attack when other emergency resources are unavailable. This procedure is helpful in providing some blood circulation to keep a person alive, but CPR chest compressions can also cause internal injuries. Internal bleeding and other injuries that can result from CPR complicate additional treatment efforts. For instance, blood thinners may be used to help release a clot that is causing the heart attack once a patient arrives in the hospital. However, blood thinners negatively affect internal injuries. Here we consider an experiment with patients who underwent CPR for a heart attack and were subsequently admitted to a hospital.81 Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours. 20.4.1 Step 1- State the null and alternative hypotheses We want to understand whether blood thinners are helpful or harmful. Well consider both of these possibilities using a two-sided hypothesis test. \\(H_0\\): Blood thinners do not have an overall survival effect, experimental treatment is independent of survival rate. \\(p_c - p_t = 0\\). \\(H_A\\): Blood thinners have an impact on survival, either positive or negative, but not zero. \\(p_c - p_t \\neq 0\\). thinner &lt;- read_csv(&quot;data/blood_thinner.csv&quot;) head(thinner) ## # A tibble: 6 x 2 ## group outcome ## &lt;chr&gt; &lt;chr&gt; ## 1 treatment survived ## 2 control survived ## 3 control died ## 4 control died ## 5 control died ## 6 treatment survived Lets put it in a table. tally(~group+outcome,data=thinner,margins = TRUE) ## outcome ## group died survived Total ## control 39 11 50 ## treatment 26 14 40 ## Total 65 25 90 20.4.2 Step 2 - Compute a test statistic. In this case the data is from a hypergeometric distribution, this is really a binomial from a finite population. We can calculate the p-value using this probability distribution. The random variable is the number of control patients that survived from a population of 90, where 50 are control patients and 40 are treatment patients, and where a total of 25 survived. 20.4.3 Step 3 - Determine the p-value. In this case we want to find \\(\\mbox{P}(X \\leq 11)\\) and double it since it is a two-sided test. 2*phyper(11,50,40,25) ## [1] 0.2581356 Note: We could have picked the lower right cell as the reference cell. But now I want the \\(\\mbox{P}(X \\geq 14)\\) with the appropriate change in parameter values. Notice we get the same answer. 2*(1-phyper(13,40,50,25)) ## [1] 0.2581356 We could the same thing for the other two cells. 2*phyper(26,40,50,65) ## [1] 0.2581356 2*(1-phyper(38,50,40,65)) ## [1] 0.2581356 Or R has a built in function, fisher.test(), that we could use. fisher.test(tally(~group+outcome,data=thinner)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: tally(~group + outcome, data = thinner) ## p-value = 0.2366 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.6794355 5.4174460 ## sample estimates: ## odds ratio ## 1.895136 The p-value is slightly different since the hypergeometric is not symmetric. Doubling the p-value from the single side result is not quite right. The algorithm in fisher.test() finds and adds all probabilities less than or equal to value of \\(\\mbox{P}(X = 11)\\), see Figure 20.3. This is the correct p-value. gf_dist(&quot;hyper&quot;,m=50,n=40,k=25) %&gt;% gf_hline(yintercept = dhyper(11,50,40,25),color=&quot;red&quot;) %&gt;% gf_labs(title=&quot;Hypergeometric pmf&quot;,subtitle=&quot;Red line is P(X=11)&quot;,y=&quot;Probability&quot;) %&gt;% gf_theme(theme_bw()) Figure 20.3: Hypergeometric pmf showing the cutoff for p-value calculation. This is how fisher.test() is calculating the p-value: temp&lt;-dhyper(0:25,50,40,25) sum(temp[temp&lt;=dhyper(11,50,40,25)]) ## [1] 0.2365928 The randomization test in the last lesson yielded a p-value of 0.257 so all tests are consistent. 20.4.4 Step 4 - Draw a conclusion Since this p-value is larger than 0.05, we do not reject the null hypothesis. That is, we do not find statistically significant evidence that the blood thinner has any influence on survival of patients who undergo CPR prior to arriving at the hospital. Once again, we can discuss the causal conclusion since this is an experiment. Notice that in these first two examples, we had a test of a single proportion and a test of two proportions. The single proportion test did not have an equivalent randomization test since there is not a second variable to shuffle. We were able to get answers since we found a probability model that we could use in each case. 20.5 Golf Balls Our last example will be interesting because the distribution has multiple parameters and a test metric is not obvious at this point. The owners of a residence located along a golf course collected the first 500 golf balls that landed on their property. Most golf balls are labeled with the make of the golf ball and a number, for example Nike 1 or Titleist 3. The numbers are typically between 1 and 4, and the owners of the residence wondered if these numbers are equally likely (at least among golf balls used by golfers of poor enough quality that they lose them in the yards of the residences along the fairway.) We will use a significance level of \\(\\alpha = 0.05\\) since there is no reason to favor one error over the other. 20.5.1 Step 1- State the null and alternative hypotheses We think that the numbers are not all equally likely. The question of one-sided versus two-sided is not relevant in this test, you will see this when we write the hypotheses. \\(H_0\\): All of the numbers are equally likely.\\(\\pi_1 = \\pi_2 = \\pi_3 = \\pi_4\\) Or \\(\\pi_1 = \\frac{1}{4}, \\pi_2 =\\frac{1}{4}, \\pi_3 =\\frac{1}{4}, \\pi_4 =\\frac{1}{4}\\) \\(H_A\\): There is some other distribution of percentages in the population. At least one population proportion is not \\(\\frac{1}{4}\\). Notice that we switched to using \\(\\pi\\) instead of \\(p\\) for the population parameter. There is no reason other than to make you aware that both are used. This problem is an extension of the binomial, instead of two outcomes, there are four outcomes. This is called a multinomial distribution. You can read more about it if you like, but our methods will not make it necessary to learn the probability mass function. Of the 500 golf balls collected, 486 of them had a number between 1 and 4. Lets get the data from `golf_balls.csv\". golf_balls &lt;- read_csv(&quot;data/golf_balls.csv&quot;) inspect(golf_balls) ## ## quantitative variables: ## name class min Q1 median Q3 max mean sd n missing ## ...1 number numeric 1 1 2 3 4 2.366255 1.107432 486 0 tally(~number,data=golf_balls) ## number ## 1 2 3 4 ## 137 138 107 104 20.5.2 Step 2 - Compute a test statistic. If all numbers were equally likely, we would expect to see 121.5 balls of each number, this is a point estimate and thus not an actual value that could be realized. Of course, in a sample we will have variation and thus departure from this state. We need a test statistic that will help us determine if the observed values are reasonable under the null hypothesis. Remember that the test statistics is a single number metric used to evaluate the hypothesis. Exercise: What would you propose for the test statistic? With four proportions, we need a way to combine them. This seems tricky, so lets just use a simple one. Lets take the maximum number of balls in any cell and subtract the minimum, this is called the range and we will denote the parameter as \\(R\\). Under the null this should be zero. We could re-write our hypotheses as: \\(H_0\\): \\(R=0\\) \\(H_A\\):] \\(R&gt;0\\) Notice that \\(R\\) will always be non-negative, thus this test is one-sided. The observed range is 34, \\(138 - 104\\). obs&lt;-diff(range(tally(~number,data=golf_balls))) obs ## [1] 34 20.5.3 Step 3 - Determine the p-value. We dont know the distribution of our test statistic so we will use simulation. We will simulate data from a multinomial under the null hypothesis and calculate a new value of the test statistic. We will repeat this 10000 times and this we give us an estimate of the sampling distribution. We will use the sample() function again to simulate the distribution of numbers under the null hypothesis. To help us understand the process and build the code, we are only initially using a sample size of 12 to keep the printout reasonable and easy to read. set.seed(3311) diff(range(table(sample(1:4,size=12,replace=TRUE)))) ## [1] 4 Notice this is not using tidyverse coding ideas. We dont think we need tibbles or data frames so we went with straight nested R code. You can break this code down by starting with the code in the center. set.seed(3311) sample(1:4,size=12,replace=TRUE) ## [1] 3 1 2 3 2 3 1 3 3 4 1 1 set.seed(3311) table(sample(1:4,size=12,replace=TRUE)) ## ## 1 2 3 4 ## 4 2 5 1 set.seed(3311) range(table(sample(1:4,size=12,replace=TRUE))) ## [1] 1 5 set.seed(3311) diff(range(table(sample(1:4,size=12,replace=TRUE)))) ## [1] 4 We are now ready to ramp up to the full problem. Lets simulated the data under the null hypothesis. We are sampling 486 golf balls with the numbers 1 through 4 on them. Each number is equally likely. We then find the range, our test statistic. Finally we repeat this 10,000 to get an estimate of the sampling distribution of our test statistic. results &lt;- do(10000)*diff(range(table(sample(1:4,size=486,replace=TRUE)))) Figure 20.4 is a plot of the sampling distribution of the range. results %&gt;% gf_histogram(~diff,fill=&quot;cyan&quot;,color = &quot;black&quot;) %&gt;% gf_vline(xintercept = obs,color=&quot;red&quot;) %&gt;% gf_labs(title=&quot;Sampling Distribution of Range&quot;,subtitle=&quot;Multinomial with equal probability&quot;, x=&quot;Range&quot;) %&gt;% gf_theme(theme_bw) Figure 20.4: Sampling distribution of the range. Notice how this distribution is skewed to the right. The p-value is 0.14, this value is greater than 0.05 so we fail to reject. However, it is not that much greater than 0.05, so the residents may want to repeat the study with more data. prop1(~(diff&gt;=obs),data=results) ## prop_TRUE ## 0.140286 20.5.4 Step 4 - Draw a conclusion Since this p-value is larger than 0.05, we do not reject the null hypothesis. That is, based on our data, we do not find statistically significant evidence against the claim that the number on the golf balls are equally likely. 20.6 Repeat with a different test statistic The test statistic we developed helped but it seems weak because we did not use the information in all four cells. So lets devise a metric that does this. We will jump to step 2. 20.6.1 Step 2 - Compute a test statistic. If each number were equally likely, we would have 121.5 balls in each bin. We can find a test statistic by looking at the deviation in each cell from 121.5. tally(~number,data=golf_balls) -121.5 ## number ## 1 2 3 4 ## 15.5 16.5 -14.5 -17.5 Now we need to collapse these into a single number. Just adding will always result in a value of 0, why? So lets take the absolute value and then add. obs&lt;-sum(abs(tally(~number,data=golf_balls) -121.5)) obs ## [1] 64 This will be our test statistic. 20.6.2 Step 3 - Determine the p-value. We will use similar code from above with our new metric. set.seed(9697) results &lt;- do(10000)*sum(abs(table(sample(1:4,size=486,replace=TRUE))-121.5)) Figure 20.5 is a plot of the sampling distribution of the absolute value of deviations. results %&gt;% gf_histogram(~sum,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_vline(xintercept = obs,color=&quot;red&quot;) %&gt;% gf_labs(title=&quot;Sampling Distribution of Absolute Deviations&quot;, subtitle=&quot;Multinomial with equal probability&quot;, x=&quot;Absolute deviations&quot;) %&gt;% gf_theme(theme_bw) Figure 20.5: Sampling distribution of the absolute deviations. Notice how this distribution is skewed to the right and our test statistic seems to be more extreme. The p-value is 0.014, this value is much smaller than our previous result. The test statistic matters in our decision process as nothing about this problem has changed except the test statistic. prop1(~(sum&gt;=obs),data=results) ## prop_TRUE ## 0.01359864 20.6.3 Step 4 - Draw a conclusion Since this p-value is smaller than 0.05, we reject the null hypothesis. That is, based on our data, we find statistically significant evidence against the claim that the number on the golf balls are equally likely. 20.7 Summary In this lesson we used probability models to help us make decisions from data. This lesson is different from the randomization section in that randomization had two variables and the null hypothesis of no difference. In the case of a 2 x 2 table, we were able to show that we could use the hypergeometric distribution to get an exact p-value under the assumptions of the model. We also found that the choice of test statistic has an impact on our decision. Even though we get valid p-values and the desired Type 1 error rate, if the information in the data is not used to its fullest, we will lose power. Note: power is the probability of rejecting the null hypothesis when the alternative hypothesis is true. In this next lesson we will learn about mathematical solutions to finding the sampling distribution. The key difference in all these methods is the selection of the test statistic and the assumptions made to derive a sampling distribution. 20.8 Homework Problems 1. Repeat the analysis of the yawning data from last lesson but this time use the hypergeometric distribution. Is yawning contagious? An experiment conducted by the , a science entertainment TV program on the Discovery Channel, tested if a person can be subconsciously influenced into yawning if another person near them yawns. 50 people were randomly assigned to two groups: 34 to a group where a person near them yawned (treatment) and 16 to a group where there wasnt a person yawning near them (control). The following table shows the results of this experiment. \\[ \\begin{array}{cc|ccc} &amp; &amp; &amp;\\textbf{Group}\\\\ &amp; &amp; \\text{Treatment } &amp; \\text{Control} &amp; \\text{Total} \\\\ &amp; \\hline \\text{Yawn} &amp; 10 &amp; 4 &amp; 14 \\\\ \\textbf{Result} &amp; \\text{Not Yawn} &amp; 24 &amp; 12 &amp; 36 \\\\ &amp;\\text{Total} &amp; 34 &amp; 16 &amp; 50 \\\\ \\end{array} \\] The data is in the file yawn.csv. What are the hypotheses? Calculate the observed statistic, pick a cell. Find the p-value using the hypergeometric distribution. Plot the the sampling distribution. Determine the conclusion of the hypothesis test. Compare your results with the randomization test. 2. Repeat the golf ball example using a different test statistic. Use a level of significance of 0.05. State the null and alternative hypotheses. Compute a test statistic. Determine the p-value. Draw a conclusion. 3. Body Temperature Shoemaker82 cites a paper from the American Medical Association83 that questions conventional wisdom that the average body temperature of a human is 98.6. One of the main points of the original article  the traditional mean of 98.6 is, in essence, 100 years out of date. The authors cite problems with Wunderlichs original methodology, diurnal fluctuations (up to 0.9 degrees F per day), and unreliable thermometers. The authors believe the average temperature is less than 98.6. Test the hypothesis. State the null and alternative hypotheses. State the significance level that will be used. Load the data from the file temperature.csv and generate summary statistics and a boxplot of the temperature data. We will not be using gender or heart rate for this problem. Compute a test statistic. We are going to help you with this part. We cannot do a randomization test since we dont have a second variable. It would be nice to use the mean as a test statistic but we dont yet know the sampling distribution of the sample mean. Lets get clever. If the distribution of the sample is symmetric, this is an assumption but look at the boxplot and summary statistics to determine if you are comfortable with it, then under the null hypothesis the observed values should be equally likely to either be greater or less than 98.6. Thus our test statistic is the number of cases that have a positive difference between the observed value and 98.6. This will be a binomial distribution with a probability of success of 0.5. You must also account for the possibility that there are observations of 98.6 in the data. Determine the p-value. Draw a conclusion. This case study is described in http://www.openintro.org/redirect.php?go=made-to-stick&amp;redirect=simulation_textbook_pdf_preliminary Made to Stick by Chip and Dan Heath. The p-value is the chance of seeing the data summary or something more in favor of the alternative hypothesis given that guessing has a probability of success of 0.5. Since we didnt observe many even close to just 42 correct, the p-value will be small, around 1-in-1000 or smaller. The p-value is less than 0.05, so we reject the null hypothesis. There is statistically significant evidence, and the data provide strong evidence that the chance a listener will guess the correct tune is less than 50%. Efficacy and safety of thrombolytic therapy after initially unsuccessful cardiopulmonary resuscitation: a prospective clinical trial. The Lancet, 2001. L. Shoemaker Allen (1996) Whats Normal?  Temperature, Gender, and Heart Rate, Journal of Statistics Education, 4:2 Mackowiak, P. A., Wasserman, S. S., and Levine, M. M. (1992), A Critical Appraisal of 98.6 Degrees F, the Upper Limit of the Normal Body Temperature, and Other Legacies of Carl Reinhold August Wunderlich, Journal of the American Medical Association, 268, 1578-1580. "],["CLT.html", "Chapter 21 Central Limit Theorem 21.1 Objectives 21.2 Central limit theorem 21.3 Other distribution for estimators 21.4 Hypotheses tests using CLT 21.5 Summary and rules of thumb 21.6 Homework Problems", " Chapter 21 Central Limit Theorem 21.1 Objectives Explain the central limit theorem and when you can use it for inference. Conduct hypothesis tests of a single mean and proportion using the CLT and R. Explain how the chi-squared and \\(t\\) distributions relate to the normal distribution, where we use them, and describe the impact on the shape of the distribution when the parameters are changed. 21.2 Central limit theorem Weve encountered several research questions and associated hypothesis tests so far in this block of material. While they differ in the settings, in their outcomes, and also in the technique weve used to analyze the data, many of them had something in common: for a certain class of test statistics, the general shape of the sampling distribution under the null hypothesis looks like a normal distribution. 21.2.1 Null distribution As a reminder, in the tapping and listening problem, we used the proportion of correct answers as our test statistic. Under the null hypothesis we assumed the probability of success was 0.5. The estimate of the sampling distribution of our test statistic is shown in Figure 21.1. Figure 21.1: Sampling distribution of the proportion. Exercise: Describe the shape of the distribution and note anything that you find interesting.84 In the Figure 21.2 we have overlayed a normal distribution on the histogram of the estimated sampling distribution. This allows us to visually compare a normal probability density curve with the empirical distribution of the sampling distribution. Figure 21.2: Sampling distribution of the sample proportion. This similarity between the empirical and theoretical distributions is not a coincidence, but rather, is guaranteed by mathematical theory. This lesson will be a little more notation and algebra intensive than the previous lessons. However, the goal is to develop a tool that will help us find sampling distributions for test statistics and thus find p-values. This lesson is classical statistics often taught in AP high school classes as well as many introductory undergraduate statistics courses. Remember that before the advances of modern computing, these mathematical solutions were all that was available. 21.2.2 Theorem - central limit theorem Theorem: Let \\(X_1,X_2,...,X_n\\) be a sequence of iid random variables from a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma&lt;\\infty\\). Then, \\[ \\bar{X} \\overset{approx}{\\sim}\\textsf{Norm}\\left(\\mu,{\\sigma\\over\\sqrt{n}}\\right) \\] There is a lot going on in this theorem. First is that we are drawing independent samples from the same parent population. The central limit theorem (CLT) does not specify the form of this parent distribution, only that it has a finite variance. Then if we form a new random variable that involves the sum of the individual random variables, in this case the sample mean, the distribution of the new random variable is approximately normal. In the case of the sample mean, the expected value is the same mean as the parent population and the variance is the variance of the parent population divided by the sample size. Lets summarize these ideas. The process of creating a new random variable from the sum of independent identically distributed random variables is approximately normal. The approximation to a normal distribution improves with sample size \\(n\\). The mean and variance of the sampling distribution are a function of the mean and variance of the parent population, the sample size \\(n\\), and the form of the new random variable. If you go back and review examples, exercises, and homework problems from the previous lessons on hypothesis testing, you will see that we get symmetric normal looking distributions when we created test statistics that involved the process of summing. The example of a skewed distribution was the golf ball example where our test statistic was the difference of the max and min. It is hard to overstate the historical importance of this theorem to the field of inferential statistics and science in general. To get an understanding and some intuition of the central limit theorem, lets simulate some data and evaluate. 21.2.3 Simulating data for CLT For this next section, we are going to use an artificial example where we know the population distribution and parameters. We will repeat sampling from this many times and plot the distribution of the summary statistic of interest to demonstrate the CLT. This is purely an educational thought experiment to convince ourselves about the validity of the CLT. Suppose there is an upcoming election in Colorado and Proposition A is on the ballot. Now suppose that 65% of Colorado voters support Proposition A. We poll a random sample of \\(n\\) Colorado voters. Prior to conducting the sample, we can think about the sample as a sequence of iid random variables from the binomial distribution with one trial in each run and a probability of success (support for the measure) of 0.65. In other words, each random variable will take a value of 1 (support) or 0 (oppose). Figure 21.3 is a plot of the pmf of the parent distribution (\\(\\textsf{Binom}(1,0.65)\\)): gf_dist(&quot;binom&quot;,size=1,prob=.65,plot_size=1) %&gt;% gf_theme(theme_classic()) %&gt;% gf_theme(scale_x_continuous(breaks = c(0,1))) %&gt;% gf_labs(y=&quot;Probability&quot;,x=&quot;X&quot;) Figure 21.3: Binomial pmf with 1 trial and probability of succes of 0.65. This is clearly not normal, it is in fact discrete. The mean of \\(X\\) is 0.65 and the standard deviation is \\(\\sqrt{0.65(1-0.65)}=0.477\\). In our first simulation, we let the sample size be ten, \\(n=10\\). This is typically too small for the CLT to apply but we will still use it as a starting point. In the code box below, we will obtain a sample of size 10 from this distribution and record the observed mean \\(\\bar{x}\\), which is our method of moments estimate of the probability of success. We will repeat this 10,000 times to get an empirical distribution of \\(\\bar{X}\\). (Note that \\(\\bar{X}\\) is a mean of 1s and 0s and can be thought of as a proportion of voters in the sample that support the measure. Often, population proportion is denoted as \\(\\pi\\) and the sample proportion is denoted as \\(\\hat{\\pi}\\).) set.seed(5501) results&lt;-do(10000)*mean(rbinom(10,1,0.65)) Since we are summing iid variables, the sampling distribution of the mean should look like a normal distribution. The mean should be close to 0.65, and the standard deviation \\(\\sqrt{\\frac{p(1-p)}{n}} = \\sqrt{\\frac{0.65(1-0.65)}{10}}=0.151\\) favstats(~mean,data=results) ## min Q1 median Q3 max mean sd n missing ## 0.1 0.5 0.7 0.8 1 0.64932 0.1505716 10000 0 Remember from our lessons on probability, these results for the mean and standard deviation do not depend on the CLT, they are results from the properties of expectation on independent samples. The distribution of the sample mean, the shape of the sampling distribution, is approximately normal as a result of the CLT, Figure 21.4. Figure 21.4: Sampling distribution of the sample proportion with sample size of 10. Note the sampling distribution of the sample mean has a bell-curvish shape, but with some skew to the left for this particular small sample size. That is why we state that the approximation improves with sample size. As a way to determine the impact of the sample size on the inference to the population, lets record how often a sample of 10 failed to indicate support for the measure. (How often was the sample proportion less than or equal to 0.5?) Remember, in this artificial example, we know that the population is in favor of the measure, 65% approval. However, if our point estimate is below 0.5, we would be led to believe that the population does not support the measure. results %&gt;% summarise(low_result=mean(~mean&lt;=0.5)) ## low_result ## 1 0.2505 Even though we know that 65% of Colorado voters support the measure, a sample of size 10 failed to indicate support 25.05% of the time. Lets take a larger sample. In the code below, we will repeat the above but with a sample of size 25. Figure 21.5 plots the sampling distribution. set.seed(5501) results&lt;-do(10000)*mean(rbinom(25,1,0.65)) Figure 21.5: Sampling distribution of the sample proportion with sample size of 25. results %&gt;% summarise(low_result=mean(~mean&lt;=0.5)) ## low_result ## 1 0.0623 When increasing the sample size to 25, the standard deviation of our sample proportion decreased. According to the central limit theorem, it should have decreased to \\(\\sigma/\\sqrt{25}=\\sqrt{\\frac{p(1-p)}{25}}=0.095\\). Also, the skew became less severe. Further, the sample of size 25 failed to indicate support only 6.23% of the time. It reasonably follows that an even larger sample would continue these trends. Figure 21.6 demonstrates these trends. clt %&gt;% gf_histogram(~mean,color=&quot;black&quot;,fill=&quot;cyan&quot;) %&gt;% gf_facet_grid(~samp) %&gt;% gf_theme(theme_bw()) Figure 21.6: Sampling distribution of the proportion for different trail sizes. 21.2.4 Summary of example In this example, we knew the true proportion of voters who supported the proposition. Based on that knowledge, we simulated the behavior of the sample proportion. We did this by taking a sample of size \\(n\\), recording the sample proportion, sample mean, and repeating that process thousands of times. In reality, we will not know the true underlying level of support; further, we will not take a sample repeatedly thousands of times from the parent population. Sampling can be expensive and time-consuming. Thus, we would take one random sample of size \\(n\\), and acknowledge that the resulting sample proportion is but one observation from an underlying normal distribution. We would then figure out what values of \\(\\pi\\) (the true unknown population proportion) could reasonably have resulted in the observed sample proportion. 21.3 Other distribution for estimators Prior to using the CLT in hypothesis testing, we want to discuss other sampling distributions that are based on the CLT or normality assumptions. A large part of theoretical statistics has been mathematically deriving the distribution of sample statistics. In these methods we obtain a sample statistic, determine the distribution of that statistic under certain conditions, and then use that information to make a statement about the population parameter. 21.3.1 Chi-sqaured Recall that the central limit theorem tells us that for reasonably large sample sizes, \\(\\bar{X}\\overset{approx}{\\sim}\\textsf{Norm}(\\mu,\\sigma/\\sqrt{n})\\). However, this expression involves two unknowns: \\(\\mu\\) and \\(\\sigma\\). In the case of binary data, population variance is a function of population proportion (\\(\\mbox{Var}(X)=\\pi(1-\\pi)\\)), so there is really just one unknown. In the case of continuous data, the standard deviation would need to be estimated. Let \\(S^2\\) be defined as: \\[ S^2={\\sum (X_i-\\bar{X})^2\\over n-1} \\] Recall that this is an unbiased estimate for \\(\\sigma^2\\). The sampling distribution of \\(S^2\\) can be found using the following lemma: Lemma: Let \\(X_1,X_2,...,X_n\\) be an iid sequence of random variables from a normal population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Then, \\[ {(n-1)S^2\\over \\sigma^2}\\sim \\textsf{Chisq}(n-1) \\] The \\(\\textsf{Chisq}(n-1)\\) distribution is read as the chi-squared distribution (chi is pronounced kye). The chi-squared distribution has one parameter: degrees of freedom. The chi-squared distribution is used in other contexts such as goodness of fit problems like the golf ball example from last lesson, we will discuss this particular application in a later lesson. The proof of this lemma is outside the scope of this class, but it is not terribly complicated. It follows from the fact that the sum of \\(n\\) squared random variables, each with the standard normal distribution, follows the chi-squared distribution with \\(n\\) degrees of freedom. This lemma can be used to draw inferences about \\(\\sigma^2\\). For a particular value of \\(\\sigma^2\\), we know how \\(S^2\\) should behave. So, for a particular value of \\(S^2\\), we can figure out reasonable values of \\(\\sigma^2\\). In practice, one rarely estimates \\(\\sigma\\) for the purpose of inferring on \\(\\sigma\\). Typically, we are interested in estimating \\(\\mu\\) and we need to account for the added uncertainty in estimating \\(\\sigma\\) as well. That is what we will discuss in the next section. 21.3.2 Students t Let \\(X_1,X_2,...,X_n\\) be an iid sequence of random variables, each with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Recall that the central limit theorem tells us that \\[ \\bar{X}\\overset{approx}{\\sim}\\textsf{Norm}(\\mu,\\sigma/\\sqrt{n}) \\] Rearranging: \\[ {\\bar{X}-\\mu\\over\\sigma/\\sqrt{n}}\\overset{approx}{\\sim}\\textsf{Norm}(0,1) \\] Again, \\(\\sigma\\) is unknown. Thus, we have to estimate it. We can estimate it with \\(S\\), but now we need to know the distribution of \\({\\bar{X}-\\mu\\over S/\\sqrt{n}}\\). This does not follow the normal distribution. Lemma: Let \\(X_1,X_2,...,X_n\\) be an iid sequence of random variables from a normal population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Then, \\[ {\\bar{X}-\\mu\\over S/\\sqrt{n}} \\sim \\textsf{t}(n-1) \\] The \\(\\textsf{t}(n-1)\\) distribution is read as the \\(t\\) distribution. The \\(t\\) distribution has one parameter: degrees of freedom. The expression above \\(\\left({\\bar{X}-\\mu\\over S/\\sqrt{n}}\\right)\\) is referred to as the \\(t\\) statistic. Similar to the chi-squared distribution, we wont go over the proof, but it follows from some simple algebra and from the fact that the ratio between a standard normal random variable and the square root of a chi-squared random variable, divided by its degrees of freedom follows a \\(t\\) distribution. The \\(t\\) distribution is very similar to the standard normal distribution, but has longer tails. This seems to make sense in the context of estimating \\(\\mu\\) since substituting \\(S\\) for \\(\\sigma\\) adds variability to the random variable. Figure 21.7 is a plot of the \\(t\\) distribution, shown as a blue line, and has a bell shape that looks very similar to a normal distribution, red line. However, its tails are thicker, which means observations are more likely to fall beyond two standard deviations from the mean than under the normal distribution. When our sample is small, the value \\(s\\) used to compute the standard error isnt very reliable. The extra thick tails of the \\(t\\) distribution are exactly the correction we need to resolve this problem. When the degrees of freedom is about 30 or more, the \\(t\\) distribution is nearly indistinguishable from the normal distribution. gf_dist(&quot;norm&quot;,color=&quot;red&quot;) %&gt;% gf_dist(&quot;t&quot;,df=3,color=&quot;blue&quot;) %&gt;% gf_theme(theme_bw()) Figure 21.7: The distibtion of t. 21.3.3 Important Note You may have noticed an important condition in the two lemmas above. It was assumed that each \\(X_i\\) in the sequence of random variables was normally distributed. While the central limit theorem has no such normality assumption, the distribution of the \\(t\\)-statistic is subject to the distribution of the underlying population. With a large enough sample size, this assumption is not necessary. There is no magic number, but some resources state that as long as \\(n\\) is at least 30-40, the underlying distribution doesnt matter. For smaller sample sizes, the underlying distribution should be relatively symmetric and unimodal. One advantage of simulation-based inference methods is that these methods do not rely on any such distributional assumptions. However, the simulation-based methods may have a smaller power for the same sample size. 21.4 Hypotheses tests using CLT We are now ready to repeat some of our previous problems using the mathematically derived sampling distribution via the CLT. 21.4.1 Tappers and listeners 21.4.1.1 Step 1- State the null and alternative hypotheses Here are the two hypotheses: \\(H_0\\): The tappers are correct, and generally 50% of the time listeners are able to guess the tune. \\(p = 0.50\\) \\(H_A\\): The tappers are incorrect, and either more than or less than 50% of listeners will be able to guess the tune. \\(p \\neq 0.50\\) 21.4.1.2 Step 2 - Compute a test statistic. The test statistic that we want to use is the sample mean \\(\\bar{X}\\), this is a method of moments estimate of the probability of success. Since these are independent samples from the same binomial distribution, by the CLT \\[ \\bar{X} \\overset{approx}{\\sim}\\textsf{Norm}\\left(\\pi,\\sqrt\\frac{\\pi(1-\\pi)}{n}\\right) \\] As we learned, this approximation improves with sample size. As a rule of thumb, most analysts are comfortable with using the CLT for this problem if the number of success and failures are both 10 or greater. In our study 42 out of 120 listeners (\\(\\bar{x}=\\hat{p} = 0.35\\)) were able to guess the tune. This is the observed value of test statistic. 21.4.1.3 Step 3 - Determine the p-value. We now want to find the p-value from the one-sided probability \\(\\mbox{P}(\\bar{X} \\leq 0.35)\\) given the null hypothesis is true, the probability of success is 0.50. We will use R to get the one-sided value and then double it since the test is two-sided and the sampling distribution is symmetrical. 2*pnorm(0.35,mean=.5,sd=sqrt(.5*.5/120)) ## [1] 0.001015001 That is a small p-value and consistent with what we would got using both the exact binomial test and the simulation empirical p-values. Important note: In the calculation of the standard deviation of the sampling distribution, we used the null hypothesized value of the probability of success. 21.4.1.4 Step 4 - Draw a conclusion Based on our data, if the listeners were guessing correct 50% of the time, there is less than a 1 in 1000 probability that only 42 or less or 78 or more listeners would get it right. This is much less than 0.05, so we reject that the listeners are guessing correctly half of the time. Now R has built in functions to perform this test. If you explore these functions, use ?prop.test to learn more, you will find options to improve the performance of the test. You are welcome and should read about these methods. Again, before computers, researchers spent time optimizing the performance of the asymptotic methods such as the CLT. Here is the test of a single proportion using R. prop.test(42,120) ## ## 1-sample proportions test with continuity correction ## ## data: 42 out of 120 ## X-squared = 10.208, df = 1, p-value = 0.001398 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.2667083 0.4430441 ## sample estimates: ## p ## 0.35 The p-value is small, reported as \\(0.0014\\). We will study the confidence interval soon so dont worry about that part of the output. The alternative hypothesis is also listed. Exercise: How do you conduct a one-sided test? What if the null value where 0.45?85 pval(prop.test(42,120,alternative=&quot;less&quot;,p=.45)) ## p.value ## 0.0174214 The exact test uses the function binom.test(). binom.test(42,120) ## ## ## ## data: 42 out of 120 ## number of successes = 42, number of trials = 120, p-value = 0.001299 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.2652023 0.4423947 ## sample estimates: ## probability of success ## 0.35 21.4.2 Body temperature We will repeat the body temperature analysis using the CLT. We will use \\(\\alpha = 0.05\\) 21.4.2.1 Step 1- State the null and alternative hypotheses \\(H_0\\): The average body temperature is 98.6; \\(\\mu = 98.6\\) \\(H_A\\): The average body temperature is less than 98.6; \\(\\mu &lt; 98.6\\) 21.4.2.2 Step 2 - Compute a test statistic. We dont know the population variance, so we will use the \\(t\\) distribution. Remember that \\[ {\\bar{X}-\\mu\\over S/\\sqrt{n}} \\sim \\textsf{t}(n-1) \\] thus our test statistic is \\[ \\frac{\\bar{x}-98.6}{S/\\sqrt{n}} \\] favstats(~temperature,data=temperature) ## min Q1 median Q3 max mean sd n missing ## 96.3 97.8 98.3 98.7 100.8 98.24923 0.7331832 130 0 temperature %&gt;% summarise(mean=mean(temperature),sd=sd(temperature),test_stat=(mean-98.6)/(sd/sqrt(130))) ## # A tibble: 1 x 3 ## mean sd test_stat ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 98.2 0.733 -5.45 We are over 5 standard deviation below the null hypothesis mean. We have some assumptions that we will discuss at the end of this problem. 21.4.2.3 Step 3 - Determine the p-value. We now want to find the p-value from \\(\\mbox{P}(t \\leq -5.45)\\) on 129 degrees of freedom, given the null hypothesis is true, which is that the probability of success is 0.50. We will use R to get the one-sided p-value. pt(-5.45,129) ## [1] 1.232178e-07 We could also use the R function t_test(). t_test(~temperature,data=temperature,mu=98.6,alternative=&quot;less&quot;) ## ## One Sample t-test ## ## data: temperature ## t = -5.4548, df = 129, p-value = 1.205e-07 ## alternative hypothesis: true mean is less than 98.6 ## 95 percent confidence interval: ## -Inf 98.35577 ## sample estimates: ## mean of x ## 98.24923 21.4.2.4 Step 4 - Draw a conclusion Based our data, if the true mean body temperature is 98.6, then the probability of observing a mean of 98.25 or less is 0.00000012. This is too unlikely so we reject the hypothesis that the average body temperature is 98.6. 21.5 Summary and rules of thumb We have covered a great deal in this lesson. At its core, the central limit theorem is a statement about the distribution of a sum of independent identically distributed random variables. This sum is approximately normal. First we summarize rules of thumb for the use of the CLT and \\(t\\) distribution. 21.5.1 Numerical data The central limit works regardless of the distribution. However, if the parent population is highly skewed, then more data is needed. The CLT works well once the sample sizes exceed 30 to 40. If the data is fairly symmetric, then less data is needed. When estimating the mean and standard error from a sample of numerical data, the \\(t\\) distribution is a little more accurate than the normal model. But there is an assumption that the parent population is normally distributed. This distribution works well even for small samples as long as the data is close to symmetrical and unimodal. For medium samples, at least 15 data points, the \\(t\\) distribution still works as long as the data is roughly symmetric. For large data sets 30-40 or more, the \\(t\\) or even the normal can be used. Now, lets discuss the assumptions of the \\(t\\) distribution and how to check them. Independence of observations. This is a difficult assumption to verify. If we collect a simple random sample from less than 10% of the population, or if the data are from an experiment or random process, we feel better about this assumption. If the data comes from an experiment, we can plot the data versus time collected to see if there are any patterns that indicate a relationship. A design of experiment course discusses these ideas. Observations come from a nearly normal distribution. This second condition is difficult to verify with small data sets. We often (i) take a look at a plot of the data for obvious departures from the normal model, usually in the form of prominent outliers, and (ii) consider whether any previous experiences alert us that the data may not be nearly normal. However, if the sample size is somewhat large, then we can relax this condition, e.g. moderate skew is acceptable when the sample size is 30 or more, and strong skew is acceptable when the size is about 60 or more. A typical plot to use to evaluate the normality assumption is called the quantile-quantile plot. We form a scatterplot of the empirical quantiles from the data versus exact quantile values from the theoretical distribution. If the points fall along a line then the data match the distribution. An exact match is not realistic, so we look for major departures from the line. Figure 21.8 is our normal-quantile plot for the body temperature data. The largest value may be an outlier, we may want to verify it was entered correctly. The fact that the points are above the line for the larger values and below the line for the smaller values indicates that our data may have longer tails than the normal distribution. There are really only 3 values in the larger quantiles so in fact the data may be slightly skewed to the left, this was also indicated by a comparison of the mean and median. However, since we have 130 data points these results should not impact our findings. gf_qq(~temperature,data=temperature) %&gt;% gf_qqline(~temperature,data=temperature) %&gt;% gf_theme(theme_bw()) Figure 21.8: Q-Q plot for body temperature data. We can also check the impacts of the assumptions by using other methods for the hypothesis test. If all methods give the same conclusion, we can be confident in the results. Another way to check robustness to assumptions is to simulate data from different distributions and evaluate the performance of the test under the simulated data. 21.5.2 Binary data The distribution of a binomial random variable or simple scalar transformations of it, such as the proportions of success found by dividing by the sample size, are approximately normal by the CLT. Since binomial random variables are bounded by zero and the number of trails, we have to make sure our probability of success is not close to zero or one, that is the number of successes is not close to 0 or \\(n\\). A general rule of thumb is that the number of success and failures be at least 10. 21.6 Homework Problems 1. Suppose we roll a fair six-sided die and let \\(X\\) be the resulting number. The distribution of \\(X\\) is discrete uniform. (Each of the six discrete outcomes is equally likely.) Suppose we roll the fair die 5 times and record the value of \\(\\bar{X}\\), the mean of the resulting rolls. Under the central limit theorem, what should be the distribution of \\(\\bar{X}\\)? Simulate this process in R. Plot the resulting empirical distribution of \\(\\bar{X}\\) and report the mean and standard deviation of \\(\\bar{X}\\). Was it what you expected? (HINT: You can simulate a die roll using the sample function. Be careful and make sure you use it properly.) c. Repeat parts a) and b) for \\(n=20\\) and \\(n=50\\). Describe what you notice. Make sure all three plots are plotted on the same \\(x\\)-axis scale. You can use facets if you combine your data into one tibble. 2. The nutrition label on a bag of potato chips says that a one ounce (28 gram) serving of potato chips has 130 calories and contains ten grams of fat, with three grams of saturated fat. A random sample of 35 bags yielded a sample mean of 134 calories with a standard deviation of 17 calories. Is there evidence that the nutrition label does not provide an accurate measure of calories in the bags of potato chips? The conditions necessary for applying the normal model have been checked and are satisfied. The question has been framed in terms of two possibilities: the nutrition label accurately lists the correct average calories per bag of chips or it does not, which may be framed as a hypothesis test. Write the null and alternative hypothesis. What level of significance are you going to use? What is the distribution of the test statistic \\({\\bar{X}-\\mu\\over S/\\sqrt{n}}\\)? Calculate the observed value. Calculate a p-value. Draw a conclusion. 3. Exploration of the chi-squared and \\(t\\) distributions. In R, plot the pdf of a random variable with the chi-squared distribution with 1 degree of freedom. On the same plot, include the pdfs with degrees of freedom of 5, 10 and 50. Describe how the behavior of the pdf changes with increasing degrees of freedom. Repeat part (a) with the \\(t\\) distribution. Add the pdf of a standard normal random variable as well. What do you notice? 4. In this lesson, we have used the expression degrees of freedom a lot. What does this expression mean? When we have sample of size \\(n\\), why are there \\(n-1\\) degrees of freedom for the \\(t\\) distribution? Give a short concise answer (about one paragraph). You will likely have to do a little research on your own. 5. Deborah Toohey is running for Congress, and her campaign manager claims she has more than 50% support from the districts electorate. Ms. Tooheys opponent claimed that Ms. Toohey has less than 50%. Set up a hypothesis test to evaluate who is right. Should we run a one-sided or two-sided hypothesis test? Write the null and alternative hypothesis. What level of significance are you going to use? What are the assumptions of this test? Calculate the test statistic. Calculate a p-value. Draw a conclusion. Note: A newspaper collects a simple random sample of 500 likely voters in the district and estimates Tooheys support to be 52%. In general, the distribution is reasonably symmetric. It is unimodal and looks like a normal distribution. We will only extract the p-value in this exercise "],["CI.html", "Chapter 22 Confidence Intervals 22.1 Objectives 22.2 Confidence interval 22.3 Confidence intervals for two proportions 22.4 Changing the confidence level 22.5 Interpreting confidence intervals 22.6 Homework Problems", " Chapter 22 Confidence Intervals 22.1 Objectives Using asymptotic methods based on the normal distribution, construct and interpret a confidence interval for an unknown parameter. Describe the relationships between confidence intervals, confidence level, and sample size. For proportions, be able to calculate the three different approaches for confidence intervals using R. 22.2 Confidence interval A point estimate provides a single plausible value for a parameter. However, a point estimate is rarely perfect; usually there is some error in the estimate. In addition to supplying a point estimate of a parameter, a next logical step would be to provide a plausible range of values for the parameter. 22.2.1 Capturing the population parameter A plausible range of values for the population parameter is called a confidence interval. Using only a point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net. We can throw a spear where we saw a fish, but we will probably miss. On the other hand, if we toss a net in that area, we have a good chance of catching the fish. If we report a point estimate, we probably will not hit the exact population parameter. On the other hand, if we report a range of plausible values  a confidence interval  we have a good shot at capturing the parameter. Exercise: If we want to be very certain we capture the population parameter, should we use a wider interval or a smaller interval?86 22.2.2 Constructing a confidence interval A point estimate is our best guess for the value of the parameter, so it makes sense to build the confidence interval around that value. The standard error, which is a measure of the uncertainty associated with the point estimate, provides a guide for how large we should make the confidence interval. Generally, what you should know about building confidence intervals is laid out in the following steps: Identify the parameter you would like to estimate (for example, \\(\\mu\\)). Identify a good estimate for that parameter (sample mean, \\(\\bar{X}\\)). Determine the distribution of your estimate or a function of your estimate. Use this distribution to obtain a range of feasible values (confidence interval) for the parameter. (For example if \\(\\mu\\) is the parameter of interest and we are using the CLT, then \\(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim \\textsf{Norm}(0,1)\\). We can solve the equation for \\(\\mu\\) to find a reasonable range of feasible values.) Lets do an example to solidify these ideas. Constructing a 95% confidence interval for the mean When the sampling distribution of a point estimate can reasonably be modeled as normal, the point estimate we observe will be within 1.96 standard errors of the true value of interest about 95% of the time. Thus, a 95% confidence interval for such a point estimate can be constructed: \\[ \\hat{\\theta} \\pm\\ 1.96 \\times SE_{\\hat{\\theta}}\\] Where \\(\\hat{\\theta}\\) is our estimate of the parameter and \\(SE_{\\hat{\\theta}}\\) is the standard error of that estimate. We can be 95% confident this interval captures the true value. The 1.96 can be found using the qnorm() function. If we want .95 in the middle, that leaves 0.025 in each tail. Thus we use .975 in the qnorm() function. qnorm(.975) ## [1] 1.959964 Exercise: Compute the area between -1.96 and 1.96 for a normal distribution with mean 0 and standard deviation 1. pnorm(1.96)-pnorm(-1.96) ## [1] 0.9500042 In mathematical terms, the derivation of this confidence is as follows: Let \\(X_1,X_2,...,X_n\\) be an iid sequence of random variables, each with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). The central limit theorem tells us that \\[ \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\overset{approx}{\\sim}\\textsf{Norm}(0,1) \\] If the significance level is \\(0\\leq \\alpha \\leq 1\\), then the confidence level is \\(1-\\alpha\\). Yes \\(\\alpha\\) is the same as the significance level in hypothesis testing. Thus \\[ \\mbox{P}\\left(-z_{\\alpha/2}\\leq {\\bar{X}-\\mu\\over \\sigma/\\sqrt{n}} \\leq z_{\\alpha/2}\\right)=1-\\alpha \\] where \\(z_{\\alpha/2}\\) is such that \\(\\mbox{P}(Z\\geq z_{\\alpha/2})=\\alpha/2\\), where \\(Z\\sim \\textsf{Norm}(0,1)\\), see Figure 22.1. Figure 22.1: The pdf of a standard normal distribution showing idea of how to develop a confidence interval. So, we know that \\((1-\\alpha)*100\\%\\) of the time, \\({\\bar{X}-\\mu\\over \\sigma/\\sqrt{n}}\\) will be between \\(-z_{\\alpha/2}\\) and \\(z_{\\alpha/2}\\). By rearranging the expression above and solving for \\(\\mu\\), we get: \\[ \\mbox{P}\\left(\\bar{X}-z_{\\alpha/2}{\\sigma\\over\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+z_{\\alpha/2}{\\sigma\\over\\sqrt{n}}\\right)=1-\\alpha \\] Be careful with the interpretation of this expression. As a reminder \\(\\bar{X}\\) is the random variable here. The population mean, \\(\\mu\\), is NOT a variable. It is an unknown parameter. Thus, the above expression is NOT a probabilistic statement about \\(\\mu\\), but rather about \\(\\bar{X}\\). Nonetheless, the above expression gives us a nice interval for reasonable values of \\(\\mu\\) given a particular sample. A \\((1-\\alpha)*100\\%\\) confidence interval for the mean is given by: \\[ \\mu\\in\\left(\\bar{x}\\pm z_{\\alpha/2}{\\sigma\\over\\sqrt{n}}\\right) \\] In most applications, the most common value of \\(\\alpha\\) is 0.05. In that case, to construct a 95% confidence interval, we would need to find \\(z_{0.025}\\) which can be found quickly with qnorm(): qnorm(1-0.05/2) ## [1] 1.959964 qnorm(.975) ## [1] 1.959964 22.2.2.1 Unknown Variance When inferring about the population mean, we usually will have to estimate the underlying standard deviation as well. This introduces an extra level of uncertainty. We found that while \\({\\bar{X}-\\mu\\over\\sigma/\\sqrt{n}}\\) has an approximate normal distribution, \\({\\bar{X}-\\mu\\over S/\\sqrt{n}}\\) follows the \\(t\\)-distribution with \\(n-1\\) degrees of freedom. This adds the additional assumption that the parent population, the distribution of \\(X\\), must be normal. Thus, when \\(\\sigma\\) is unknown, a \\((1-\\alpha)*100\\%\\) confidence interval for the mean is given by: \\[ \\mu\\in\\left(\\bar{x}\\pm t_{\\alpha/2,n-1}{s\\over\\sqrt{n}}\\right) \\] Similar to the case above, \\(t_{\\alpha/2,n-1}\\) can be found using the qt() function in R. In practice, if \\(X\\) is close to symmetrical and unimodal, we can relax the assumption of normality. Always look at your sample data. Outliers or skewness can be causes of concern. You can always run other methods that dont require the assumption of normality and compare results. For large sample sizes, the choice of using the normal distribution or the \\(t\\) distribution is irrelevant since they are close to each other. The \\(t\\) distribution requires you to use the degrees of freedom so be careful. 22.2.3 Body Temperature Example Example: Find a 95% confidence interval for the body temperature data from last lesson. We need the mean, standard deviation, and sample size from this data. The following R code calculates the confidence interval, make sure you can follow the code. temperature %&gt;% favstats(~temperature,data=.) %&gt;% select(mean,sd,n) %&gt;% summarise(lower_bound=mean-qt(0.975,129)*sd/sqrt(n), upper_bound=mean+qt(0.975,129)*sd/sqrt(n)) ## lower_bound upper_bound ## 1 98.122 98.37646 The 95% confidence interval for \\(\\mu\\) is \\((98.12,98.38)\\). We am 95% confident that \\(\\mu\\), the average human body temperature, is in this interval. Also, we could say that 95% of similarly constructed intervals will contain the true mean, \\(\\mu\\). There is a link between hypothesis testing and confidence intervals. Remember when we used this data in a hypothesis test, the null hypothesis was \\(H_0\\): The average body temperature is 98.6 \\(\\mu = 98.6\\). This null hypothesized value is not in the interval, so we could reject the null hypothesis with this confidence interval. We could also use R to find the confidence interval and conduct the hypothesis test. Read about the function t_test() in the help menu to determine why we used the mu option. t_test(~temperature,data=temperature,mu=98.6) ## ## One Sample t-test ## ## data: temperature ## t = -5.4548, df = 129, p-value = 2.411e-07 ## alternative hypothesis: true mean is not equal to 98.6 ## 95 percent confidence interval: ## 98.12200 98.37646 ## sample estimates: ## mean of x ## 98.24923 Or if you just want the interval: confint(t_test(~temperature,data=temperature,mu=98.6)) ## mean of x lower upper level ## 1 98.24923 98.122 98.37646 0.95 In reviewing the hypothesis test for a single mean, you can see how this confidence interval was formed by inverting the test statistic. As a reminder, the following equation inverts the test statistic. \\[ \\mbox{P}\\left(\\bar{X}-z_{\\alpha/2}{\\sigma\\over\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+z_{\\alpha/2}{\\sigma\\over\\sqrt{n}}\\right)=1-\\alpha \\] 22.2.4 One-sided Intervals If you remember the hypothesis test for temperature in the central limit theorem lesson, you may be crying foul. That was a one-sided hypothesis test and we just conducted a two-sided test. So far, we have discussed only two-sided intervals. These intervals have an upper and lower bound. Typically, \\(\\alpha\\) is apportioned equally between the two tails. (Thus, we look for \\(z_{\\alpha/2}\\).) In one-sided intervals, we only bound the interval on one side. We construct one-sided intervals when we are concerned with whether a parameter exceeds or stays below some threshold. Building a one-sided interval is similar to building two-sided intervals, except rather than dividing \\(\\alpha\\) into two, you simply apportion all of \\(\\alpha\\) to the relevant side. The difficult part is to determine if we need an upper bound or lower bound. For the body temperature study, the alternative hypothesis was that the mean was less than 98.6. In our confidence interval, we want to find the largest value the mean could be and thus we want the upper bound. We are trying to reject the hypothesis by showing an alternative that is smaller than the null hypothesized value. Finding the lower limit does not help us since the confidence interval indicates an interval that starts at the lower value and is unbounded above. Lets just make up some numbers; suppose the lower confidence bound is 97.5. All we know is the true average temperature is this value or greater. This is not helpful. However, if we find an upper confidence bound and the value is 98.1, we know the true average temperature is most likely no larger than this value. This is much more helpful. Repeating the analysis with this in mind. temperature %&gt;% favstats(~temperature,data=.) %&gt;% select(mean,sd,n) %&gt;% summarise(upper_bound=mean+qt(0.95,129)*sd/sqrt(n)) ## upper_bound ## 1 98.35577 confint(t_test(~temperature,data=temperature,alternative=&quot;less&quot;)) ## mean of x lower upper level ## 1 98.24923 -Inf 98.35577 0.95 Notice the upper bound in the one-sided interval is smaller than the upper bound in the two-sided interval since all 0.05 is going into the upper tail. 22.3 Confidence intervals for two proportions In hypothesis testing we had several examples of two proportions. We tested these problems with a permutation test or using a hypergeometric. In our notes or applications, we have not presented the hypothesis test for two proportions using the asymptotic normal distribution, the central limit theorem. So in this section we will present three methods of answering our research question, a permutation test, a hypothesis test using the normal distribution, and a confidence interval. Earlier this semester, in fact in the first lesson notes, we encountered an experiment that examined whether implanting a stent in the brain of a patient at risk for a stroke helps reduce the risk of a stroke. The results from the first 30 days of this study, which included 451 patients, are summarized in the R code below. These results are surprising! The point estimate suggests that patients who received stents may have a higher risk of stroke: \\(p_{trmt} - p_{control} = 0.090\\). stent &lt;- read_csv(&quot;data/stent_study.csv&quot;) tally(~group+outcome30,data=stent,margins = TRUE) ## outcome30 ## group no_event stroke Total ## control 214 13 227 ## trmt 191 33 224 ## Total 405 46 451 tally(outcome30~group,data=stent,margins = TRUE,format=&quot;proportion&quot;) ## group ## outcome30 control trmt ## no_event 0.94273128 0.85267857 ## stroke 0.05726872 0.14732143 ## Total 1.00000000 1.00000000 obs&lt;-diffprop(outcome30~group,data=stent) obs ## diffprop ## -0.09005271 Notice that because R uses the variables by names in alphabetic order we have \\(p_{control} - p_{trmt} = - 0.090\\). This is not a problem. We could fix this by changing the variables to factors. 22.3.1 Permutation test for two proportions We start with the null hypothesis which is two-sided since we dont know if the treatment is harmful or beneficial. \\(H_0\\): The treatment and outcome are independent. \\(p_{control} - p_{trmt} = 0\\) or \\(p_{control} = p_{trmt}\\). \\(H_A\\): The treatment and outcome are dependent \\(p_{control} \\neq p_{trmt}\\). We will use \\(\\alpha = 0.05\\). The test statistic is the difference in proportions of patients with stroke in the control and treatment groups. obs&lt;-diffprop(outcome30~group,data=stent) obs ## diffprop ## -0.09005271 To calculate the p-value, we will shuffle the treatment and control labels because under the null hypothesis, there is no difference. set.seed(2027) results &lt;- do(10000)*diffprop(outcome30~shuffle(group),data=stent) Figure 22.2 a visual summary of the distribution of the test statistics generated under the null hypothesis, the sampling distribution. results %&gt;% gf_dhistogram(~diffprop,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_vline(xintercept =obs ) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;Sampling distribution of randomization test&quot;, x=&quot;Difference in proportions&quot;,y=&quot;&quot;) Figure 22.2: Sampling distribution of the difference in proportions. 2*prop1(~(diffprop&lt;=obs),data=results) ## prop_TRUE ## 0.00259974 Based on the data, if there were no difference between the treatment and control groups, the probability of the observed differences in proportion of strokes being - 0.09 or more extreme is 0.0026. This is too unlikely, so we reject that there is no difference between control and stroke groups. 22.3.2 Hypothesis test for two proportions using normal model We must check two conditions before applying the normal model to a generic test of \\(\\hat{p}_1 - \\hat{p}_2\\). First, the sampling distribution for each sample proportion must be nearly normal, and secondly, the samples must be independent. Under these two conditions, the sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\) may be well approximated using the normal model. The hypotheses are the same as above. 22.3.2.1 Conditions for the sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\) to be normal The difference \\(\\hat{p}_1 - \\hat{p}_2\\) tends to follow a normal model when each proportion separately follows a normal model, and the two samples are independent of each other 22.3.2.2 Standard error For our research question the conditions must be verified. Because each group is a simple random sample from less than 10% of the population, the observations are independent, both within the samples and between the samples. The success-failure condition also holds for each sample, at least 10 in each cell is the easiest way to think about it. Because all conditions are met, the normal model can be used for the point estimate of the difference in proportion of strokes \\[p_{control} - p_{trmt} = 0.05726872 - 0.14732143 = - 0.090\\] The standard error of the difference in sample proportions is \\[ SE_{\\hat{p}_1 - \\hat{p}_2} = \\sqrt{SE_{\\hat{p}_1}^2 + SE_{\\hat{p}_2}^2}\\] \\[ = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\] where \\(p_1\\) and \\(p_2\\) represent the population proportions, and \\(n_1\\) and \\(n_2\\) represent the sample sizes. The calculation of the standard error for our problem must be done carefully. Remember in hypothesis testing, we assume the null hypothesis is true; this means the proportions of strokes must be the same. \\[SE = \\sqrt{\\frac{p(1-p)}{n_{control}} + \\frac{p(1-p)}{n_{trmt}}}\\] We dont know the exposure rate, \\(p\\), but we can obtain a good estimate of it by pooling the results of both samples: \\[\\hat{p} = \\frac{\\text{\\# of ``successes&#39;&#39;}}{\\text{\\# of cases}} = \\frac{13 + 33}{451} = 0.102\\] This is called the pooled estimate of the sample proportion, and we use it to compute the standard error when the null hypothesis is that \\(p_{control} = p_{trmt}\\). \\[SE \\approx \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n_{control}} + \\frac{\\hat{p}(1-\\hat{p})}{n_{trmt}}}\\] \\[SE \\approx \\sqrt{\\frac{0.102(1-0.102)}{227} + \\frac{0.102(1-0.102)}{224}} = 0.0285\\] The test statistic is \\[Z = \\frac{\\text{point estimate} - \\text{null value}}{SE} = \\frac{-.09 - 0}{0.0285} = - 3.16 \\] The p-value is 2*pnorm(-3.16) ## [1] 0.001577691 Which is close to what we got with permutation test. This should not surprise us as the sampling distribution under the permutation test looked normal. Figure 22.3 plots the empirical sampling distribution from the permutation test again with a normal density curve overlayed. results %&gt;% gf_dhistogram(~diffprop,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_vline(xintercept =obs ) %&gt;% gf_dist(&quot;norm&quot;,sd=0.0285,color=&quot;red&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;Sampling distribution of randomization test&quot;, subtitle=&quot;Reference normal distribution in red&quot;, x=&quot;Difference in proportions&quot;) Figure 22.3: The sampling distribution of the randomization test with a normal distribution plotted in red. 22.3.3 Confidence interval for two proportions using normal model The conditions for applying the normal model have already been verified, so we can proceed to the construction of the confidence interval. Remember the form of the confidence interval is \\[\\text{point estimate} \\ \\pm\\ z^{\\star}SE\\] Our point estimate is -0.09. The standard error is different since we cant assume the proportion of strokes are equal. We will estimate the standard error from \\[SE = \\sqrt{\\frac{p_{control}(1-p_{control})}{n_{control}} + \\frac{p_{trmt}(1-p_{trmt})}{n_{trmt}}}\\] \\[SE \\approx \\sqrt{\\frac{0.057(1-0.057)}{227} + \\frac{0.15(1-0.15)}{224}} = 0.0284\\] It is close to the pooled value because of the nearly equal sample sizes. The critical value is found from the normal quantile. qnorm(.975) ## [1] 1.959964 The 95% confidence interval is \\[ - 0.09 \\ \\pm\\ 1.96 \\times 0.0284 \\quad \\to \\quad (-0.146,- 0.034)\\] We are 95% confident that the difference in proportions of strokes in the control and treatment groups is between -0.146 and -0.034. Since this does not include zero, we are confident they are different. This supports the hypothesis tests. Of course, R has a built in function to calculate the hypothesis test and confidence interval for two proportions. prop_test(outcome30~group,data=stent) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: tally(outcome30 ~ group) ## X-squared = 9.0233, df = 1, p-value = 0.002666 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.03022922 0.14987619 ## sample estimates: ## prop 1 prop 2 ## 0.9427313 0.8526786 The p-value is a little different from the one we calculated and closer to the randomization test, which is an approximation of the exact permutation test, because a correction factor was applied. Read online about this correction to learn more. We run the code below with the correction factor off and get the same p-value as we calculated above. The confidence interval is a little different because the function used no stroke as its success event, but since zero is not in the interval, we get the same conclusion. prop_test(outcome30~group,data=stent,correct=FALSE) ## ## 2-sample test for equality of proportions without continuity ## correction ## ## data: tally(outcome30 ~ group) ## X-squared = 9.9823, df = 1, p-value = 0.001581 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.03466401 0.14544140 ## sample estimates: ## prop 1 prop 2 ## 0.9427313 0.8526786 Essentially, confidence intervals and hypothesis tests serve similar purposes, but answer slightly different questions. A confidence interval gives you a range of feasible values of a parameter given a particular sample. A hypothesis test tells you whether a specific value is feasible given a sample. Sometimes you can informally conduct a hypothesis test simply by building an interval and observing whether the hypothesized value is contained in the interval. The disadvantage to this approach is that it does not yield a specific \\(p\\)-value. The disadvantage of the hypothesis test is that it does not give a range of values for the test statistic. As with hypothesis tests, confidence intervals are imperfect. About 1-in-20 properly constructed 95% confidence intervals will fail to capture the parameter of interest. This is a similar idea to our Type 1 error. 22.4 Changing the confidence level Suppose we want to consider confidence intervals where the confidence level is somewhat higher than 95%; perhaps we would like a confidence level of 99%. Think back to the analogy about trying to catch a fish: if we want to be more sure that we will catch the fish, we should use a wider net. To create a 99% confidence level, we must also widen our 95% interval. On the other hand, if we want an interval with lower confidence, such as 90%, we could make our original 95% interval slightly slimmer. The 95% confidence interval structure provides guidance in how to make intervals with new confidence levels. Below is a general 95% confidence interval for a point estimate that comes from a nearly normal distribution: \\[\\text{point estimate}\\ \\pm\\ 1.96\\times SE \\] There are three components to this interval: the point estimate, ``1.96, and the standard error. The choice of \\(1.96\\times SE\\), which is also called margin of error, was based on capturing 95% of the data since the estimate is within 1.96 standard errors of the true value about 95% of the time. The choice of 1.96 corresponds to a 95% confidence level. Exercise: If \\(X\\) is a normally distributed random variable, how often will \\(X\\) be within 2.58 standard deviations of the mean?87 To create a 99% confidence interval, change 1.96 in the 95% confidence interval formula to be \\(2.58\\). The normal approximation is crucial to the precision of these confidence intervals. We will learn a method called the bootstrap that will allow us to find confidence intervals without the assumption of normality. 22.5 Interpreting confidence intervals A careful eye might have observed the somewhat awkward language used to describe confidence intervals. Correct interpretation: We are XX% confident that the population parameter is between Incorrect language might try to describe the confidence interval as capturing the population parameter with a certain probability. This is one of the most common errors: while it might be useful to think of it as a probability, the confidence level only quantifies how plausible it is that the parameter is in the interval. Another especially important consideration of confidence intervals is that they only try to capture the population parameter. Our intervals say nothing about the confidence of capturing individual observations, a proportion of the observations, or about capturing point estimates. Confidence intervals only attempt to capture population parameters. 22.6 Homework Problems Chronic illness In 2013, the Pew Research Foundation reported that 45% of U.S. adults report that they live with one or more chronic conditions.88 However, this value was based on a sample, so it may not be a perfect estimate for the population parameter of interest on its own. The study reported a standard error of about 1.2%, and a normal model may reasonably be used in this setting. Create a 95% confidence interval for the proportion of U.S. adults who live with one or more chronic conditions. Also interpret the confidence interval in the context of the study. Create a 99% confidence interval for the proportion of U.S. adults who live with one or more chronic conditions. Also interpret the confidence interval in the context of the study. Identify each of the following statements as true or false. Provide an explanation to justify each of your answers. We can say with certainty that the confidence interval from part a contains the true percentage of U.S. adults who suffer from a chronic illness. If we repeated this study 1,000 times and constructed a 95% confidence interval for each study, then approximately 950 of those confidence intervals would contain the true fraction of U.S. adults who suffer from chronic illnesses. The poll provides statistically significant evidence (at the \\(\\alpha = 0.05\\) level) that the percentage of U.S. adults who suffer from chronic illnesses is not 50%. Since the standard error is 1.2%, only 1.2% of people in the study communicated uncertainty about their answer. Suppose the researchers had formed a one-sided hypothesis, they believed that the true proportion is less than 50%. We could find an equivalent one-sided 95% confidence interval by taking the upper bound of our two-sided 95% confidence interval. 2. Vegetarian college students Suppose that 8% of college students are vegetarians. Determine if the following statements are true or false, and explain your reasoning. The distribution of the sample proportions of vegetarians in random samples of size 60 is approximately normal since \\(n \\ge 30\\). The distribution of the sample proportions of vegetarian college students in random samples of size 50 is right skewed. A random sample of 125 college students where 12% are vegetarians would be considered unusual. A random sample of 250 college students where 12% are vegetarians would be considered unusual. The standard error would be reduced by one-half if we increased the sample size from 125 to~250. A 99% confidence will be wider than a 95% because to have a higher confidence level requires a wider interval. 3. Orange tabbies Suppose that 90% of orange tabby cats are male. Determine if the following statements are true or false, and explain your reasoning. a. The distribution of sample proportions of random samples of size 30 is left skewed. b. Using a sample size that is 4 times as large will reduce the standard error of the sample proportion by one-half. c. The distribution of sample proportions of random samples of size 140 is approximately normal. 4. Working backwards A 90% confidence interval for a population mean is (65,77). The population distribution is approximately normal and the population standard deviation is unknown. This confidence interval is based on a simple random sample of 25 observations. Calculate the sample mean, the margin of error, and the sample standard deviation. 5. Find the p-value An independent random sample is selected from an approximately normal population with an unknown standard deviation. Find the p-value for the given set of hypotheses and \\(T\\) test statistic. Also determine if the null hypothesis would be rejected at \\(\\alpha = 0.05\\). \\(H_{A}: \\mu &gt; \\mu_{0}\\), \\(n = 11\\), \\(T = 1.91\\) \\(H_{A}: \\mu &lt; \\mu_{0}\\), \\(n = 17\\), \\(T = - 3.45\\) \\(H_{A}: \\mu \\ne \\mu_{0}\\), \\(n = 7\\), \\(T = 0.83\\) \\(H_{A}: \\mu &gt; \\mu_{0}\\), \\(n = 28\\), \\(T = 2.13\\) 6. Sleep habits of New Yorkers New York is known as the city that never sleeps. A random sample of 25 New Yorkers were asked how much sleep they get per night. Statistical summaries of these data are shown below. Do these data provide strong evidence that New Yorkers sleep less than 8 hours a night on average? \\[ \\begin{array}{ccccc} &amp; &amp; &amp;\\\\ \\hline n &amp; \\bar{x} &amp; s &amp; min &amp; max \\\\ \\hline 25 &amp; 7.73 &amp; 0.77 &amp; 6.17 &amp; 9.78 \\\\ \\hline \\end{array} \\] Write the hypotheses in symbols and in words. Check conditions, then calculate the test statistic, \\(T\\), and the associated degrees of freedom. Find and interpret the p-value in this context. What is the conclusion of the hypothesis test? Construct a 95% confidence interval that corresponded to this hypothesis test, would you expect 8 hours to be in the interval? 7. Vegetarian college students II From problem 2 part c, suppose that it has been reported that 8% of college students are vegetarians. We think USAFA is not typical because of their fitness and health awareness, we think there are more vegetarians. We collect a random sample of 125 cadets and find 12% claimed they are vegetarians. Is there enough evidence to claim that USAFA cadets are different? Use binom.test() to conduct the hypothesis test and find a confidence interval. Use prop.test() with correct=FALSE to conduct the hypothesis test and find a confidence interval. Use prop.test() with correct=TRUE to conduct the hypothesis test and find a confidence interval. Which test should you use? "]]
